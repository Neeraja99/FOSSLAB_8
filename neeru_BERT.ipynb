{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neeru_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neeraja99/FOSSLAB_8/blob/master/neeru_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbwJV6CqzLZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c8ccd6-4823-45a2-cfe3-c41c42e5e15c"
      },
      "source": [
        "!pip install pbr funcsigs\r\n",
        "!pip install tensorflow-gpu==1.15"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pbr\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\r\u001b[K     |███                             | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 20kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 30kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 40kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 51kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 61kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 71kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 81kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 92kB 9.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 102kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 7.7MB/s \n",
            "\u001b[?25hCollecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Installing collected packages: pbr, funcsigs\n",
            "Successfully installed funcsigs-1.0.2 pbr-5.5.1\n",
            "Collecting tensorflow-gpu==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 48.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.32.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 57.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.36.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.19.4)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.12.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (51.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=7af488095aa0843127ec07660ef5008b0bd2d2eb499997b0f725936cd0f9e685\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.4.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.0 has requirement tensorboard~=2.4, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.0 has requirement tensorflow-estimator<2.5.0,>=2.4.0rc0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, gast, keras-applications, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.0\n",
            "    Uninstalling tensorboard-2.4.0:\n",
            "      Successfully uninstalled tensorboard-2.4.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfMTG5gStHM7",
        "outputId": "ca81ef3c-5fe2-4bc9-e621-c8f9c85c43e0"
      },
      "source": [
        "!git clone https://github.com/santhoshkolloju/Abstractive-Summarization-With-Transfer-Learning.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Abstractive-Summarization-With-Transfer-Learning'...\n",
            "remote: Enumerating objects: 461, done.\u001b[K\n",
            "remote: Total 461 (delta 0), reused 0 (delta 0), pack-reused 461\u001b[K\n",
            "Receiving objects: 100% (461/461), 739.39 KiB | 14.50 MiB/s, done.\n",
            "Resolving deltas: 100% (121/121), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjCklX0Z4-Va"
      },
      "source": [
        "import os\r\n",
        "os.chdir('/content/Abstractive-Summarization-With-Transfer-Learning')\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-EYCIp4PZx_",
        "outputId": "8bb9f272-a245-4e2a-ad70-8d7a25d6d3eb"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT_SUMM.ipynb  Inference.py  models\t      Readme.md\n",
            "config.py\t main.py       preprocess.py  texar_repo\n",
            "data\t\t model.py      __pycache__    uncased_L-12_H-768_A-12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZZsPxXgtVaK",
        "outputId": "7b955bce-e2cf-4e37-b8c8-527e50f7e5bf"
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-07 06:21:33--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 74.125.195.128, 74.125.142.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   101MB/s    in 3.8s    \n",
            "\n",
            "2021-01-07 06:21:37 (101 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01WZzVOVyLX0",
        "outputId": "0f92882d-41f3-4e61-f333-b889eb626fd9"
      },
      "source": [
        "!unzip /content/Abstractive-Summarization-With-Transfer-Learning/uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/Abstractive-Summarization-With-Transfer-Learning/uncased_L-12_H-768_A-12.zip\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "replace uncased_L-12_H-768_A-12/vocab.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grx_5qGLy7j8",
        "outputId": "178b4fb8-1c46-46bd-e44e-84ab3cfedc1e"
      },
      "source": [
        "!python preprocess.py"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/layers.py:628: The name tf.layers.Layer is deprecated. Please use tf.compat.v1.layers.Layer instead.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/layers.py:681: The name tf.layers.MaxPooling1D is deprecated. Please use tf.compat.v1.layers.MaxPooling1D instead.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/layers.py:682: The name tf.layers.AveragePooling1D is deprecated. Please use tf.compat.v1.layers.AveragePooling1D instead.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/layers.py:1157: The name tf.layers.Conv1D is deprecated. Please use tf.compat.v1.layers.Conv1D instead.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/layers.py:1158: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/layers.py:1159: The name tf.layers.Conv3D is deprecated. Please use tf.compat.v1.layers.Conv3D instead.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/layers.py:1160: The name tf.layers.Conv2DTranspose is deprecated. Please use tf.compat.v1.layers.Conv2DTranspose instead.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/layers.py:1161: The name tf.layers.Conv3DTranspose is deprecated. Please use tf.compat.v1.layers.Conv3DTranspose instead.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/layers.py:1162: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/layers.py:1163: The name tf.layers.Dropout is deprecated. Please use tf.compat.v1.layers.Dropout instead.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/layers.py:1164: The name tf.layers.Flatten is deprecated. Please use tf.compat.v1.layers.Flatten instead.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/layers.py:1166: The name tf.layers.MaxPooling2D is deprecated. Please use tf.compat.v1.layers.MaxPooling2D instead.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/layers.py:1167: The name tf.layers.MaxPooling3D is deprecated. Please use tf.compat.v1.layers.MaxPooling3D instead.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/layers.py:1168: The name tf.layers.SeparableConv2D is deprecated. Please use tf.compat.v1.layers.SeparableConv2D instead.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/layers.py:1169: The name tf.layers.BatchNormalization is deprecated. Please use tf.compat.v1.layers.BatchNormalization instead.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/layers.py:1171: The name tf.layers.AveragePooling2D is deprecated. Please use tf.compat.v1.layers.AveragePooling2D instead.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/layers.py:1172: The name tf.layers.AveragePooling3D is deprecated. Please use tf.compat.v1.layers.AveragePooling3D instead.\n",
            "\n",
            "WARNING:tensorflow:From texar_repo/texar/core/optimization.py:482: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/utils/tokenization.py:41: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From preprocess.py:77: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From preprocess.py:128: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From preprocess.py:251: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From preprocess.py:301: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "WARNING:tensorflow:From preprocess.py:308: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "{'src_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:0' shape=(512,) dtype=int64>, 'src_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:1' shape=(512,) dtype=int64>, 'src_segment_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:2' shape=(512,) dtype=int64>, 'tgt_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:3' shape=(400,) dtype=int64>, 'tgt_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:4' shape=(400,) dtype=int64>, 'tgt_labels': <tf.Tensor 'ParseSingleExample/ParseSingleExample:5' shape=(400,) dtype=int64>}\n",
            "dict_keys(['src_input_ids', 'src_input_mask', 'src_segment_ids', 'tgt_input_ids', 'tgt_input_mask', 'tgt_labels'])\n",
            "WARNING:tensorflow:From preprocess.py:272: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "{'src_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:0' shape=(512,) dtype=int64>, 'src_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:1' shape=(512,) dtype=int64>, 'src_segment_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:2' shape=(512,) dtype=int64>, 'tgt_input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:3' shape=(400,) dtype=int64>, 'tgt_input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:4' shape=(400,) dtype=int64>, 'tgt_labels': <tf.Tensor 'ParseSingleExample/ParseSingleExample:5' shape=(400,) dtype=int64>}\n",
            "dict_keys(['src_input_ids', 'src_input_mask', 'src_segment_ids', 'tgt_input_ids', 'tgt_input_mask', 'tgt_labels'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0zVVcEXy_pO",
        "outputId": "09134930-d950-4b5b-f192-6e62073b33e6"
      },
      "source": [
        "!python main.py"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "step: 15010, loss: 1.4215\n",
            "step: 15011, loss: 1.3834\n",
            "step: 15012, loss: 1.3857\n",
            "step: 15013, loss: 1.3855\n",
            "step: 15014, loss: 1.3855\n",
            "step: 15015, loss: 1.3957\n",
            "step: 15016, loss: 1.3902\n",
            "step: 15017, loss: 1.3955\n",
            "step: 15018, loss: 1.3866\n",
            "step: 15019, loss: 1.4325\n",
            "step: 15020, loss: 1.3882\n",
            "step: 15021, loss: 1.3862\n",
            "step: 15022, loss: 1.3861\n",
            "step: 15023, loss: 1.3867\n",
            "step: 15024, loss: 1.6165\n",
            "step: 15025, loss: 1.4401\n",
            "step: 15026, loss: 1.3919\n",
            "step: 15027, loss: 1.4187\n",
            "step: 15028, loss: 1.3859\n",
            "step: 15029, loss: 1.4493\n",
            "step: 15030, loss: 1.4082\n",
            "step: 15031, loss: 1.3949\n",
            "step: 15032, loss: 1.4023\n",
            "step: 15033, loss: 1.4015\n",
            "step: 15034, loss: 1.4542\n",
            "step: 15035, loss: 1.3981\n",
            "step: 15036, loss: 1.3970\n",
            "step: 15037, loss: 1.3912\n",
            "step: 15038, loss: 1.4058\n",
            "step: 15039, loss: 1.3929\n",
            "step: 15040, loss: 1.3894\n",
            "step: 15041, loss: 1.3870\n",
            "step: 15042, loss: 1.4280\n",
            "step: 15043, loss: 1.3956\n",
            "step: 15044, loss: 1.3915\n",
            "step: 15045, loss: 1.3883\n",
            "step: 15046, loss: 1.4038\n",
            "step: 15047, loss: 1.3967\n",
            "step: 15048, loss: 1.4117\n",
            "step: 15049, loss: 1.3895\n",
            "step: 15050, loss: 1.4278\n",
            "step: 15051, loss: 1.4021\n",
            "step: 15052, loss: 1.3869\n",
            "step: 15053, loss: 1.3902\n",
            "step: 15054, loss: 1.3917\n",
            "step: 15055, loss: 1.3918\n",
            "step: 15056, loss: 1.3950\n",
            "step: 15057, loss: 1.4041\n",
            "step: 15058, loss: 1.3922\n",
            "step: 15059, loss: 1.3932\n",
            "step: 15060, loss: 1.4026\n",
            "step: 15061, loss: 1.3893\n",
            "step: 15062, loss: 1.3890\n",
            "step: 15063, loss: 1.4130\n",
            "step: 15064, loss: 1.4645\n",
            "step: 15065, loss: 1.4027\n",
            "step: 15066, loss: 1.3899\n",
            "step: 15067, loss: 1.4163\n",
            "step: 15068, loss: 1.3983\n",
            "step: 15069, loss: 1.3916\n",
            "step: 15070, loss: 1.3887\n",
            "step: 15071, loss: 1.3950\n",
            "step: 15072, loss: 1.3974\n",
            "step: 15073, loss: 1.3943\n",
            "step: 15074, loss: 1.3937\n",
            "step: 15075, loss: 1.4145\n",
            "step: 15076, loss: 1.3919\n",
            "step: 15077, loss: 1.3989\n",
            "step: 15078, loss: 1.5190\n",
            "step: 15079, loss: 1.3866\n",
            "step: 15080, loss: 1.3856\n",
            "step: 15081, loss: 1.3919\n",
            "step: 15082, loss: 1.4314\n",
            "step: 15083, loss: 1.3872\n",
            "step: 15084, loss: 1.3956\n",
            "step: 15085, loss: 1.3876\n",
            "step: 15086, loss: 1.4471\n",
            "step: 15087, loss: 1.4245\n",
            "step: 15088, loss: 1.4142\n",
            "step: 15089, loss: 1.4154\n",
            "step: 15090, loss: 1.3851\n",
            "step: 15091, loss: 1.3878\n",
            "step: 15092, loss: 1.3872\n",
            "step: 15093, loss: 1.3926\n",
            "step: 15094, loss: 1.4549\n",
            "step: 15095, loss: 1.4005\n",
            "step: 15096, loss: 1.3862\n",
            "step: 15097, loss: 1.3927\n",
            "step: 15098, loss: 1.3902\n",
            "step: 15099, loss: 1.3956\n",
            "step: 15100, loss: 1.3910\n",
            "step: 15101, loss: 1.3954\n",
            "step: 15102, loss: 1.3875\n",
            "step: 15103, loss: 1.4704\n",
            "step: 15104, loss: 1.4339\n",
            "step: 15105, loss: 1.4007\n",
            "step: 15106, loss: 1.3884\n",
            "step: 15107, loss: 1.3906\n",
            "step: 15108, loss: 1.3907\n",
            "step: 15109, loss: 1.4203\n",
            "step: 15110, loss: 1.3966\n",
            "step: 15111, loss: 1.4036\n",
            "step: 15112, loss: 1.4347\n",
            "step: 15113, loss: 1.3887\n",
            "step: 15114, loss: 1.3940\n",
            "step: 15115, loss: 1.3867\n",
            "step: 15116, loss: 1.3892\n",
            "step: 15117, loss: 1.3871\n",
            "step: 15118, loss: 1.3906\n",
            "step: 15119, loss: 1.3955\n",
            "step: 15120, loss: 1.3948\n",
            "step: 15121, loss: 1.3925\n",
            "step: 15122, loss: 1.3950\n",
            "step: 15123, loss: 1.3920\n",
            "step: 15124, loss: 1.4038\n",
            "step: 15125, loss: 1.3875\n",
            "step: 15126, loss: 1.3885\n",
            "step: 15127, loss: 1.4272\n",
            "step: 15128, loss: 1.3931\n",
            "step: 15129, loss: 1.3863\n",
            "step: 15130, loss: 1.3857\n",
            "step: 15131, loss: 1.3993\n",
            "step: 15132, loss: 1.3890\n",
            "step: 15133, loss: 1.3865\n",
            "step: 15134, loss: 1.3860\n",
            "step: 15135, loss: 1.3960\n",
            "step: 15136, loss: 1.3896\n",
            "step: 15137, loss: 1.3861\n",
            "step: 15138, loss: 1.4009\n",
            "step: 15139, loss: 1.3875\n",
            "step: 15140, loss: 1.3847\n",
            "step: 15141, loss: 1.3913\n",
            "step: 15142, loss: 1.3898\n",
            "step: 15143, loss: 1.4302\n",
            "step: 15144, loss: 1.3865\n",
            "step: 15145, loss: 1.3966\n",
            "step: 15146, loss: 1.3858\n",
            "step: 15147, loss: 1.4224\n",
            "step: 15148, loss: 1.3882\n",
            "step: 15149, loss: 1.3990\n",
            "step: 15150, loss: 1.3854\n",
            "step: 15151, loss: 1.4044\n",
            "step: 15152, loss: 1.3844\n",
            "step: 15153, loss: 1.4006\n",
            "step: 15154, loss: 1.3868\n",
            "step: 15155, loss: 1.4039\n",
            "step: 15156, loss: 1.4064\n",
            "step: 15157, loss: 1.3983\n",
            "step: 15158, loss: 1.4128\n",
            "step: 15159, loss: 1.3870\n",
            "step: 15160, loss: 1.3949\n",
            "step: 15161, loss: 1.3883\n",
            "step: 15162, loss: 1.3952\n",
            "step: 15163, loss: 1.3862\n",
            "step: 15164, loss: 1.4263\n",
            "step: 15165, loss: 1.3870\n",
            "step: 15166, loss: 1.4799\n",
            "step: 15167, loss: 1.3905\n",
            "step: 15168, loss: 1.3885\n",
            "step: 15169, loss: 1.4393\n",
            "step: 15170, loss: 1.3940\n",
            "step: 15171, loss: 1.3888\n",
            "step: 15172, loss: 1.3974\n",
            "step: 15173, loss: 1.3885\n",
            "step: 15174, loss: 1.3863\n",
            "step: 15175, loss: 1.3884\n",
            "step: 15176, loss: 1.3957\n",
            "step: 15177, loss: 1.3859\n",
            "step: 15178, loss: 1.3903\n",
            "step: 15179, loss: 1.3840\n",
            "step: 15180, loss: 1.3912\n",
            "step: 15181, loss: 1.3877\n",
            "step: 15182, loss: 1.4655\n",
            "step: 15183, loss: 1.3900\n",
            "step: 15184, loss: 1.3893\n",
            "step: 15185, loss: 1.4004\n",
            "step: 15186, loss: 1.3888\n",
            "step: 15187, loss: 1.4100\n",
            "step: 15188, loss: 1.3844\n",
            "step: 15189, loss: 1.4174\n",
            "step: 15190, loss: 1.3878\n",
            "step: 15191, loss: 1.4233\n",
            "step: 15192, loss: 1.4029\n",
            "step: 15193, loss: 1.4991\n",
            "step: 15194, loss: 1.4249\n",
            "step: 15195, loss: 1.3847\n",
            "step: 15196, loss: 1.3842\n",
            "step: 15197, loss: 1.4058\n",
            "step: 15198, loss: 1.3938\n",
            "step: 15199, loss: 1.4005\n",
            "step: 15200, loss: 1.4082\n",
            "step: 15201, loss: 1.3844\n",
            "step: 15202, loss: 1.3862\n",
            "step: 15203, loss: 1.3893\n",
            "step: 15204, loss: 1.3981\n",
            "step: 15205, loss: 1.4332\n",
            "step: 15206, loss: 1.3871\n",
            "step: 15207, loss: 1.3907\n",
            "step: 15208, loss: 1.3914\n",
            "step: 15209, loss: 1.3906\n",
            "step: 15210, loss: 1.3909\n",
            "step: 15211, loss: 1.3914\n",
            "step: 15212, loss: 1.3856\n",
            "step: 15213, loss: 1.3956\n",
            "step: 15214, loss: 1.3929\n",
            "step: 15215, loss: 1.3923\n",
            "step: 15216, loss: 1.4123\n",
            "step: 15217, loss: 1.4014\n",
            "step: 15218, loss: 1.3940\n",
            "step: 15219, loss: 1.4050\n",
            "step: 15220, loss: 1.3919\n",
            "step: 15221, loss: 1.3987\n",
            "step: 15222, loss: 1.3997\n",
            "step: 15223, loss: 1.3885\n",
            "step: 15224, loss: 1.3841\n",
            "step: 15225, loss: 1.4347\n",
            "step: 15226, loss: 1.3870\n",
            "step: 15227, loss: 1.3883\n",
            "step: 15228, loss: 1.3951\n",
            "step: 15229, loss: 1.3953\n",
            "step: 15230, loss: 1.3880\n",
            "step: 15231, loss: 1.3872\n",
            "step: 15232, loss: 1.3890\n",
            "step: 15233, loss: 1.3925\n",
            "step: 15234, loss: 1.3841\n",
            "step: 15235, loss: 1.4384\n",
            "step: 15236, loss: 1.4012\n",
            "step: 15237, loss: 1.4109\n",
            "step: 15238, loss: 1.3857\n",
            "step: 15239, loss: 1.4698\n",
            "step: 15240, loss: 1.3903\n",
            "step: 15241, loss: 1.4867\n",
            "step: 15242, loss: 1.3908\n",
            "step: 15243, loss: 1.3993\n",
            "step: 15244, loss: 1.4133\n",
            "step: 15245, loss: 1.3885\n",
            "step: 15246, loss: 1.3864\n",
            "step: 15247, loss: 1.4206\n",
            "step: 15248, loss: 1.3833\n",
            "step: 15249, loss: 1.3880\n",
            "step: 15250, loss: 1.3865\n",
            "step: 15251, loss: 1.3848\n",
            "step: 15252, loss: 1.3884\n",
            "step: 15253, loss: 1.3859\n",
            "step: 15254, loss: 1.4429\n",
            "step: 15255, loss: 1.4627\n",
            "step: 15256, loss: 1.3845\n",
            "step: 15257, loss: 1.3836\n",
            "step: 15258, loss: 1.4273\n",
            "step: 15259, loss: 1.3945\n",
            "step: 15260, loss: 1.3858\n",
            "step: 15261, loss: 1.3841\n",
            "step: 15262, loss: 1.3924\n",
            "step: 15263, loss: 1.3978\n",
            "step: 15264, loss: 1.4168\n",
            "step: 15265, loss: 1.3886\n",
            "step: 15266, loss: 1.4057\n",
            "step: 15267, loss: 1.3848\n",
            "step: 15268, loss: 1.4140\n",
            "step: 15269, loss: 1.3900\n",
            "step: 15270, loss: 1.4089\n",
            "step: 15271, loss: 1.3888\n",
            "step: 15272, loss: 1.3920\n",
            "step: 15273, loss: 1.3879\n",
            "step: 15274, loss: 1.3962\n",
            "step: 15275, loss: 1.3945\n",
            "step: 15276, loss: 1.3935\n",
            "step: 15277, loss: 1.3872\n",
            "step: 15278, loss: 1.3941\n",
            "step: 15279, loss: 1.4295\n",
            "step: 15280, loss: 1.3915\n",
            "step: 15281, loss: 1.4031\n",
            "step: 15282, loss: 1.4033\n",
            "step: 15283, loss: 1.3917\n",
            "step: 15284, loss: 1.3843\n",
            "step: 15285, loss: 1.3926\n",
            "step: 15286, loss: 1.3939\n",
            "step: 15287, loss: 1.4139\n",
            "step: 15288, loss: 1.4860\n",
            "step: 15289, loss: 1.3994\n",
            "step: 15290, loss: 1.3908\n",
            "step: 15291, loss: 1.3918\n",
            "step: 15292, loss: 1.4289\n",
            "step: 15293, loss: 1.3854\n",
            "step: 15294, loss: 1.3881\n",
            "step: 15295, loss: 1.3901\n",
            "step: 15296, loss: 1.3986\n",
            "step: 15297, loss: 1.3906\n",
            "step: 15298, loss: 1.3927\n",
            "step: 15299, loss: 1.3879\n",
            "step: 15300, loss: 1.5135\n",
            "step: 15301, loss: 1.4381\n",
            "step: 15302, loss: 1.4524\n",
            "step: 15303, loss: 1.4149\n",
            "step: 15304, loss: 1.4052\n",
            "step: 15305, loss: 1.3864\n",
            "step: 15306, loss: 1.4735\n",
            "step: 15307, loss: 1.4305\n",
            "step: 15308, loss: 1.4903\n",
            "step: 15309, loss: 1.4266\n",
            "step: 15310, loss: 1.3893\n",
            "step: 15311, loss: 1.3883\n",
            "step: 15312, loss: 1.3862\n",
            "step: 15313, loss: 1.5008\n",
            "step: 15314, loss: 1.3967\n",
            "step: 15315, loss: 1.3916\n",
            "step: 15316, loss: 1.4326\n",
            "step: 15317, loss: 1.4013\n",
            "step: 15318, loss: 1.3932\n",
            "step: 15319, loss: 1.4272\n",
            "step: 15320, loss: 1.3866\n",
            "step: 15321, loss: 1.3860\n",
            "step: 15322, loss: 1.4091\n",
            "step: 15323, loss: 1.3846\n",
            "step: 15324, loss: 1.4457\n",
            "step: 15325, loss: 1.3854\n",
            "step: 15326, loss: 1.3873\n",
            "step: 15327, loss: 1.3894\n",
            "step: 15328, loss: 1.3861\n",
            "step: 15329, loss: 1.4024\n",
            "step: 15330, loss: 1.3882\n",
            "step: 15331, loss: 1.4082\n",
            "step: 15332, loss: 1.3867\n",
            "step: 15333, loss: 1.4381\n",
            "step: 15334, loss: 1.3864\n",
            "step: 15335, loss: 1.3893\n",
            "step: 15336, loss: 1.4279\n",
            "step: 15337, loss: 1.3902\n",
            "step: 15338, loss: 1.3872\n",
            "step: 15339, loss: 1.3860\n",
            "step: 15340, loss: 1.4247\n",
            "step: 15341, loss: 1.3851\n",
            "step: 15342, loss: 1.3876\n",
            "step: 15343, loss: 1.3855\n",
            "step: 15344, loss: 1.4004\n",
            "step: 15345, loss: 1.4948\n",
            "step: 15346, loss: 1.3874\n",
            "step: 15347, loss: 1.4150\n",
            "step: 15348, loss: 1.3993\n",
            "step: 15349, loss: 1.3870\n",
            "step: 15350, loss: 1.3847\n",
            "step: 15351, loss: 1.4118\n",
            "step: 15352, loss: 1.3859\n",
            "step: 15353, loss: 1.3872\n",
            "step: 15354, loss: 1.3862\n",
            "step: 15355, loss: 1.4010\n",
            "step: 15356, loss: 1.3895\n",
            "step: 15357, loss: 1.3997\n",
            "step: 15358, loss: 1.4156\n",
            "step: 15359, loss: 1.3855\n",
            "step: 15360, loss: 1.4406\n",
            "step: 15361, loss: 1.3980\n",
            "step: 15362, loss: 1.3863\n",
            "step: 15363, loss: 1.4071\n",
            "step: 15364, loss: 1.3848\n",
            "step: 15365, loss: 1.3858\n",
            "step: 15366, loss: 1.3844\n",
            "step: 15367, loss: 1.3841\n",
            "step: 15368, loss: 1.4158\n",
            "step: 15369, loss: 1.3842\n",
            "step: 15370, loss: 1.3870\n",
            "step: 15371, loss: 1.4179\n",
            "step: 15372, loss: 1.3882\n",
            "step: 15373, loss: 1.3887\n",
            "step: 15374, loss: 1.4085\n",
            "step: 15375, loss: 1.3899\n",
            "step: 15376, loss: 1.3925\n",
            "step: 15377, loss: 1.3920\n",
            "step: 15378, loss: 1.3903\n",
            "step: 15379, loss: 1.4232\n",
            "step: 15380, loss: 1.3939\n",
            "step: 15381, loss: 1.4056\n",
            "step: 15382, loss: 1.3948\n",
            "step: 15383, loss: 1.3911\n",
            "step: 15384, loss: 1.4071\n",
            "step: 15385, loss: 1.4337\n",
            "step: 15386, loss: 1.4012\n",
            "step: 15387, loss: 1.3869\n",
            "step: 15388, loss: 1.4121\n",
            "step: 15389, loss: 1.4152\n",
            "step: 15390, loss: 1.3950\n",
            "step: 15391, loss: 1.4404\n",
            "step: 15392, loss: 1.3892\n",
            "step: 15393, loss: 1.3862\n",
            "step: 15394, loss: 1.3866\n",
            "step: 15395, loss: 1.3878\n",
            "step: 15396, loss: 1.3838\n",
            "step: 15397, loss: 1.3863\n",
            "step: 15398, loss: 1.4201\n",
            "step: 15399, loss: 1.4158\n",
            "step: 15400, loss: 1.3899\n",
            "step: 15401, loss: 1.4842\n",
            "step: 15402, loss: 1.4320\n",
            "step: 15403, loss: 1.3954\n",
            "step: 15404, loss: 1.4006\n",
            "step: 15405, loss: 1.3878\n",
            "step: 15406, loss: 1.3844\n",
            "step: 15407, loss: 1.3908\n",
            "step: 15408, loss: 1.3874\n",
            "step: 15409, loss: 1.3848\n",
            "step: 15410, loss: 1.3892\n",
            "step: 15411, loss: 1.3947\n",
            "step: 15412, loss: 1.3887\n",
            "step: 15413, loss: 1.3902\n",
            "step: 15414, loss: 1.3918\n",
            "step: 15415, loss: 1.3850\n",
            "step: 15416, loss: 1.3867\n",
            "step: 15417, loss: 1.4232\n",
            "step: 15418, loss: 1.3897\n",
            "step: 15419, loss: 1.3835\n",
            "step: 15420, loss: 1.3865\n",
            "step: 15421, loss: 1.3899\n",
            "step: 15422, loss: 1.3943\n",
            "step: 15423, loss: 1.3886\n",
            "step: 15424, loss: 1.3908\n",
            "step: 15425, loss: 1.4330\n",
            "step: 15426, loss: 1.4107\n",
            "step: 15427, loss: 1.3843\n",
            "step: 15428, loss: 1.4898\n",
            "step: 15429, loss: 1.4106\n",
            "step: 15430, loss: 1.3861\n",
            "step: 15431, loss: 1.3951\n",
            "step: 15432, loss: 1.4059\n",
            "step: 15433, loss: 1.3863\n",
            "step: 15434, loss: 1.3852\n",
            "step: 15435, loss: 1.3839\n",
            "step: 15436, loss: 1.3887\n",
            "step: 15437, loss: 1.4321\n",
            "step: 15438, loss: 1.3880\n",
            "step: 15439, loss: 1.4250\n",
            "step: 15440, loss: 1.3877\n",
            "step: 15441, loss: 1.3847\n",
            "step: 15442, loss: 1.3847\n",
            "step: 15443, loss: 1.3916\n",
            "step: 15444, loss: 1.4058\n",
            "step: 15445, loss: 1.4241\n",
            "step: 15446, loss: 1.4099\n",
            "step: 15447, loss: 1.3835\n",
            "step: 15448, loss: 1.4430\n",
            "step: 15449, loss: 1.3834\n",
            "step: 15450, loss: 1.4067\n",
            "step: 15451, loss: 1.3876\n",
            "step: 15452, loss: 1.4143\n",
            "step: 15453, loss: 1.4298\n",
            "step: 15454, loss: 1.4028\n",
            "step: 15455, loss: 1.3850\n",
            "step: 15456, loss: 1.4214\n",
            "step: 15457, loss: 1.4172\n",
            "step: 15458, loss: 1.3862\n",
            "step: 15459, loss: 1.3862\n",
            "step: 15460, loss: 1.3993\n",
            "step: 15461, loss: 1.4279\n",
            "step: 15462, loss: 1.3944\n",
            "step: 15463, loss: 1.3931\n",
            "step: 15464, loss: 1.4274\n",
            "step: 15465, loss: 1.3884\n",
            "step: 15466, loss: 1.3893\n",
            "step: 15467, loss: 1.4009\n",
            "step: 15468, loss: 1.4215\n",
            "step: 15469, loss: 1.3965\n",
            "step: 15470, loss: 1.4426\n",
            "step: 15471, loss: 1.4036\n",
            "step: 15472, loss: 1.4345\n",
            "step: 15473, loss: 1.4229\n",
            "step: 15474, loss: 1.3904\n",
            "step: 15475, loss: 1.3843\n",
            "step: 15476, loss: 1.4099\n",
            "step: 15477, loss: 1.4263\n",
            "step: 15478, loss: 1.4075\n",
            "step: 15479, loss: 1.3899\n",
            "step: 15480, loss: 1.3835\n",
            "step: 15481, loss: 1.3874\n",
            "step: 15482, loss: 1.4153\n",
            "step: 15483, loss: 1.3830\n",
            "step: 15484, loss: 1.3849\n",
            "step: 15485, loss: 1.3960\n",
            "step: 15486, loss: 1.3861\n",
            "step: 15487, loss: 1.4073\n",
            "step: 15488, loss: 1.3939\n",
            "step: 15489, loss: 1.3840\n",
            "step: 15490, loss: 1.4137\n",
            "step: 15491, loss: 1.3866\n",
            "step: 15492, loss: 1.3995\n",
            "step: 15493, loss: 1.3835\n",
            "step: 15494, loss: 1.3858\n",
            "step: 15495, loss: 1.3914\n",
            "step: 15496, loss: 1.3869\n",
            "step: 15497, loss: 1.3950\n",
            "step: 15498, loss: 1.4027\n",
            "step: 15499, loss: 1.4685\n",
            "step: 15500, loss: 1.3866\n",
            "step: 15501, loss: 1.4700\n",
            "step: 15502, loss: 1.4100\n",
            "step: 15503, loss: 1.3862\n",
            "step: 15504, loss: 1.3882\n",
            "step: 15505, loss: 1.4024\n",
            "step: 15506, loss: 1.3923\n",
            "step: 15507, loss: 1.4586\n",
            "step: 15508, loss: 1.3849\n",
            "step: 15509, loss: 1.3920\n",
            "step: 15510, loss: 1.5299\n",
            "step: 15511, loss: 1.3873\n",
            "step: 15512, loss: 1.3889\n",
            "step: 15513, loss: 1.3897\n",
            "step: 15514, loss: 1.4543\n",
            "step: 15515, loss: 1.3945\n",
            "step: 15516, loss: 1.3921\n",
            "step: 15517, loss: 1.3928\n",
            "step: 15518, loss: 1.3913\n",
            "step: 15519, loss: 1.3939\n",
            "step: 15520, loss: 1.3853\n",
            "step: 15521, loss: 1.3994\n",
            "step: 15522, loss: 1.3864\n",
            "step: 15523, loss: 1.4528\n",
            "step: 15524, loss: 1.4234\n",
            "step: 15525, loss: 1.3900\n",
            "step: 15526, loss: 1.3840\n",
            "step: 15527, loss: 1.3850\n",
            "step: 15528, loss: 1.3859\n",
            "step: 15529, loss: 1.3871\n",
            "step: 15530, loss: 1.4144\n",
            "step: 15531, loss: 1.3889\n",
            "step: 15532, loss: 1.3857\n",
            "step: 15533, loss: 1.4562\n",
            "step: 15534, loss: 1.3854\n",
            "step: 15535, loss: 1.3843\n",
            "step: 15536, loss: 1.3947\n",
            "step: 15537, loss: 1.3872\n",
            "step: 15538, loss: 1.4181\n",
            "step: 15539, loss: 1.4250\n",
            "step: 15540, loss: 1.4162\n",
            "step: 15541, loss: 1.4163\n",
            "step: 15542, loss: 1.3932\n",
            "step: 15543, loss: 1.4041\n",
            "step: 15544, loss: 1.4115\n",
            "step: 15545, loss: 1.3900\n",
            "step: 15546, loss: 1.3967\n",
            "step: 15547, loss: 1.4290\n",
            "step: 15548, loss: 1.4943\n",
            "step: 15549, loss: 1.5014\n",
            "step: 15550, loss: 1.3908\n",
            "step: 15551, loss: 1.3980\n",
            "step: 15552, loss: 1.3890\n",
            "step: 15553, loss: 1.4056\n",
            "step: 15554, loss: 1.3934\n",
            "step: 15555, loss: 1.4124\n",
            "step: 15556, loss: 1.4592\n",
            "step: 15557, loss: 1.3905\n",
            "step: 15558, loss: 1.3966\n",
            "step: 15559, loss: 1.3900\n",
            "step: 15560, loss: 1.3888\n",
            "step: 15561, loss: 1.3869\n",
            "step: 15562, loss: 1.3998\n",
            "step: 15563, loss: 1.3893\n",
            "step: 15564, loss: 1.3878\n",
            "step: 15565, loss: 1.3883\n",
            "step: 15566, loss: 1.3888\n",
            "step: 15567, loss: 1.3865\n",
            "step: 15568, loss: 1.4131\n",
            "step: 15569, loss: 1.5187\n",
            "step: 15570, loss: 1.3921\n",
            "step: 15571, loss: 1.3933\n",
            "step: 15572, loss: 1.3880\n",
            "step: 15573, loss: 1.4027\n",
            "step: 15574, loss: 1.3869\n",
            "step: 15575, loss: 1.3921\n",
            "step: 15576, loss: 1.3856\n",
            "step: 15577, loss: 1.4085\n",
            "step: 15578, loss: 1.4623\n",
            "step: 15579, loss: 1.4500\n",
            "step: 15580, loss: 1.3945\n",
            "step: 15581, loss: 1.3869\n",
            "step: 15582, loss: 1.3882\n",
            "step: 15583, loss: 1.3919\n",
            "step: 15584, loss: 1.4413\n",
            "step: 15585, loss: 1.4028\n",
            "step: 15586, loss: 1.3878\n",
            "step: 15587, loss: 1.3952\n",
            "step: 15588, loss: 1.3946\n",
            "step: 15589, loss: 1.4078\n",
            "step: 15590, loss: 1.4896\n",
            "step: 15591, loss: 1.3887\n",
            "step: 15592, loss: 1.4230\n",
            "step: 15593, loss: 1.3866\n",
            "step: 15594, loss: 1.3886\n",
            "step: 15595, loss: 1.3929\n",
            "step: 15596, loss: 1.4007\n",
            "step: 15597, loss: 1.3894\n",
            "step: 15598, loss: 1.3874\n",
            "step: 15599, loss: 1.3913\n",
            "step: 15600, loss: 1.4171\n",
            "step: 15601, loss: 1.4005\n",
            "step: 15602, loss: 1.3868\n",
            "step: 15603, loss: 1.4470\n",
            "step: 15604, loss: 1.3864\n",
            "step: 15605, loss: 1.4073\n",
            "step: 15606, loss: 1.4144\n",
            "step: 15607, loss: 1.3873\n",
            "step: 15608, loss: 1.3867\n",
            "step: 15609, loss: 1.3895\n",
            "step: 15610, loss: 1.4047\n",
            "step: 15611, loss: 1.4694\n",
            "step: 15612, loss: 1.3896\n",
            "step: 15613, loss: 1.4209\n",
            "step: 15614, loss: 1.3865\n",
            "step: 15615, loss: 1.3912\n",
            "step: 15616, loss: 1.3876\n",
            "step: 15617, loss: 1.3864\n",
            "step: 15618, loss: 1.4199\n",
            "step: 15619, loss: 1.4012\n",
            "step: 15620, loss: 1.3978\n",
            "step: 15621, loss: 1.3885\n",
            "step: 15622, loss: 1.3861\n",
            "step: 15623, loss: 1.3903\n",
            "step: 15624, loss: 1.3849\n",
            "step: 15625, loss: 1.4125\n",
            "step: 15626, loss: 1.3871\n",
            "step: 15627, loss: 1.3913\n",
            "step: 15628, loss: 1.3853\n",
            "step: 15629, loss: 1.4228\n",
            "step: 15630, loss: 1.4159\n",
            "step: 15631, loss: 1.4372\n",
            "step: 15632, loss: 1.3877\n",
            "step: 15633, loss: 1.3891\n",
            "step: 15634, loss: 1.3914\n",
            "step: 15635, loss: 1.3923\n",
            "step: 15636, loss: 1.3928\n",
            "step: 15637, loss: 1.3846\n",
            "step: 15638, loss: 1.3869\n",
            "step: 15639, loss: 1.4101\n",
            "step: 15640, loss: 1.3854\n",
            "step: 15641, loss: 1.3842\n",
            "step: 15642, loss: 1.3861\n",
            "step: 15643, loss: 1.4253\n",
            "step: 15644, loss: 1.4048\n",
            "step: 15645, loss: 1.4055\n",
            "step: 15646, loss: 1.4245\n",
            "step: 15647, loss: 1.3854\n",
            "step: 15648, loss: 1.4028\n",
            "step: 15649, loss: 1.4234\n",
            "step: 15650, loss: 1.4000\n",
            "step: 15651, loss: 1.3831\n",
            "step: 15652, loss: 1.3864\n",
            "step: 15653, loss: 1.3888\n",
            "step: 15654, loss: 1.4341\n",
            "step: 15655, loss: 1.3853\n",
            "step: 15656, loss: 1.4252\n",
            "step: 15657, loss: 1.3857\n",
            "step: 15658, loss: 1.4413\n",
            "step: 15659, loss: 1.3897\n",
            "step: 15660, loss: 1.3847\n",
            "step: 15661, loss: 1.3936\n",
            "step: 15662, loss: 1.4075\n",
            "step: 15663, loss: 1.3932\n",
            "step: 15664, loss: 1.4132\n",
            "step: 15665, loss: 1.3871\n",
            "step: 15666, loss: 1.4117\n",
            "step: 15667, loss: 1.3846\n",
            "step: 15668, loss: 1.4195\n",
            "step: 15669, loss: 1.3889\n",
            "step: 15670, loss: 1.3841\n",
            "step: 15671, loss: 1.3847\n",
            "step: 15672, loss: 1.3864\n",
            "step: 15673, loss: 1.4207\n",
            "step: 15674, loss: 1.3940\n",
            "step: 15675, loss: 1.4063\n",
            "step: 15676, loss: 1.3847\n",
            "step: 15677, loss: 1.4743\n",
            "step: 15678, loss: 1.4105\n",
            "step: 15679, loss: 1.4082\n",
            "step: 15680, loss: 1.3859\n",
            "step: 15681, loss: 1.3862\n",
            "step: 15682, loss: 1.3904\n",
            "step: 15683, loss: 1.4404\n",
            "step: 15684, loss: 1.3892\n",
            "step: 15685, loss: 1.4070\n",
            "step: 15686, loss: 1.3857\n",
            "step: 15687, loss: 1.3848\n",
            "step: 15688, loss: 1.3920\n",
            "step: 15689, loss: 1.3853\n",
            "step: 15690, loss: 1.3878\n",
            "step: 15691, loss: 1.3917\n",
            "step: 15692, loss: 1.4392\n",
            "step: 15693, loss: 1.3840\n",
            "step: 15694, loss: 1.3882\n",
            "step: 15695, loss: 1.3860\n",
            "step: 15696, loss: 1.4816\n",
            "step: 15697, loss: 1.3866\n",
            "step: 15698, loss: 1.3860\n",
            "step: 15699, loss: 1.3840\n",
            "step: 15700, loss: 1.3945\n",
            "step: 15701, loss: 1.4337\n",
            "step: 15702, loss: 1.3932\n",
            "step: 15703, loss: 1.3945\n",
            "step: 15704, loss: 1.3971\n",
            "step: 15705, loss: 1.3875\n",
            "step: 15706, loss: 1.4236\n",
            "step: 15707, loss: 1.3881\n",
            "step: 15708, loss: 1.3867\n",
            "step: 15709, loss: 1.3869\n",
            "step: 15710, loss: 1.4053\n",
            "step: 15711, loss: 1.3992\n",
            "step: 15712, loss: 1.4652\n",
            "step: 15713, loss: 1.3852\n",
            "step: 15714, loss: 1.3854\n",
            "step: 15715, loss: 1.3859\n",
            "step: 15716, loss: 1.3862\n",
            "step: 15717, loss: 1.3920\n",
            "step: 15718, loss: 1.3931\n",
            "step: 15719, loss: 1.4223\n",
            "step: 15720, loss: 1.3874\n",
            "step: 15721, loss: 1.3877\n",
            "step: 15722, loss: 1.3895\n",
            "step: 15723, loss: 1.4220\n",
            "step: 15724, loss: 1.3829\n",
            "step: 15725, loss: 1.4589\n",
            "step: 15726, loss: 1.3930\n",
            "step: 15727, loss: 1.3962\n",
            "step: 15728, loss: 1.4419\n",
            "step: 15729, loss: 1.4009\n",
            "step: 15730, loss: 1.3993\n",
            "step: 15731, loss: 1.3912\n",
            "step: 15732, loss: 1.4227\n",
            "step: 15733, loss: 1.3889\n",
            "step: 15734, loss: 1.4029\n",
            "step: 15735, loss: 1.3847\n",
            "step: 15736, loss: 1.3959\n",
            "step: 15737, loss: 1.4004\n",
            "step: 15738, loss: 1.3891\n",
            "step: 15739, loss: 1.4269\n",
            "step: 15740, loss: 1.3896\n",
            "step: 15741, loss: 1.3879\n",
            "step: 15742, loss: 1.3866\n",
            "step: 15743, loss: 1.3896\n",
            "step: 15744, loss: 1.4313\n",
            "step: 15745, loss: 1.3887\n",
            "step: 15746, loss: 1.3846\n",
            "step: 15747, loss: 1.3933\n",
            "step: 15748, loss: 1.4181\n",
            "step: 15749, loss: 1.3836\n",
            "step: 15750, loss: 1.4059\n",
            "step: 15751, loss: 1.4593\n",
            "step: 15752, loss: 1.4108\n",
            "step: 15753, loss: 1.3861\n",
            "step: 15754, loss: 1.3864\n",
            "step: 15755, loss: 1.3836\n",
            "step: 15756, loss: 1.3839\n",
            "step: 15757, loss: 1.3992\n",
            "step: 15758, loss: 1.3940\n",
            "step: 15759, loss: 1.3895\n",
            "step: 15760, loss: 1.3957\n",
            "step: 15761, loss: 1.3922\n",
            "step: 15762, loss: 1.4328\n",
            "step: 15763, loss: 1.3895\n",
            "step: 15764, loss: 1.3862\n",
            "step: 15765, loss: 1.3826\n",
            "step: 15766, loss: 1.3911\n",
            "step: 15767, loss: 1.4094\n",
            "step: 15768, loss: 1.3871\n",
            "step: 15769, loss: 1.3867\n",
            "step: 15770, loss: 1.3866\n",
            "step: 15771, loss: 1.3840\n",
            "step: 15772, loss: 1.3884\n",
            "step: 15773, loss: 1.3934\n",
            "step: 15774, loss: 1.3826\n",
            "step: 15775, loss: 1.3841\n",
            "step: 15776, loss: 1.3871\n",
            "step: 15777, loss: 1.3883\n",
            "step: 15778, loss: 1.3912\n",
            "step: 15779, loss: 1.4210\n",
            "step: 15780, loss: 1.4649\n",
            "step: 15781, loss: 1.4137\n",
            "step: 15782, loss: 1.3860\n",
            "step: 15783, loss: 1.3865\n",
            "step: 15784, loss: 1.3852\n",
            "step: 15785, loss: 1.3911\n",
            "step: 15786, loss: 1.3888\n",
            "step: 15787, loss: 1.4104\n",
            "step: 15788, loss: 1.3910\n",
            "step: 15789, loss: 1.3996\n",
            "step: 15790, loss: 1.3874\n",
            "step: 15791, loss: 1.3938\n",
            "step: 15792, loss: 1.4012\n",
            "step: 15793, loss: 1.3869\n",
            "step: 15794, loss: 1.3884\n",
            "step: 15795, loss: 1.3870\n",
            "step: 15796, loss: 1.3908\n",
            "step: 15797, loss: 1.3846\n",
            "step: 15798, loss: 1.3825\n",
            "step: 15799, loss: 1.3850\n",
            "step: 15800, loss: 1.3847\n",
            "step: 15801, loss: 1.3850\n",
            "step: 15802, loss: 1.3855\n",
            "step: 15803, loss: 1.3830\n",
            "step: 15804, loss: 1.3842\n",
            "step: 15805, loss: 1.3852\n",
            "step: 15806, loss: 1.4201\n",
            "step: 15807, loss: 1.3908\n",
            "step: 15808, loss: 1.4977\n",
            "step: 15809, loss: 1.3880\n",
            "step: 15810, loss: 1.3843\n",
            "step: 15811, loss: 1.4129\n",
            "step: 15812, loss: 1.3955\n",
            "step: 15813, loss: 1.4332\n",
            "step: 15814, loss: 1.3841\n",
            "step: 15815, loss: 1.3939\n",
            "step: 15816, loss: 1.3881\n",
            "step: 15817, loss: 1.3854\n",
            "step: 15818, loss: 1.3887\n",
            "step: 15819, loss: 1.3859\n",
            "step: 15820, loss: 1.3897\n",
            "step: 15821, loss: 1.4275\n",
            "step: 15822, loss: 1.3864\n",
            "step: 15823, loss: 1.4197\n",
            "step: 15824, loss: 1.3830\n",
            "step: 15825, loss: 1.4021\n",
            "step: 15826, loss: 1.3885\n",
            "step: 15827, loss: 1.3862\n",
            "step: 15828, loss: 1.4075\n",
            "step: 15829, loss: 1.3859\n",
            "step: 15830, loss: 1.3944\n",
            "step: 15831, loss: 1.3924\n",
            "step: 15832, loss: 1.3856\n",
            "step: 15833, loss: 1.3944\n",
            "step: 15834, loss: 1.3952\n",
            "step: 15835, loss: 1.4239\n",
            "step: 15836, loss: 1.3876\n",
            "step: 15837, loss: 1.3866\n",
            "step: 15838, loss: 1.3840\n",
            "step: 15839, loss: 1.3834\n",
            "step: 15840, loss: 1.3862\n",
            "step: 15841, loss: 1.3861\n",
            "step: 15842, loss: 1.3997\n",
            "step: 15843, loss: 1.3848\n",
            "step: 15844, loss: 1.3918\n",
            "step: 15845, loss: 1.3848\n",
            "step: 15846, loss: 1.4446\n",
            "step: 15847, loss: 1.3885\n",
            "step: 15848, loss: 1.3857\n",
            "step: 15849, loss: 1.3885\n",
            "step: 15850, loss: 1.3855\n",
            "step: 15851, loss: 1.4100\n",
            "step: 15852, loss: 1.3912\n",
            "step: 15853, loss: 1.4230\n",
            "step: 15854, loss: 1.3932\n",
            "step: 15855, loss: 1.4456\n",
            "step: 15856, loss: 1.4118\n",
            "step: 15857, loss: 1.3920\n",
            "step: 15858, loss: 1.4402\n",
            "step: 15859, loss: 1.3851\n",
            "step: 15860, loss: 1.4497\n",
            "step: 15861, loss: 1.4046\n",
            "step: 15862, loss: 1.3965\n",
            "step: 15863, loss: 1.4373\n",
            "step: 15864, loss: 1.4346\n",
            "step: 15865, loss: 1.3927\n",
            "step: 15866, loss: 1.4005\n",
            "step: 15867, loss: 1.3945\n",
            "step: 15868, loss: 1.3992\n",
            "step: 15869, loss: 1.3884\n",
            "step: 15870, loss: 1.3886\n",
            "step: 15871, loss: 1.3887\n",
            "step: 15872, loss: 1.3984\n",
            "step: 15873, loss: 1.4088\n",
            "step: 15874, loss: 1.3974\n",
            "step: 15875, loss: 1.4031\n",
            "step: 15876, loss: 1.3950\n",
            "step: 15877, loss: 1.4298\n",
            "step: 15878, loss: 1.4081\n",
            "step: 15879, loss: 1.3850\n",
            "step: 15880, loss: 1.3928\n",
            "step: 15881, loss: 1.3961\n",
            "step: 15882, loss: 1.3934\n",
            "step: 15883, loss: 1.3877\n",
            "step: 15884, loss: 1.3859\n",
            "step: 15885, loss: 1.3870\n",
            "step: 15886, loss: 1.3892\n",
            "step: 15887, loss: 1.4238\n",
            "step: 15888, loss: 1.4386\n",
            "step: 15889, loss: 1.3942\n",
            "step: 15890, loss: 1.3896\n",
            "step: 15891, loss: 1.3924\n",
            "step: 15892, loss: 1.3980\n",
            "step: 15893, loss: 1.3908\n",
            "step: 15894, loss: 1.3896\n",
            "step: 15895, loss: 1.4074\n",
            "step: 15896, loss: 1.4097\n",
            "step: 15897, loss: 1.4057\n",
            "step: 15898, loss: 1.3859\n",
            "step: 15899, loss: 1.3849\n",
            "step: 15900, loss: 1.4040\n",
            "step: 15901, loss: 1.3865\n",
            "step: 15902, loss: 1.4382\n",
            "step: 15903, loss: 1.3976\n",
            "step: 15904, loss: 1.3842\n",
            "step: 15905, loss: 1.4037\n",
            "step: 15906, loss: 1.3879\n",
            "step: 15907, loss: 1.3914\n",
            "step: 15908, loss: 1.4065\n",
            "step: 15909, loss: 1.3902\n",
            "step: 15910, loss: 1.4384\n",
            "step: 15911, loss: 1.3950\n",
            "step: 15912, loss: 1.4000\n",
            "step: 15913, loss: 1.3913\n",
            "step: 15914, loss: 1.3858\n",
            "step: 15915, loss: 1.3895\n",
            "step: 15916, loss: 1.4459\n",
            "step: 15917, loss: 1.4127\n",
            "step: 15918, loss: 1.3848\n",
            "step: 15919, loss: 1.3896\n",
            "step: 15920, loss: 1.3858\n",
            "step: 15921, loss: 1.4022\n",
            "step: 15922, loss: 1.3885\n",
            "step: 15923, loss: 1.4353\n",
            "step: 15924, loss: 1.3855\n",
            "step: 15925, loss: 1.3856\n",
            "step: 15926, loss: 1.3901\n",
            "step: 15927, loss: 1.3887\n",
            "step: 15928, loss: 1.3838\n",
            "step: 15929, loss: 1.3938\n",
            "step: 15930, loss: 1.3871\n",
            "step: 15931, loss: 1.3853\n",
            "step: 15932, loss: 1.4294\n",
            "step: 15933, loss: 1.3915\n",
            "step: 15934, loss: 1.4217\n",
            "step: 15935, loss: 1.4202\n",
            "step: 15936, loss: 1.3854\n",
            "step: 15937, loss: 1.3848\n",
            "step: 15938, loss: 1.3863\n",
            "step: 15939, loss: 1.4205\n",
            "step: 15940, loss: 1.3859\n",
            "step: 15941, loss: 1.3897\n",
            "step: 15942, loss: 1.3898\n",
            "step: 15943, loss: 1.3861\n",
            "step: 15944, loss: 1.3925\n",
            "step: 15945, loss: 1.3942\n",
            "step: 15946, loss: 1.3963\n",
            "step: 15947, loss: 1.3885\n",
            "step: 15948, loss: 1.3925\n",
            "step: 15949, loss: 1.4532\n",
            "step: 15950, loss: 1.3991\n",
            "step: 15951, loss: 1.3848\n",
            "step: 15952, loss: 1.4336\n",
            "step: 15953, loss: 1.3882\n",
            "step: 15954, loss: 1.3880\n",
            "step: 15955, loss: 1.3839\n",
            "step: 15956, loss: 1.3843\n",
            "step: 15957, loss: 1.3908\n",
            "step: 15958, loss: 1.3941\n",
            "step: 15959, loss: 1.4529\n",
            "step: 15960, loss: 1.3939\n",
            "step: 15961, loss: 1.3847\n",
            "step: 15962, loss: 1.3885\n",
            "step: 15963, loss: 1.4083\n",
            "step: 15964, loss: 1.4609\n",
            "step: 15965, loss: 1.4179\n",
            "step: 15966, loss: 1.4279\n",
            "step: 15967, loss: 1.3907\n",
            "step: 15968, loss: 1.3836\n",
            "step: 15969, loss: 1.3848\n",
            "step: 15970, loss: 1.4110\n",
            "step: 15971, loss: 1.4210\n",
            "step: 15972, loss: 1.3882\n",
            "step: 15973, loss: 1.4629\n",
            "step: 15974, loss: 1.3872\n",
            "step: 15975, loss: 1.4509\n",
            "step: 15976, loss: 1.4089\n",
            "step: 15977, loss: 1.3899\n",
            "step: 15978, loss: 1.3851\n",
            "step: 15979, loss: 1.4711\n",
            "step: 15980, loss: 1.3858\n",
            "step: 15981, loss: 1.3866\n",
            "step: 15982, loss: 1.3875\n",
            "step: 15983, loss: 1.3855\n",
            "step: 15984, loss: 1.4309\n",
            "step: 15985, loss: 1.4133\n",
            "step: 15986, loss: 1.3833\n",
            "step: 15987, loss: 1.3835\n",
            "step: 15988, loss: 1.3923\n",
            "step: 15989, loss: 1.3863\n",
            "step: 15990, loss: 1.3899\n",
            "step: 15991, loss: 1.3835\n",
            "step: 15992, loss: 1.3881\n",
            "step: 15993, loss: 1.3991\n",
            "step: 15994, loss: 1.3855\n",
            "step: 15995, loss: 1.3964\n",
            "step: 15996, loss: 1.4096\n",
            "step: 15997, loss: 1.4782\n",
            "step: 15998, loss: 1.3852\n",
            "step: 15999, loss: 1.3988\n",
            "step: 16000, loss: 1.3957\n",
            "saving model to ./models/model_16000.ckpt\n",
            "Batch 0\n",
            "epoch: 0, eval_bleu 0.0000\n",
            "step: 16001, loss: 1.4119\n",
            "step: 16002, loss: 1.3827\n",
            "step: 16003, loss: 1.3860\n",
            "step: 16004, loss: 1.3970\n",
            "step: 16005, loss: 1.3889\n",
            "step: 16006, loss: 1.3882\n",
            "step: 16007, loss: 1.4210\n",
            "step: 16008, loss: 1.4084\n",
            "step: 16009, loss: 1.4586\n",
            "step: 16010, loss: 1.3994\n",
            "step: 16011, loss: 1.3859\n",
            "step: 16012, loss: 1.4092\n",
            "step: 16013, loss: 1.3894\n",
            "step: 16014, loss: 1.4000\n",
            "step: 16015, loss: 1.3910\n",
            "step: 16016, loss: 1.3864\n",
            "step: 16017, loss: 1.3841\n",
            "step: 16018, loss: 1.3829\n",
            "step: 16019, loss: 1.3843\n",
            "step: 16020, loss: 1.3901\n",
            "step: 16021, loss: 1.4073\n",
            "step: 16022, loss: 1.3990\n",
            "step: 16023, loss: 1.3874\n",
            "step: 16024, loss: 1.4686\n",
            "step: 16025, loss: 1.3863\n",
            "step: 16026, loss: 1.3841\n",
            "step: 16027, loss: 1.3846\n",
            "step: 16028, loss: 1.3944\n",
            "step: 16029, loss: 1.4079\n",
            "step: 16030, loss: 1.3919\n",
            "step: 16031, loss: 1.3864\n",
            "step: 16032, loss: 1.4257\n",
            "step: 16033, loss: 1.3857\n",
            "step: 16034, loss: 1.3917\n",
            "step: 16035, loss: 1.3893\n",
            "step: 16036, loss: 1.3920\n",
            "step: 16037, loss: 1.4039\n",
            "step: 16038, loss: 1.3849\n",
            "step: 16039, loss: 1.4382\n",
            "step: 16040, loss: 1.4383\n",
            "step: 16041, loss: 1.4732\n",
            "step: 16042, loss: 1.3865\n",
            "step: 16043, loss: 1.3913\n",
            "step: 16044, loss: 1.3964\n",
            "step: 16045, loss: 1.3910\n",
            "step: 16046, loss: 1.4216\n",
            "step: 16047, loss: 1.3842\n",
            "step: 16048, loss: 1.4061\n",
            "step: 16049, loss: 1.3837\n",
            "step: 16050, loss: 1.3879\n",
            "step: 16051, loss: 1.4078\n",
            "step: 16052, loss: 1.4017\n",
            "step: 16053, loss: 1.3886\n",
            "step: 16054, loss: 1.3888\n",
            "step: 16055, loss: 1.4741\n",
            "step: 16056, loss: 1.3850\n",
            "step: 16057, loss: 1.4040\n",
            "step: 16058, loss: 1.3960\n",
            "step: 16059, loss: 1.4144\n",
            "step: 16060, loss: 1.3960\n",
            "step: 16061, loss: 1.3844\n",
            "step: 16062, loss: 1.3838\n",
            "step: 16063, loss: 1.3884\n",
            "step: 16064, loss: 1.3851\n",
            "step: 16065, loss: 1.3870\n",
            "step: 16066, loss: 1.4000\n",
            "step: 16067, loss: 1.3926\n",
            "step: 16068, loss: 1.4316\n",
            "step: 16069, loss: 1.3874\n",
            "step: 16070, loss: 1.3993\n",
            "step: 16071, loss: 1.4505\n",
            "step: 16072, loss: 1.3965\n",
            "step: 16073, loss: 1.4081\n",
            "step: 16074, loss: 1.3832\n",
            "step: 16075, loss: 1.3833\n",
            "step: 16076, loss: 1.4276\n",
            "step: 16077, loss: 1.3837\n",
            "step: 16078, loss: 1.4795\n",
            "step: 16079, loss: 1.4024\n",
            "step: 16080, loss: 1.4299\n",
            "step: 16081, loss: 1.4542\n",
            "step: 16082, loss: 1.4019\n",
            "step: 16083, loss: 1.3895\n",
            "step: 16084, loss: 1.4094\n",
            "step: 16085, loss: 1.4175\n",
            "step: 16086, loss: 1.3996\n",
            "step: 16087, loss: 1.3857\n",
            "step: 16088, loss: 1.3870\n",
            "step: 16089, loss: 1.4489\n",
            "step: 16090, loss: 1.3867\n",
            "step: 16091, loss: 1.3998\n",
            "step: 16092, loss: 1.3987\n",
            "step: 16093, loss: 1.3859\n",
            "step: 16094, loss: 1.3850\n",
            "step: 16095, loss: 1.4018\n",
            "step: 16096, loss: 1.4090\n",
            "step: 16097, loss: 1.3861\n",
            "step: 16098, loss: 1.3848\n",
            "step: 16099, loss: 1.3845\n",
            "step: 16100, loss: 1.4170\n",
            "step: 16101, loss: 1.4111\n",
            "step: 16102, loss: 1.3845\n",
            "step: 16103, loss: 1.3854\n",
            "step: 16104, loss: 1.4018\n",
            "step: 16105, loss: 1.4008\n",
            "step: 16106, loss: 1.4084\n",
            "step: 16107, loss: 1.4248\n",
            "step: 16108, loss: 1.3846\n",
            "step: 16109, loss: 1.4137\n",
            "step: 16110, loss: 1.4062\n",
            "step: 16111, loss: 1.4035\n",
            "step: 16112, loss: 1.3882\n",
            "step: 16113, loss: 1.3842\n",
            "step: 16114, loss: 1.3844\n",
            "step: 16115, loss: 1.3843\n",
            "step: 16116, loss: 1.4215\n",
            "step: 16117, loss: 1.3859\n",
            "step: 16118, loss: 1.4232\n",
            "step: 16119, loss: 1.3865\n",
            "step: 16120, loss: 1.3865\n",
            "step: 16121, loss: 1.4022\n",
            "step: 16122, loss: 1.3842\n",
            "step: 16123, loss: 1.4062\n",
            "step: 16124, loss: 1.3848\n",
            "step: 16125, loss: 1.3844\n",
            "step: 16126, loss: 1.3918\n",
            "step: 16127, loss: 1.3840\n",
            "step: 16128, loss: 1.3869\n",
            "step: 16129, loss: 1.4010\n",
            "step: 16130, loss: 1.3832\n",
            "step: 16131, loss: 1.3893\n",
            "step: 16132, loss: 1.3908\n",
            "step: 16133, loss: 1.3848\n",
            "step: 16134, loss: 1.4501\n",
            "step: 16135, loss: 1.4058\n",
            "step: 16136, loss: 1.3846\n",
            "step: 16137, loss: 1.3837\n",
            "step: 16138, loss: 1.3977\n",
            "step: 16139, loss: 1.3920\n",
            "step: 16140, loss: 1.3893\n",
            "step: 16141, loss: 1.3987\n",
            "step: 16142, loss: 1.4671\n",
            "step: 16143, loss: 1.3936\n",
            "step: 16144, loss: 1.3851\n",
            "step: 16145, loss: 1.4322\n",
            "step: 16146, loss: 1.3829\n",
            "step: 16147, loss: 1.3941\n",
            "step: 16148, loss: 1.3845\n",
            "step: 16149, loss: 1.3930\n",
            "step: 16150, loss: 1.3844\n",
            "step: 16151, loss: 1.3853\n",
            "step: 16152, loss: 1.4109\n",
            "step: 16153, loss: 1.3837\n",
            "step: 16154, loss: 1.3828\n",
            "step: 16155, loss: 1.4016\n",
            "step: 16156, loss: 1.3842\n",
            "step: 16157, loss: 1.3866\n",
            "step: 16158, loss: 1.3923\n",
            "step: 16159, loss: 1.3999\n",
            "step: 16160, loss: 1.4099\n",
            "step: 16161, loss: 1.3901\n",
            "step: 16162, loss: 1.3996\n",
            "step: 16163, loss: 1.3950\n",
            "step: 16164, loss: 1.3814\n",
            "step: 16165, loss: 1.3855\n",
            "step: 16166, loss: 1.3820\n",
            "step: 16167, loss: 1.3859\n",
            "step: 16168, loss: 1.3832\n",
            "step: 16169, loss: 1.3858\n",
            "step: 16170, loss: 1.4260\n",
            "step: 16171, loss: 1.3997\n",
            "step: 16172, loss: 1.4641\n",
            "step: 16173, loss: 1.3848\n",
            "step: 16174, loss: 1.4005\n",
            "step: 16175, loss: 1.4595\n",
            "step: 16176, loss: 1.3929\n",
            "step: 16177, loss: 1.3999\n",
            "step: 16178, loss: 1.3835\n",
            "step: 16179, loss: 1.3872\n",
            "step: 16180, loss: 1.4226\n",
            "step: 16181, loss: 1.3824\n",
            "step: 16182, loss: 1.3844\n",
            "step: 16183, loss: 1.3851\n",
            "step: 16184, loss: 1.3954\n",
            "step: 16185, loss: 1.3938\n",
            "step: 16186, loss: 1.4846\n",
            "step: 16187, loss: 1.4467\n",
            "step: 16188, loss: 1.4108\n",
            "step: 16189, loss: 1.3889\n",
            "step: 16190, loss: 1.4323\n",
            "step: 16191, loss: 1.3853\n",
            "step: 16192, loss: 1.3977\n",
            "step: 16193, loss: 1.4029\n",
            "step: 16194, loss: 1.4638\n",
            "step: 16195, loss: 1.4101\n",
            "step: 16196, loss: 1.3843\n",
            "step: 16197, loss: 1.3974\n",
            "step: 16198, loss: 1.3844\n",
            "step: 16199, loss: 1.4015\n",
            "step: 16200, loss: 1.4522\n",
            "step: 16201, loss: 1.3857\n",
            "step: 16202, loss: 1.3901\n",
            "step: 16203, loss: 1.3840\n",
            "step: 16204, loss: 1.4235\n",
            "step: 16205, loss: 1.3889\n",
            "step: 16206, loss: 1.3918\n",
            "step: 16207, loss: 1.3863\n",
            "step: 16208, loss: 1.4349\n",
            "step: 16209, loss: 1.3856\n",
            "step: 16210, loss: 1.3883\n",
            "step: 16211, loss: 1.3995\n",
            "step: 16212, loss: 1.3896\n",
            "step: 16213, loss: 1.4030\n",
            "step: 16214, loss: 1.3866\n",
            "step: 16215, loss: 1.3916\n",
            "step: 16216, loss: 1.3824\n",
            "step: 16217, loss: 1.3879\n",
            "step: 16218, loss: 1.3846\n",
            "step: 16219, loss: 1.3863\n",
            "step: 16220, loss: 1.3859\n",
            "step: 16221, loss: 1.4532\n",
            "step: 16222, loss: 1.3939\n",
            "step: 16223, loss: 1.3890\n",
            "step: 16224, loss: 1.3994\n",
            "step: 16225, loss: 1.3842\n",
            "step: 16226, loss: 1.3841\n",
            "step: 16227, loss: 1.3893\n",
            "step: 16228, loss: 1.3947\n",
            "step: 16229, loss: 1.4205\n",
            "step: 16230, loss: 1.3842\n",
            "step: 16231, loss: 1.3854\n",
            "step: 16232, loss: 1.3852\n",
            "step: 16233, loss: 1.3841\n",
            "step: 16234, loss: 1.3842\n",
            "step: 16235, loss: 1.3831\n",
            "step: 16236, loss: 1.3933\n",
            "step: 16237, loss: 1.4609\n",
            "step: 16238, loss: 1.3855\n",
            "step: 16239, loss: 1.3823\n",
            "step: 16240, loss: 1.3836\n",
            "step: 16241, loss: 1.4075\n",
            "step: 16242, loss: 1.3963\n",
            "step: 16243, loss: 1.4280\n",
            "step: 16244, loss: 1.5145\n",
            "step: 16245, loss: 1.4030\n",
            "step: 16246, loss: 1.3946\n",
            "step: 16247, loss: 1.4017\n",
            "step: 16248, loss: 1.3853\n",
            "step: 16249, loss: 1.4088\n",
            "step: 16250, loss: 1.3865\n",
            "step: 16251, loss: 1.4062\n",
            "step: 16252, loss: 1.3859\n",
            "step: 16253, loss: 1.3908\n",
            "step: 16254, loss: 1.3843\n",
            "step: 16255, loss: 1.3827\n",
            "step: 16256, loss: 1.3834\n",
            "step: 16257, loss: 1.4725\n",
            "step: 16258, loss: 1.3862\n",
            "step: 16259, loss: 1.3886\n",
            "step: 16260, loss: 1.3861\n",
            "step: 16261, loss: 1.5892\n",
            "step: 16262, loss: 1.4075\n",
            "step: 16263, loss: 1.3863\n",
            "step: 16264, loss: 1.4337\n",
            "step: 16265, loss: 1.3933\n",
            "step: 16266, loss: 1.4349\n",
            "step: 16267, loss: 1.4082\n",
            "step: 16268, loss: 1.3928\n",
            "step: 16269, loss: 1.3933\n",
            "step: 16270, loss: 1.4516\n",
            "step: 16271, loss: 1.4008\n",
            "step: 16272, loss: 1.3905\n",
            "step: 16273, loss: 1.3862\n",
            "step: 16274, loss: 1.4330\n",
            "step: 16275, loss: 1.3868\n",
            "step: 16276, loss: 1.3890\n",
            "step: 16277, loss: 1.3847\n",
            "step: 16278, loss: 1.3922\n",
            "step: 16279, loss: 1.3948\n",
            "step: 16280, loss: 1.4852\n",
            "step: 16281, loss: 1.4324\n",
            "step: 16282, loss: 1.3840\n",
            "step: 16283, loss: 1.3973\n",
            "step: 16284, loss: 1.3883\n",
            "step: 16285, loss: 1.4097\n",
            "step: 16286, loss: 1.4015\n",
            "step: 16287, loss: 1.4009\n",
            "step: 16288, loss: 1.3891\n",
            "step: 16289, loss: 1.5318\n",
            "step: 16290, loss: 1.4567\n",
            "step: 16291, loss: 1.3833\n",
            "step: 16292, loss: 1.3922\n",
            "step: 16293, loss: 1.4103\n",
            "step: 16294, loss: 1.5288\n",
            "step: 16295, loss: 1.3933\n",
            "step: 16296, loss: 1.4021\n",
            "step: 16297, loss: 1.3886\n",
            "step: 16298, loss: 1.4284\n",
            "step: 16299, loss: 1.4302\n",
            "step: 16300, loss: 1.3904\n",
            "step: 16301, loss: 1.4195\n",
            "step: 16302, loss: 1.3858\n",
            "step: 16303, loss: 1.3857\n",
            "step: 16304, loss: 1.3901\n",
            "step: 16305, loss: 1.3886\n",
            "step: 16306, loss: 1.4541\n",
            "step: 16307, loss: 1.3868\n",
            "step: 16308, loss: 1.3937\n",
            "step: 16309, loss: 1.3912\n",
            "step: 16310, loss: 1.3970\n",
            "step: 16311, loss: 1.4160\n",
            "step: 16312, loss: 1.4185\n",
            "step: 16313, loss: 1.3875\n",
            "step: 16314, loss: 1.3983\n",
            "step: 16315, loss: 1.3974\n",
            "step: 16316, loss: 1.3976\n",
            "step: 16317, loss: 1.4836\n",
            "step: 16318, loss: 1.3882\n",
            "step: 16319, loss: 1.4512\n",
            "step: 16320, loss: 1.4246\n",
            "step: 16321, loss: 1.3975\n",
            "step: 16322, loss: 1.3973\n",
            "step: 16323, loss: 1.4174\n",
            "step: 16324, loss: 1.3946\n",
            "step: 16325, loss: 1.4273\n",
            "step: 16326, loss: 1.3879\n",
            "step: 16327, loss: 1.4613\n",
            "step: 16328, loss: 1.4440\n",
            "step: 16329, loss: 1.3887\n",
            "step: 16330, loss: 1.3874\n",
            "step: 16331, loss: 1.3852\n",
            "step: 16332, loss: 1.3866\n",
            "step: 16333, loss: 1.3934\n",
            "step: 16334, loss: 1.3861\n",
            "step: 16335, loss: 1.4004\n",
            "step: 16336, loss: 1.4021\n",
            "step: 16337, loss: 1.3855\n",
            "step: 16338, loss: 1.3878\n",
            "step: 16339, loss: 1.4165\n",
            "step: 16340, loss: 1.4061\n",
            "step: 16341, loss: 1.3968\n",
            "step: 16342, loss: 1.3960\n",
            "step: 16343, loss: 1.3937\n",
            "step: 16344, loss: 1.4420\n",
            "step: 16345, loss: 1.3894\n",
            "step: 16346, loss: 1.3926\n",
            "step: 16347, loss: 1.3887\n",
            "step: 16348, loss: 1.3928\n",
            "step: 16349, loss: 1.3904\n",
            "step: 16350, loss: 1.3890\n",
            "step: 16351, loss: 1.4539\n",
            "step: 16352, loss: 1.3999\n",
            "step: 16353, loss: 1.3966\n",
            "step: 16354, loss: 1.4182\n",
            "step: 16355, loss: 1.3887\n",
            "step: 16356, loss: 1.3851\n",
            "step: 16357, loss: 1.4453\n",
            "step: 16358, loss: 1.3888\n",
            "step: 16359, loss: 1.3871\n",
            "step: 16360, loss: 1.3860\n",
            "step: 16361, loss: 1.3845\n",
            "step: 16362, loss: 1.3961\n",
            "step: 16363, loss: 1.3985\n",
            "step: 16364, loss: 1.4157\n",
            "step: 16365, loss: 1.3862\n",
            "step: 16366, loss: 1.3899\n",
            "step: 16367, loss: 1.3924\n",
            "step: 16368, loss: 1.4779\n",
            "step: 16369, loss: 1.3861\n",
            "step: 16370, loss: 1.3840\n",
            "step: 16371, loss: 1.3923\n",
            "step: 16372, loss: 1.3852\n",
            "step: 16373, loss: 1.4033\n",
            "step: 16374, loss: 1.4992\n",
            "step: 16375, loss: 1.3872\n",
            "step: 16376, loss: 1.3853\n",
            "step: 16377, loss: 1.4082\n",
            "step: 16378, loss: 1.3913\n",
            "step: 16379, loss: 1.3866\n",
            "step: 16380, loss: 1.3853\n",
            "step: 16381, loss: 1.4072\n",
            "step: 16382, loss: 1.3921\n",
            "step: 16383, loss: 1.4014\n",
            "step: 16384, loss: 1.3945\n",
            "step: 16385, loss: 1.3866\n",
            "step: 16386, loss: 1.4402\n",
            "step: 16387, loss: 1.4852\n",
            "step: 16388, loss: 1.4059\n",
            "step: 16389, loss: 1.4051\n",
            "step: 16390, loss: 1.3904\n",
            "step: 16391, loss: 1.3844\n",
            "step: 16392, loss: 1.4198\n",
            "step: 16393, loss: 1.3863\n",
            "step: 16394, loss: 1.3908\n",
            "step: 16395, loss: 1.3849\n",
            "step: 16396, loss: 1.4278\n",
            "step: 16397, loss: 1.4038\n",
            "step: 16398, loss: 1.4069\n",
            "step: 16399, loss: 1.3983\n",
            "step: 16400, loss: 1.3852\n",
            "step: 16401, loss: 1.3849\n",
            "step: 16402, loss: 1.4188\n",
            "step: 16403, loss: 1.3885\n",
            "step: 16404, loss: 1.3863\n",
            "step: 16405, loss: 1.4001\n",
            "step: 16406, loss: 1.4010\n",
            "step: 16407, loss: 1.3868\n",
            "step: 16408, loss: 1.4423\n",
            "step: 16409, loss: 1.3955\n",
            "step: 16410, loss: 1.3949\n",
            "step: 16411, loss: 1.3971\n",
            "step: 16412, loss: 1.4349\n",
            "step: 16413, loss: 1.3874\n",
            "step: 16414, loss: 1.4378\n",
            "step: 16415, loss: 1.4096\n",
            "step: 16416, loss: 1.4021\n",
            "step: 16417, loss: 1.3922\n",
            "step: 16418, loss: 1.3867\n",
            "step: 16419, loss: 1.3870\n",
            "step: 16420, loss: 1.3933\n",
            "step: 16421, loss: 1.3957\n",
            "step: 16422, loss: 1.3968\n",
            "step: 16423, loss: 1.3949\n",
            "step: 16424, loss: 1.4224\n",
            "step: 16425, loss: 1.3988\n",
            "step: 16426, loss: 1.3876\n",
            "step: 16427, loss: 1.3888\n",
            "step: 16428, loss: 1.4637\n",
            "step: 16429, loss: 1.3907\n",
            "step: 16430, loss: 1.4581\n",
            "step: 16431, loss: 1.3943\n",
            "step: 16432, loss: 1.3839\n",
            "step: 16433, loss: 1.3914\n",
            "step: 16434, loss: 1.4133\n",
            "step: 16435, loss: 1.3859\n",
            "step: 16436, loss: 1.3889\n",
            "step: 16437, loss: 1.4053\n",
            "step: 16438, loss: 1.3851\n",
            "step: 16439, loss: 1.3878\n",
            "step: 16440, loss: 1.3928\n",
            "step: 16441, loss: 1.4533\n",
            "step: 16442, loss: 1.3912\n",
            "step: 16443, loss: 1.3909\n",
            "step: 16444, loss: 1.3847\n",
            "step: 16445, loss: 1.4014\n",
            "step: 16446, loss: 1.4248\n",
            "step: 16447, loss: 1.3904\n",
            "step: 16448, loss: 1.3931\n",
            "step: 16449, loss: 1.4174\n",
            "step: 16450, loss: 1.3918\n",
            "step: 16451, loss: 1.4351\n",
            "step: 16452, loss: 1.4384\n",
            "step: 16453, loss: 1.3874\n",
            "step: 16454, loss: 1.3981\n",
            "step: 16455, loss: 1.3898\n",
            "step: 16456, loss: 1.4076\n",
            "step: 16457, loss: 1.4723\n",
            "step: 16458, loss: 1.3900\n",
            "step: 16459, loss: 1.4071\n",
            "step: 16460, loss: 1.4115\n",
            "step: 16461, loss: 1.3998\n",
            "step: 16462, loss: 1.4008\n",
            "step: 16463, loss: 1.3922\n",
            "step: 16464, loss: 1.3887\n",
            "step: 16465, loss: 1.4028\n",
            "step: 16466, loss: 1.3892\n",
            "step: 16467, loss: 1.3902\n",
            "step: 16468, loss: 1.4112\n",
            "step: 16469, loss: 1.3876\n",
            "step: 16470, loss: 1.4296\n",
            "step: 16471, loss: 1.3859\n",
            "step: 16472, loss: 1.4147\n",
            "step: 16473, loss: 1.3842\n",
            "step: 16474, loss: 1.4042\n",
            "step: 16475, loss: 1.3854\n",
            "step: 16476, loss: 1.4026\n",
            "step: 16477, loss: 1.4382\n",
            "step: 16478, loss: 1.3886\n",
            "step: 16479, loss: 1.3974\n",
            "step: 16480, loss: 1.3863\n",
            "step: 16481, loss: 1.3930\n",
            "step: 16482, loss: 1.3854\n",
            "step: 16483, loss: 1.3988\n",
            "step: 16484, loss: 1.3888\n",
            "step: 16485, loss: 1.4416\n",
            "step: 16486, loss: 1.4085\n",
            "step: 16487, loss: 1.3881\n",
            "step: 16488, loss: 1.3859\n",
            "step: 16489, loss: 1.3959\n",
            "step: 16490, loss: 1.3853\n",
            "step: 16491, loss: 1.4144\n",
            "step: 16492, loss: 1.3935\n",
            "step: 16493, loss: 1.4444\n",
            "step: 16494, loss: 1.3838\n",
            "step: 16495, loss: 1.4477\n",
            "step: 16496, loss: 1.3949\n",
            "step: 16497, loss: 1.3929\n",
            "step: 16498, loss: 1.3863\n",
            "step: 16499, loss: 1.3846\n",
            "step: 16500, loss: 1.3925\n",
            "step: 16501, loss: 1.3847\n",
            "step: 16502, loss: 1.4289\n",
            "step: 16503, loss: 1.3850\n",
            "step: 16504, loss: 1.3869\n",
            "step: 16505, loss: 1.4203\n",
            "step: 16506, loss: 1.3851\n",
            "step: 16507, loss: 1.4030\n",
            "step: 16508, loss: 1.3900\n",
            "step: 16509, loss: 1.3911\n",
            "step: 16510, loss: 1.3933\n",
            "step: 16511, loss: 1.4205\n",
            "step: 16512, loss: 1.3939\n",
            "step: 16513, loss: 1.4196\n",
            "step: 16514, loss: 1.4239\n",
            "step: 16515, loss: 1.3923\n",
            "step: 16516, loss: 1.3825\n",
            "step: 16517, loss: 1.4197\n",
            "step: 16518, loss: 1.3846\n",
            "step: 16519, loss: 1.3946\n",
            "step: 16520, loss: 1.4026\n",
            "step: 16521, loss: 1.3849\n",
            "step: 16522, loss: 1.3832\n",
            "step: 16523, loss: 1.3843\n",
            "step: 16524, loss: 1.4101\n",
            "step: 16525, loss: 1.3896\n",
            "step: 16526, loss: 1.4339\n",
            "step: 16527, loss: 1.3937\n",
            "step: 16528, loss: 1.4204\n",
            "step: 16529, loss: 1.4075\n",
            "step: 16530, loss: 1.4464\n",
            "step: 16531, loss: 1.3859\n",
            "step: 16532, loss: 1.3872\n",
            "step: 16533, loss: 1.3887\n",
            "step: 16534, loss: 1.4084\n",
            "step: 16535, loss: 1.4150\n",
            "step: 16536, loss: 1.4606\n",
            "step: 16537, loss: 1.3910\n",
            "step: 16538, loss: 1.4287\n",
            "step: 16539, loss: 1.4396\n",
            "step: 16540, loss: 1.3840\n",
            "step: 16541, loss: 1.3892\n",
            "step: 16542, loss: 1.3913\n",
            "step: 16543, loss: 1.3856\n",
            "step: 16544, loss: 1.3851\n",
            "step: 16545, loss: 1.3920\n",
            "step: 16546, loss: 1.3901\n",
            "step: 16547, loss: 1.4404\n",
            "step: 16548, loss: 1.3862\n",
            "step: 16549, loss: 1.3867\n",
            "step: 16550, loss: 1.3879\n",
            "step: 16551, loss: 1.4071\n",
            "step: 16552, loss: 1.3846\n",
            "step: 16553, loss: 1.3878\n",
            "step: 16554, loss: 1.4423\n",
            "step: 16555, loss: 1.3977\n",
            "step: 16556, loss: 1.4169\n",
            "step: 16557, loss: 1.3928\n",
            "step: 16558, loss: 1.3853\n",
            "step: 16559, loss: 1.3893\n",
            "step: 16560, loss: 1.3849\n",
            "step: 16561, loss: 1.3851\n",
            "step: 16562, loss: 1.3858\n",
            "step: 16563, loss: 1.3999\n",
            "step: 16564, loss: 1.3848\n",
            "step: 16565, loss: 1.4638\n",
            "step: 16566, loss: 1.3881\n",
            "step: 16567, loss: 1.3908\n",
            "step: 16568, loss: 1.3924\n",
            "step: 16569, loss: 1.3861\n",
            "step: 16570, loss: 1.3892\n",
            "step: 16571, loss: 1.3855\n",
            "step: 16572, loss: 1.3868\n",
            "step: 16573, loss: 1.3967\n",
            "step: 16574, loss: 1.3870\n",
            "step: 16575, loss: 1.3877\n",
            "step: 16576, loss: 1.3854\n",
            "step: 16577, loss: 1.4012\n",
            "step: 16578, loss: 1.4375\n",
            "step: 16579, loss: 1.3963\n",
            "step: 16580, loss: 1.3855\n",
            "step: 16581, loss: 1.3844\n",
            "step: 16582, loss: 1.3873\n",
            "step: 16583, loss: 1.3946\n",
            "step: 16584, loss: 1.4600\n",
            "step: 16585, loss: 1.3864\n",
            "step: 16586, loss: 1.3865\n",
            "step: 16587, loss: 1.4101\n",
            "step: 16588, loss: 1.3869\n",
            "step: 16589, loss: 1.3846\n",
            "step: 16590, loss: 1.3888\n",
            "step: 16591, loss: 1.3950\n",
            "step: 16592, loss: 1.3858\n",
            "step: 16593, loss: 1.3968\n",
            "step: 16594, loss: 1.4201\n",
            "step: 16595, loss: 1.4865\n",
            "step: 16596, loss: 1.4744\n",
            "step: 16597, loss: 1.4006\n",
            "step: 16598, loss: 1.4049\n",
            "step: 16599, loss: 1.3901\n",
            "step: 16600, loss: 1.5016\n",
            "step: 16601, loss: 1.4124\n",
            "step: 16602, loss: 1.4189\n",
            "step: 16603, loss: 1.3982\n",
            "step: 16604, loss: 1.3919\n",
            "step: 16605, loss: 1.4701\n",
            "step: 16606, loss: 1.4219\n",
            "step: 16607, loss: 1.4118\n",
            "step: 16608, loss: 1.4150\n",
            "step: 16609, loss: 1.4026\n",
            "step: 16610, loss: 1.4755\n",
            "step: 16611, loss: 1.3865\n",
            "step: 16612, loss: 1.4519\n",
            "step: 16613, loss: 1.3915\n",
            "step: 16614, loss: 1.3886\n",
            "step: 16615, loss: 1.3896\n",
            "step: 16616, loss: 1.3924\n",
            "step: 16617, loss: 1.3922\n",
            "step: 16618, loss: 1.3891\n",
            "step: 16619, loss: 1.3971\n",
            "step: 16620, loss: 1.3950\n",
            "step: 16621, loss: 1.3919\n",
            "step: 16622, loss: 1.3950\n",
            "step: 16623, loss: 1.3905\n",
            "step: 16624, loss: 1.3885\n",
            "step: 16625, loss: 1.3898\n",
            "step: 16626, loss: 1.4170\n",
            "step: 16627, loss: 1.3888\n",
            "step: 16628, loss: 1.3978\n",
            "step: 16629, loss: 1.3885\n",
            "step: 16630, loss: 1.3868\n",
            "step: 16631, loss: 1.3876\n",
            "step: 16632, loss: 1.3880\n",
            "step: 16633, loss: 1.4060\n",
            "step: 16634, loss: 1.3863\n",
            "step: 16635, loss: 1.4282\n",
            "step: 16636, loss: 1.3857\n",
            "step: 16637, loss: 1.3857\n",
            "step: 16638, loss: 1.3920\n",
            "step: 16639, loss: 1.4309\n",
            "step: 16640, loss: 1.3880\n",
            "step: 16641, loss: 1.4038\n",
            "step: 16642, loss: 1.4041\n",
            "step: 16643, loss: 1.4281\n",
            "step: 16644, loss: 1.4064\n",
            "step: 16645, loss: 1.4127\n",
            "step: 16646, loss: 1.3854\n",
            "step: 16647, loss: 1.4118\n",
            "step: 16648, loss: 1.3863\n",
            "step: 16649, loss: 1.3988\n",
            "step: 16650, loss: 1.3909\n",
            "step: 16651, loss: 1.4116\n",
            "step: 16652, loss: 1.4059\n",
            "step: 16653, loss: 1.3846\n",
            "step: 16654, loss: 1.4373\n",
            "step: 16655, loss: 1.3861\n",
            "step: 16656, loss: 1.4369\n",
            "step: 16657, loss: 1.3864\n",
            "step: 16658, loss: 1.3870\n",
            "step: 16659, loss: 1.3921\n",
            "step: 16660, loss: 1.4629\n",
            "step: 16661, loss: 1.4562\n",
            "step: 16662, loss: 1.3926\n",
            "step: 16663, loss: 1.4249\n",
            "step: 16664, loss: 1.3889\n",
            "step: 16665, loss: 1.3928\n",
            "step: 16666, loss: 1.4346\n",
            "step: 16667, loss: 1.4526\n",
            "step: 16668, loss: 1.4065\n",
            "step: 16669, loss: 1.4011\n",
            "step: 16670, loss: 1.3852\n",
            "step: 16671, loss: 1.3949\n",
            "step: 16672, loss: 1.3955\n",
            "step: 16673, loss: 1.3925\n",
            "step: 16674, loss: 1.3901\n",
            "step: 16675, loss: 1.4183\n",
            "step: 16676, loss: 1.3861\n",
            "step: 16677, loss: 1.3880\n",
            "step: 16678, loss: 1.4905\n",
            "step: 16679, loss: 1.3874\n",
            "step: 16680, loss: 1.3865\n",
            "step: 16681, loss: 1.3989\n",
            "step: 16682, loss: 1.4199\n",
            "step: 16683, loss: 1.4290\n",
            "step: 16684, loss: 1.3947\n",
            "step: 16685, loss: 1.4329\n",
            "step: 16686, loss: 1.3944\n",
            "step: 16687, loss: 1.3951\n",
            "step: 16688, loss: 1.3990\n",
            "step: 16689, loss: 1.4327\n",
            "step: 16690, loss: 1.4880\n",
            "step: 16691, loss: 1.4011\n",
            "step: 16692, loss: 1.4162\n",
            "step: 16693, loss: 1.4156\n",
            "step: 16694, loss: 1.4727\n",
            "step: 16695, loss: 1.4030\n",
            "step: 16696, loss: 1.4143\n",
            "step: 16697, loss: 1.4404\n",
            "step: 16698, loss: 1.4173\n",
            "step: 16699, loss: 1.3983\n",
            "step: 16700, loss: 1.4095\n",
            "step: 16701, loss: 1.4102\n",
            "step: 16702, loss: 1.3863\n",
            "step: 16703, loss: 1.3894\n",
            "step: 16704, loss: 1.3955\n",
            "step: 16705, loss: 1.4050\n",
            "step: 16706, loss: 1.3850\n",
            "step: 16707, loss: 1.3989\n",
            "step: 16708, loss: 1.3924\n",
            "step: 16709, loss: 1.3961\n",
            "step: 16710, loss: 1.3908\n",
            "step: 16711, loss: 1.3880\n",
            "step: 16712, loss: 1.3983\n",
            "step: 16713, loss: 1.3887\n",
            "step: 16714, loss: 1.4191\n",
            "step: 16715, loss: 1.3899\n",
            "step: 16716, loss: 1.4051\n",
            "step: 16717, loss: 1.3871\n",
            "step: 16718, loss: 1.3922\n",
            "step: 16719, loss: 1.4031\n",
            "step: 16720, loss: 1.3890\n",
            "step: 16721, loss: 1.3902\n",
            "step: 16722, loss: 1.4171\n",
            "step: 16723, loss: 1.3883\n",
            "step: 16724, loss: 1.3919\n",
            "step: 16725, loss: 1.4053\n",
            "step: 16726, loss: 1.4035\n",
            "step: 16727, loss: 1.3931\n",
            "step: 16728, loss: 1.4044\n",
            "step: 16729, loss: 1.4274\n",
            "step: 16730, loss: 1.4247\n",
            "step: 16731, loss: 1.3901\n",
            "step: 16732, loss: 1.3899\n",
            "step: 16733, loss: 1.3880\n",
            "step: 16734, loss: 1.3872\n",
            "step: 16735, loss: 1.4032\n",
            "step: 16736, loss: 1.3863\n",
            "step: 16737, loss: 1.4082\n",
            "step: 16738, loss: 1.3905\n",
            "step: 16739, loss: 1.3894\n",
            "step: 16740, loss: 1.4102\n",
            "step: 16741, loss: 1.5046\n",
            "step: 16742, loss: 1.3950\n",
            "step: 16743, loss: 1.4096\n",
            "step: 16744, loss: 1.3891\n",
            "step: 16745, loss: 1.3856\n",
            "step: 16746, loss: 1.3999\n",
            "step: 16747, loss: 1.4022\n",
            "step: 16748, loss: 1.3876\n",
            "step: 16749, loss: 1.3869\n",
            "step: 16750, loss: 1.3864\n",
            "step: 16751, loss: 1.3877\n",
            "step: 16752, loss: 1.3852\n",
            "step: 16753, loss: 1.4452\n",
            "step: 16754, loss: 1.3841\n",
            "step: 16755, loss: 1.3996\n",
            "step: 16756, loss: 1.3971\n",
            "step: 16757, loss: 1.3856\n",
            "step: 16758, loss: 1.3852\n",
            "step: 16759, loss: 1.3929\n",
            "step: 16760, loss: 1.3896\n",
            "step: 16761, loss: 1.4008\n",
            "step: 16762, loss: 1.3973\n",
            "step: 16763, loss: 1.3886\n",
            "step: 16764, loss: 1.3849\n",
            "step: 16765, loss: 1.3985\n",
            "step: 16766, loss: 1.3920\n",
            "step: 16767, loss: 1.3949\n",
            "step: 16768, loss: 1.3926\n",
            "step: 16769, loss: 1.4142\n",
            "step: 16770, loss: 1.3856\n",
            "step: 16771, loss: 1.3924\n",
            "step: 16772, loss: 1.3840\n",
            "step: 16773, loss: 1.4007\n",
            "step: 16774, loss: 1.4215\n",
            "step: 16775, loss: 1.4159\n",
            "step: 16776, loss: 1.4381\n",
            "step: 16777, loss: 1.3937\n",
            "step: 16778, loss: 1.4009\n",
            "step: 16779, loss: 1.3850\n",
            "step: 16780, loss: 1.3874\n",
            "step: 16781, loss: 1.4119\n",
            "step: 16782, loss: 1.3963\n",
            "step: 16783, loss: 1.3884\n",
            "step: 16784, loss: 1.4178\n",
            "step: 16785, loss: 1.4111\n",
            "step: 16786, loss: 1.3848\n",
            "step: 16787, loss: 1.4196\n",
            "step: 16788, loss: 1.4044\n",
            "step: 16789, loss: 1.4240\n",
            "step: 16790, loss: 1.4132\n",
            "step: 16791, loss: 1.4575\n",
            "step: 16792, loss: 1.3868\n",
            "step: 16793, loss: 1.3960\n",
            "step: 16794, loss: 1.3854\n",
            "step: 16795, loss: 1.4878\n",
            "step: 16796, loss: 1.3886\n",
            "step: 16797, loss: 1.3848\n",
            "step: 16798, loss: 1.3902\n",
            "step: 16799, loss: 1.4275\n",
            "step: 16800, loss: 1.3973\n",
            "step: 16801, loss: 1.4090\n",
            "step: 16802, loss: 1.3885\n",
            "step: 16803, loss: 1.3978\n",
            "step: 16804, loss: 1.3947\n",
            "step: 16805, loss: 1.3976\n",
            "step: 16806, loss: 1.4123\n",
            "step: 16807, loss: 1.3881\n",
            "step: 16808, loss: 1.3937\n",
            "step: 16809, loss: 1.4151\n",
            "step: 16810, loss: 1.3845\n",
            "step: 16811, loss: 1.4032\n",
            "step: 16812, loss: 1.4086\n",
            "step: 16813, loss: 1.3944\n",
            "step: 16814, loss: 1.3826\n",
            "step: 16815, loss: 1.3921\n",
            "step: 16816, loss: 1.3980\n",
            "step: 16817, loss: 1.3926\n",
            "step: 16818, loss: 1.3859\n",
            "step: 16819, loss: 1.4250\n",
            "step: 16820, loss: 1.3939\n",
            "step: 16821, loss: 1.3815\n",
            "step: 16822, loss: 1.3877\n",
            "step: 16823, loss: 1.4776\n",
            "step: 16824, loss: 1.3829\n",
            "step: 16825, loss: 1.3846\n",
            "step: 16826, loss: 1.3930\n",
            "step: 16827, loss: 1.3942\n",
            "step: 16828, loss: 1.4111\n",
            "step: 16829, loss: 1.4026\n",
            "step: 16830, loss: 1.3940\n",
            "step: 16831, loss: 1.3841\n",
            "step: 16832, loss: 1.3869\n",
            "step: 16833, loss: 1.3897\n",
            "step: 16834, loss: 1.3862\n",
            "step: 16835, loss: 1.3865\n",
            "step: 16836, loss: 1.4020\n",
            "step: 16837, loss: 1.4545\n",
            "step: 16838, loss: 1.4091\n",
            "step: 16839, loss: 1.4142\n",
            "step: 16840, loss: 1.3879\n",
            "step: 16841, loss: 1.3856\n",
            "step: 16842, loss: 1.3853\n",
            "step: 16843, loss: 1.4151\n",
            "step: 16844, loss: 1.3988\n",
            "step: 16845, loss: 1.4040\n",
            "step: 16846, loss: 1.3835\n",
            "step: 16847, loss: 1.3952\n",
            "step: 16848, loss: 1.3884\n",
            "step: 16849, loss: 1.4136\n",
            "step: 16850, loss: 1.3822\n",
            "step: 16851, loss: 1.3959\n",
            "step: 16852, loss: 1.3846\n",
            "step: 16853, loss: 1.3940\n",
            "step: 16854, loss: 1.3839\n",
            "step: 16855, loss: 1.3974\n",
            "step: 16856, loss: 1.3864\n",
            "step: 16857, loss: 1.3962\n",
            "step: 16858, loss: 1.3861\n",
            "step: 16859, loss: 1.3942\n",
            "step: 16860, loss: 1.4265\n",
            "step: 16861, loss: 1.4511\n",
            "step: 16862, loss: 1.3985\n",
            "step: 16863, loss: 1.4168\n",
            "step: 16864, loss: 1.3966\n",
            "step: 16865, loss: 1.4484\n",
            "step: 16866, loss: 1.3839\n",
            "step: 16867, loss: 1.3863\n",
            "step: 16868, loss: 1.4175\n",
            "step: 16869, loss: 1.3862\n",
            "step: 16870, loss: 1.3821\n",
            "step: 16871, loss: 1.3842\n",
            "step: 16872, loss: 1.4050\n",
            "step: 16873, loss: 1.4527\n",
            "step: 16874, loss: 1.3873\n",
            "step: 16875, loss: 1.3941\n",
            "step: 16876, loss: 1.3849\n",
            "step: 16877, loss: 1.3880\n",
            "step: 16878, loss: 1.3862\n",
            "step: 16879, loss: 1.4839\n",
            "step: 16880, loss: 1.3836\n",
            "step: 16881, loss: 1.3831\n",
            "step: 16882, loss: 1.3921\n",
            "step: 16883, loss: 1.4379\n",
            "step: 16884, loss: 1.3873\n",
            "step: 16885, loss: 1.3904\n",
            "step: 16886, loss: 1.3834\n",
            "step: 16887, loss: 1.3959\n",
            "step: 16888, loss: 1.3955\n",
            "step: 16889, loss: 1.3839\n",
            "step: 16890, loss: 1.3854\n",
            "step: 16891, loss: 1.4086\n",
            "step: 16892, loss: 1.3875\n",
            "step: 16893, loss: 1.3866\n",
            "step: 16894, loss: 1.4123\n",
            "step: 16895, loss: 1.3841\n",
            "step: 16896, loss: 1.3964\n",
            "step: 16897, loss: 1.3953\n",
            "step: 16898, loss: 1.3845\n",
            "step: 16899, loss: 1.3861\n",
            "step: 16900, loss: 1.3865\n",
            "step: 16901, loss: 1.3862\n",
            "step: 16902, loss: 1.4135\n",
            "step: 16903, loss: 1.4187\n",
            "step: 16904, loss: 1.4019\n",
            "step: 16905, loss: 1.3843\n",
            "step: 16906, loss: 1.3868\n",
            "step: 16907, loss: 1.3972\n",
            "step: 16908, loss: 1.3982\n",
            "step: 16909, loss: 1.4217\n",
            "step: 16910, loss: 1.4092\n",
            "step: 16911, loss: 1.3917\n",
            "step: 16912, loss: 1.4331\n",
            "step: 16913, loss: 1.3929\n",
            "step: 16914, loss: 1.3886\n",
            "step: 16915, loss: 1.3834\n",
            "step: 16916, loss: 1.4058\n",
            "step: 16917, loss: 1.3976\n",
            "step: 16918, loss: 1.3900\n",
            "step: 16919, loss: 1.4167\n",
            "step: 16920, loss: 1.4334\n",
            "step: 16921, loss: 1.3893\n",
            "step: 16922, loss: 1.3840\n",
            "step: 16923, loss: 1.4304\n",
            "step: 16924, loss: 1.4314\n",
            "step: 16925, loss: 1.4082\n",
            "step: 16926, loss: 1.4282\n",
            "step: 16927, loss: 1.3939\n",
            "step: 16928, loss: 1.4275\n",
            "step: 16929, loss: 1.3925\n",
            "step: 16930, loss: 1.4947\n",
            "step: 16931, loss: 1.4041\n",
            "step: 16932, loss: 1.4238\n",
            "step: 16933, loss: 1.3922\n",
            "step: 16934, loss: 1.4423\n",
            "step: 16935, loss: 1.3862\n",
            "step: 16936, loss: 1.4523\n",
            "step: 16937, loss: 1.3884\n",
            "step: 16938, loss: 1.4026\n",
            "step: 16939, loss: 1.4517\n",
            "step: 16940, loss: 1.3858\n",
            "step: 16941, loss: 1.4754\n",
            "step: 16942, loss: 1.4007\n",
            "step: 16943, loss: 1.3950\n",
            "step: 16944, loss: 1.3886\n",
            "step: 16945, loss: 1.3968\n",
            "step: 16946, loss: 1.4153\n",
            "step: 16947, loss: 1.3952\n",
            "step: 16948, loss: 1.4227\n",
            "step: 16949, loss: 1.4228\n",
            "step: 16950, loss: 1.3893\n",
            "step: 16951, loss: 1.4612\n",
            "step: 16952, loss: 1.3939\n",
            "step: 16953, loss: 1.3868\n",
            "step: 16954, loss: 1.4415\n",
            "step: 16955, loss: 1.4771\n",
            "step: 16956, loss: 1.4172\n",
            "step: 16957, loss: 1.3864\n",
            "step: 16958, loss: 1.4025\n",
            "step: 16959, loss: 1.3938\n",
            "step: 16960, loss: 1.5868\n",
            "step: 16961, loss: 1.4002\n",
            "step: 16962, loss: 1.4216\n",
            "step: 16963, loss: 1.3862\n",
            "step: 16964, loss: 1.3856\n",
            "step: 16965, loss: 1.3864\n",
            "step: 16966, loss: 1.3940\n",
            "step: 16967, loss: 1.3879\n",
            "step: 16968, loss: 1.3962\n",
            "step: 16969, loss: 1.3890\n",
            "step: 16970, loss: 1.3944\n",
            "step: 16971, loss: 1.4099\n",
            "step: 16972, loss: 1.3882\n",
            "step: 16973, loss: 1.3889\n",
            "step: 16974, loss: 1.3885\n",
            "step: 16975, loss: 1.4248\n",
            "step: 16976, loss: 1.3870\n",
            "step: 16977, loss: 1.4050\n",
            "step: 16978, loss: 1.4223\n",
            "step: 16979, loss: 1.3873\n",
            "step: 16980, loss: 1.3855\n",
            "step: 16981, loss: 1.3872\n",
            "step: 16982, loss: 1.3877\n",
            "step: 16983, loss: 1.4058\n",
            "step: 16984, loss: 1.4019\n",
            "step: 16985, loss: 1.3861\n",
            "step: 16986, loss: 1.4290\n",
            "step: 16987, loss: 1.3882\n",
            "step: 16988, loss: 1.3867\n",
            "step: 16989, loss: 1.4210\n",
            "step: 16990, loss: 1.4715\n",
            "step: 16991, loss: 1.3841\n",
            "step: 16992, loss: 1.4295\n",
            "step: 16993, loss: 1.3866\n",
            "step: 16994, loss: 1.4037\n",
            "step: 16995, loss: 1.4198\n",
            "step: 16996, loss: 1.3910\n",
            "step: 16997, loss: 1.4007\n",
            "step: 16998, loss: 1.4130\n",
            "step: 16999, loss: 1.3860\n",
            "step: 17000, loss: 1.3876\n",
            "saving model to ./models/model_17000.ckpt\n",
            "step: 17001, loss: 1.4274\n",
            "step: 17002, loss: 1.3836\n",
            "step: 17003, loss: 1.3860\n",
            "step: 17004, loss: 1.3860\n",
            "step: 17005, loss: 1.3859\n",
            "step: 17006, loss: 1.3871\n",
            "step: 17007, loss: 1.3962\n",
            "step: 17008, loss: 1.4149\n",
            "step: 17009, loss: 1.4160\n",
            "step: 17010, loss: 1.3977\n",
            "step: 17011, loss: 1.3944\n",
            "step: 17012, loss: 1.4093\n",
            "step: 17013, loss: 1.4582\n",
            "step: 17014, loss: 1.4084\n",
            "step: 17015, loss: 1.4516\n",
            "step: 17016, loss: 1.4279\n",
            "step: 17017, loss: 1.4770\n",
            "step: 17018, loss: 1.4195\n",
            "step: 17019, loss: 1.3961\n",
            "step: 17020, loss: 1.3904\n",
            "step: 17021, loss: 1.3900\n",
            "step: 17022, loss: 1.3867\n",
            "step: 17023, loss: 1.3844\n",
            "step: 17024, loss: 1.4243\n",
            "step: 17025, loss: 1.4345\n",
            "step: 17026, loss: 1.3894\n",
            "step: 17027, loss: 1.4074\n",
            "step: 17028, loss: 1.4125\n",
            "step: 17029, loss: 1.4153\n",
            "step: 17030, loss: 1.4110\n",
            "step: 17031, loss: 1.3963\n",
            "step: 17032, loss: 1.3890\n",
            "step: 17033, loss: 1.3844\n",
            "step: 17034, loss: 1.3888\n",
            "step: 17035, loss: 1.3903\n",
            "step: 17036, loss: 1.4015\n",
            "step: 17037, loss: 1.4996\n",
            "step: 17038, loss: 1.4068\n",
            "step: 17039, loss: 1.3916\n",
            "step: 17040, loss: 1.4446\n",
            "step: 17041, loss: 1.7302\n",
            "step: 17042, loss: 1.4521\n",
            "step: 17043, loss: 1.5897\n",
            "step: 17044, loss: 1.4437\n",
            "step: 17045, loss: 1.3910\n",
            "step: 17046, loss: 1.4937\n",
            "step: 17047, loss: 1.4001\n",
            "step: 17048, loss: 1.4105\n",
            "step: 17049, loss: 1.4127\n",
            "step: 17050, loss: 1.4016\n",
            "step: 17051, loss: 1.5168\n",
            "step: 17052, loss: 1.4474\n",
            "step: 17053, loss: 1.5065\n",
            "step: 17054, loss: 1.4399\n",
            "step: 17055, loss: 1.4056\n",
            "step: 17056, loss: 1.3987\n",
            "step: 17057, loss: 1.3946\n",
            "step: 17058, loss: 1.4000\n",
            "step: 17059, loss: 1.3925\n",
            "step: 17060, loss: 1.3927\n",
            "step: 17061, loss: 1.3906\n",
            "step: 17062, loss: 1.4517\n",
            "step: 17063, loss: 1.4033\n",
            "step: 17064, loss: 1.3981\n",
            "step: 17065, loss: 1.4366\n",
            "step: 17066, loss: 1.3916\n",
            "step: 17067, loss: 1.4214\n",
            "step: 17068, loss: 1.3947\n",
            "step: 17069, loss: 1.3950\n",
            "step: 17070, loss: 1.4131\n",
            "step: 17071, loss: 1.4330\n",
            "step: 17072, loss: 1.3939\n",
            "step: 17073, loss: 1.4469\n",
            "step: 17074, loss: 1.4076\n",
            "step: 17075, loss: 1.3923\n",
            "step: 17076, loss: 1.3897\n",
            "step: 17077, loss: 1.3945\n",
            "step: 17078, loss: 1.4060\n",
            "step: 17079, loss: 1.3932\n",
            "step: 17080, loss: 1.3889\n",
            "step: 17081, loss: 1.4264\n",
            "step: 17082, loss: 1.4134\n",
            "step: 17083, loss: 1.4078\n",
            "step: 17084, loss: 1.4332\n",
            "step: 17085, loss: 1.3963\n",
            "step: 17086, loss: 1.3942\n",
            "step: 17087, loss: 1.3888\n",
            "step: 17088, loss: 1.4229\n",
            "step: 17089, loss: 1.3950\n",
            "step: 17090, loss: 1.4384\n",
            "step: 17091, loss: 1.3924\n",
            "step: 17092, loss: 1.3962\n",
            "step: 17093, loss: 1.3871\n",
            "step: 17094, loss: 1.3993\n",
            "step: 17095, loss: 1.3882\n",
            "step: 17096, loss: 1.4048\n",
            "step: 17097, loss: 1.4404\n",
            "step: 17098, loss: 1.4545\n",
            "step: 17099, loss: 1.4042\n",
            "step: 17100, loss: 1.3874\n",
            "step: 17101, loss: 1.3928\n",
            "step: 17102, loss: 1.4140\n",
            "step: 17103, loss: 1.4530\n",
            "step: 17104, loss: 1.3994\n",
            "step: 17105, loss: 1.4198\n",
            "step: 17106, loss: 1.4279\n",
            "step: 17107, loss: 1.4090\n",
            "step: 17108, loss: 1.3944\n",
            "step: 17109, loss: 1.5163\n",
            "step: 17110, loss: 1.3897\n",
            "step: 17111, loss: 1.4195\n",
            "step: 17112, loss: 1.4166\n",
            "step: 17113, loss: 1.3903\n",
            "step: 17114, loss: 1.4904\n",
            "step: 17115, loss: 1.3865\n",
            "step: 17116, loss: 1.3923\n",
            "step: 17117, loss: 1.3889\n",
            "step: 17118, loss: 1.4023\n",
            "step: 17119, loss: 1.4429\n",
            "step: 17120, loss: 1.4233\n",
            "step: 17121, loss: 1.3908\n",
            "step: 17122, loss: 1.3936\n",
            "step: 17123, loss: 1.3943\n",
            "step: 17124, loss: 1.3951\n",
            "step: 17125, loss: 1.4106\n",
            "step: 17126, loss: 1.4363\n",
            "step: 17127, loss: 1.4107\n",
            "step: 17128, loss: 1.3942\n",
            "step: 17129, loss: 1.3965\n",
            "step: 17130, loss: 1.4120\n",
            "step: 17131, loss: 1.4138\n",
            "step: 17132, loss: 1.4378\n",
            "step: 17133, loss: 1.3910\n",
            "step: 17134, loss: 1.4265\n",
            "step: 17135, loss: 1.4379\n",
            "step: 17136, loss: 1.4183\n",
            "step: 17137, loss: 1.3878\n",
            "step: 17138, loss: 1.5163\n",
            "step: 17139, loss: 1.3888\n",
            "step: 17140, loss: 1.3889\n",
            "step: 17141, loss: 1.4052\n",
            "step: 17142, loss: 1.3889\n",
            "step: 17143, loss: 1.4014\n",
            "step: 17144, loss: 1.4070\n",
            "step: 17145, loss: 1.3857\n",
            "step: 17146, loss: 1.3872\n",
            "step: 17147, loss: 1.4107\n",
            "step: 17148, loss: 1.3876\n",
            "step: 17149, loss: 1.3886\n",
            "step: 17150, loss: 1.4584\n",
            "step: 17151, loss: 1.3866\n",
            "step: 17152, loss: 1.3864\n",
            "step: 17153, loss: 1.4221\n",
            "step: 17154, loss: 1.4290\n",
            "step: 17155, loss: 1.4038\n",
            "step: 17156, loss: 1.4764\n",
            "step: 17157, loss: 1.3960\n",
            "step: 17158, loss: 1.4029\n",
            "step: 17159, loss: 1.3848\n",
            "step: 17160, loss: 1.3938\n",
            "step: 17161, loss: 1.3971\n",
            "step: 17162, loss: 1.4189\n",
            "step: 17163, loss: 1.3866\n",
            "step: 17164, loss: 1.4080\n",
            "step: 17165, loss: 1.4035\n",
            "step: 17166, loss: 1.4085\n",
            "step: 17167, loss: 1.3843\n",
            "step: 17168, loss: 1.4619\n",
            "step: 17169, loss: 1.3871\n",
            "step: 17170, loss: 1.4785\n",
            "step: 17171, loss: 1.4236\n",
            "step: 17172, loss: 1.3901\n",
            "step: 17173, loss: 1.3848\n",
            "step: 17174, loss: 1.3861\n",
            "step: 17175, loss: 1.3957\n",
            "step: 17176, loss: 1.3870\n",
            "step: 17177, loss: 1.4019\n",
            "step: 17178, loss: 1.3926\n",
            "step: 17179, loss: 1.3869\n",
            "step: 17180, loss: 1.3891\n",
            "step: 17181, loss: 1.5224\n",
            "step: 17182, loss: 1.3849\n",
            "step: 17183, loss: 1.3912\n",
            "step: 17184, loss: 1.4105\n",
            "step: 17185, loss: 1.4059\n",
            "step: 17186, loss: 1.3842\n",
            "step: 17187, loss: 1.3959\n",
            "step: 17188, loss: 1.3897\n",
            "step: 17189, loss: 1.3904\n",
            "step: 17190, loss: 1.3861\n",
            "step: 17191, loss: 1.3912\n",
            "step: 17192, loss: 1.4259\n",
            "step: 17193, loss: 1.4672\n",
            "step: 17194, loss: 1.4081\n",
            "step: 17195, loss: 1.4011\n",
            "step: 17196, loss: 1.3864\n",
            "step: 17197, loss: 1.4524\n",
            "step: 17198, loss: 1.3871\n",
            "step: 17199, loss: 1.3879\n",
            "step: 17200, loss: 1.3908\n",
            "step: 17201, loss: 1.3878\n",
            "step: 17202, loss: 1.4051\n",
            "step: 17203, loss: 1.4278\n",
            "step: 17204, loss: 1.4207\n",
            "step: 17205, loss: 1.3901\n",
            "step: 17206, loss: 1.3888\n",
            "step: 17207, loss: 1.3916\n",
            "step: 17208, loss: 1.4357\n",
            "step: 17209, loss: 1.3903\n",
            "step: 17210, loss: 1.3912\n",
            "step: 17211, loss: 1.3946\n",
            "step: 17212, loss: 1.3898\n",
            "step: 17213, loss: 1.4154\n",
            "step: 17214, loss: 1.3990\n",
            "step: 17215, loss: 1.3993\n",
            "step: 17216, loss: 1.4525\n",
            "step: 17217, loss: 1.3890\n",
            "step: 17218, loss: 1.3867\n",
            "step: 17219, loss: 1.3873\n",
            "step: 17220, loss: 1.3852\n",
            "step: 17221, loss: 1.3949\n",
            "step: 17222, loss: 1.4509\n",
            "step: 17223, loss: 1.4044\n",
            "step: 17224, loss: 1.3948\n",
            "step: 17225, loss: 1.4397\n",
            "step: 17226, loss: 1.3906\n",
            "step: 17227, loss: 1.3994\n",
            "step: 17228, loss: 1.4546\n",
            "step: 17229, loss: 1.4057\n",
            "step: 17230, loss: 1.4158\n",
            "step: 17231, loss: 1.3920\n",
            "step: 17232, loss: 1.3859\n",
            "step: 17233, loss: 1.4168\n",
            "step: 17234, loss: 1.3909\n",
            "step: 17235, loss: 1.3902\n",
            "step: 17236, loss: 1.4023\n",
            "step: 17237, loss: 1.4073\n",
            "step: 17238, loss: 1.3894\n",
            "step: 17239, loss: 1.4110\n",
            "step: 17240, loss: 1.3946\n",
            "step: 17241, loss: 1.4050\n",
            "step: 17242, loss: 1.3939\n",
            "step: 17243, loss: 1.3896\n",
            "step: 17244, loss: 1.4554\n",
            "step: 17245, loss: 1.3980\n",
            "step: 17246, loss: 1.3861\n",
            "step: 17247, loss: 1.3915\n",
            "step: 17248, loss: 1.3866\n",
            "step: 17249, loss: 1.3836\n",
            "step: 17250, loss: 1.3939\n",
            "step: 17251, loss: 1.3848\n",
            "step: 17252, loss: 1.4130\n",
            "step: 17253, loss: 1.3927\n",
            "step: 17254, loss: 1.4277\n",
            "step: 17255, loss: 1.3854\n",
            "step: 17256, loss: 1.4164\n",
            "step: 17257, loss: 1.3836\n",
            "step: 17258, loss: 1.3877\n",
            "step: 17259, loss: 1.4362\n",
            "step: 17260, loss: 1.4075\n",
            "step: 17261, loss: 1.4250\n",
            "step: 17262, loss: 1.3992\n",
            "step: 17263, loss: 1.3930\n",
            "step: 17264, loss: 1.4025\n",
            "step: 17265, loss: 1.4239\n",
            "step: 17266, loss: 1.4423\n",
            "step: 17267, loss: 1.4223\n",
            "step: 17268, loss: 1.3875\n",
            "step: 17269, loss: 1.4016\n",
            "step: 17270, loss: 1.3827\n",
            "step: 17271, loss: 1.3966\n",
            "step: 17272, loss: 1.3990\n",
            "step: 17273, loss: 1.4067\n",
            "step: 17274, loss: 1.3895\n",
            "step: 17275, loss: 1.3870\n",
            "step: 17276, loss: 1.4088\n",
            "step: 17277, loss: 1.3897\n",
            "step: 17278, loss: 1.3871\n",
            "step: 17279, loss: 1.4018\n",
            "step: 17280, loss: 1.3861\n",
            "step: 17281, loss: 1.3864\n",
            "step: 17282, loss: 1.4439\n",
            "step: 17283, loss: 1.3897\n",
            "step: 17284, loss: 1.3877\n",
            "step: 17285, loss: 1.3902\n",
            "step: 17286, loss: 1.3910\n",
            "step: 17287, loss: 1.3870\n",
            "step: 17288, loss: 1.3869\n",
            "step: 17289, loss: 1.3879\n",
            "step: 17290, loss: 1.4288\n",
            "step: 17291, loss: 1.3871\n",
            "step: 17292, loss: 1.4015\n",
            "step: 17293, loss: 1.3843\n",
            "step: 17294, loss: 1.3982\n",
            "step: 17295, loss: 1.3962\n",
            "step: 17296, loss: 1.3992\n",
            "step: 17297, loss: 1.3924\n",
            "step: 17298, loss: 1.3864\n",
            "step: 17299, loss: 1.3989\n",
            "step: 17300, loss: 1.3851\n",
            "step: 17301, loss: 1.3845\n",
            "step: 17302, loss: 1.3873\n",
            "step: 17303, loss: 1.4036\n",
            "step: 17304, loss: 1.3888\n",
            "step: 17305, loss: 1.3853\n",
            "step: 17306, loss: 1.3992\n",
            "step: 17307, loss: 1.3871\n",
            "step: 17308, loss: 1.4043\n",
            "step: 17309, loss: 1.3865\n",
            "step: 17310, loss: 1.3873\n",
            "step: 17311, loss: 1.4232\n",
            "step: 17312, loss: 1.3839\n",
            "step: 17313, loss: 1.3851\n",
            "step: 17314, loss: 1.4195\n",
            "step: 17315, loss: 1.3939\n",
            "step: 17316, loss: 1.4081\n",
            "step: 17317, loss: 1.4086\n",
            "step: 17318, loss: 1.3855\n",
            "step: 17319, loss: 1.3838\n",
            "step: 17320, loss: 1.3919\n",
            "step: 17321, loss: 1.3865\n",
            "step: 17322, loss: 1.4052\n",
            "step: 17323, loss: 1.4444\n",
            "step: 17324, loss: 1.3858\n",
            "step: 17325, loss: 1.4020\n",
            "step: 17326, loss: 1.4033\n",
            "step: 17327, loss: 1.3873\n",
            "step: 17328, loss: 1.3828\n",
            "step: 17329, loss: 1.4989\n",
            "step: 17330, loss: 1.3928\n",
            "step: 17331, loss: 1.4124\n",
            "step: 17332, loss: 1.4070\n",
            "step: 17333, loss: 1.3914\n",
            "step: 17334, loss: 1.3837\n",
            "step: 17335, loss: 1.3856\n",
            "step: 17336, loss: 1.4292\n",
            "step: 17337, loss: 1.4111\n",
            "step: 17338, loss: 1.3845\n",
            "step: 17339, loss: 1.4470\n",
            "step: 17340, loss: 1.4395\n",
            "step: 17341, loss: 1.3855\n",
            "step: 17342, loss: 1.4030\n",
            "step: 17343, loss: 1.3862\n",
            "step: 17344, loss: 1.3867\n",
            "step: 17345, loss: 1.3908\n",
            "step: 17346, loss: 1.4082\n",
            "step: 17347, loss: 1.4213\n",
            "step: 17348, loss: 1.3884\n",
            "step: 17349, loss: 1.3843\n",
            "step: 17350, loss: 1.3900\n",
            "step: 17351, loss: 1.4148\n",
            "step: 17352, loss: 1.3918\n",
            "step: 17353, loss: 1.3927\n",
            "step: 17354, loss: 1.3885\n",
            "step: 17355, loss: 1.3954\n",
            "step: 17356, loss: 1.4666\n",
            "step: 17357, loss: 1.3982\n",
            "step: 17358, loss: 1.4364\n",
            "step: 17359, loss: 1.3948\n",
            "step: 17360, loss: 1.4257\n",
            "step: 17361, loss: 1.4216\n",
            "step: 17362, loss: 1.4754\n",
            "step: 17363, loss: 1.4187\n",
            "step: 17364, loss: 1.3887\n",
            "step: 17365, loss: 1.3900\n",
            "step: 17366, loss: 1.3926\n",
            "step: 17367, loss: 1.3937\n",
            "step: 17368, loss: 1.3957\n",
            "step: 17369, loss: 1.3997\n",
            "step: 17370, loss: 1.3907\n",
            "step: 17371, loss: 1.3864\n",
            "step: 17372, loss: 1.3878\n",
            "step: 17373, loss: 1.3910\n",
            "step: 17374, loss: 1.3934\n",
            "step: 17375, loss: 1.3895\n",
            "step: 17376, loss: 1.3931\n",
            "step: 17377, loss: 1.4069\n",
            "step: 17378, loss: 1.3946\n",
            "step: 17379, loss: 1.4009\n",
            "step: 17380, loss: 1.3884\n",
            "step: 17381, loss: 1.4418\n",
            "step: 17382, loss: 1.3861\n",
            "step: 17383, loss: 1.3917\n",
            "step: 17384, loss: 1.3839\n",
            "step: 17385, loss: 1.3858\n",
            "step: 17386, loss: 1.3881\n",
            "step: 17387, loss: 1.3939\n",
            "step: 17388, loss: 1.3821\n",
            "step: 17389, loss: 1.3942\n",
            "step: 17390, loss: 1.3905\n",
            "step: 17391, loss: 1.3927\n",
            "step: 17392, loss: 1.3853\n",
            "step: 17393, loss: 1.4344\n",
            "step: 17394, loss: 1.3908\n",
            "step: 17395, loss: 1.3920\n",
            "step: 17396, loss: 1.3905\n",
            "step: 17397, loss: 1.3967\n",
            "step: 17398, loss: 1.3859\n",
            "step: 17399, loss: 1.3861\n",
            "step: 17400, loss: 1.3897\n",
            "step: 17401, loss: 1.3873\n",
            "step: 17402, loss: 1.4232\n",
            "step: 17403, loss: 1.3842\n",
            "step: 17404, loss: 1.4136\n",
            "step: 17405, loss: 1.3949\n",
            "step: 17406, loss: 1.3872\n",
            "step: 17407, loss: 1.4310\n",
            "step: 17408, loss: 1.3862\n",
            "step: 17409, loss: 1.3839\n",
            "step: 17410, loss: 1.4046\n",
            "step: 17411, loss: 1.3908\n",
            "step: 17412, loss: 1.4328\n",
            "step: 17413, loss: 1.3898\n",
            "step: 17414, loss: 1.3882\n",
            "step: 17415, loss: 1.4146\n",
            "step: 17416, loss: 1.3899\n",
            "step: 17417, loss: 1.4167\n",
            "step: 17418, loss: 1.3859\n",
            "step: 17419, loss: 1.3931\n",
            "step: 17420, loss: 1.3891\n",
            "step: 17421, loss: 1.3955\n",
            "step: 17422, loss: 1.3848\n",
            "step: 17423, loss: 1.4050\n",
            "step: 17424, loss: 1.3833\n",
            "step: 17425, loss: 1.3883\n",
            "step: 17426, loss: 1.3898\n",
            "step: 17427, loss: 1.3953\n",
            "step: 17428, loss: 1.4140\n",
            "step: 17429, loss: 1.3878\n",
            "step: 17430, loss: 1.3943\n",
            "step: 17431, loss: 1.3858\n",
            "step: 17432, loss: 1.3888\n",
            "step: 17433, loss: 1.4047\n",
            "step: 17434, loss: 1.4219\n",
            "step: 17435, loss: 1.4263\n",
            "step: 17436, loss: 1.4011\n",
            "step: 17437, loss: 1.3894\n",
            "step: 17438, loss: 1.3874\n",
            "step: 17439, loss: 1.3906\n",
            "step: 17440, loss: 1.4334\n",
            "step: 17441, loss: 1.3878\n",
            "step: 17442, loss: 1.4826\n",
            "step: 17443, loss: 1.3902\n",
            "step: 17444, loss: 1.4607\n",
            "step: 17445, loss: 1.3850\n",
            "step: 17446, loss: 1.4261\n",
            "step: 17447, loss: 1.4016\n",
            "step: 17448, loss: 1.3850\n",
            "step: 17449, loss: 1.3886\n",
            "step: 17450, loss: 1.3849\n",
            "step: 17451, loss: 1.3913\n",
            "step: 17452, loss: 1.3864\n",
            "step: 17453, loss: 1.3867\n",
            "step: 17454, loss: 1.3921\n",
            "step: 17455, loss: 1.4481\n",
            "step: 17456, loss: 1.4219\n",
            "step: 17457, loss: 1.3944\n",
            "step: 17458, loss: 1.3921\n",
            "step: 17459, loss: 1.4348\n",
            "step: 17460, loss: 1.3973\n",
            "step: 17461, loss: 1.3966\n",
            "step: 17462, loss: 1.4117\n",
            "step: 17463, loss: 1.3889\n",
            "step: 17464, loss: 1.3851\n",
            "step: 17465, loss: 1.3849\n",
            "step: 17466, loss: 1.4043\n",
            "step: 17467, loss: 1.4012\n",
            "step: 17468, loss: 1.4873\n",
            "step: 17469, loss: 1.3893\n",
            "step: 17470, loss: 1.3842\n",
            "step: 17471, loss: 1.3898\n",
            "step: 17472, loss: 1.3922\n",
            "step: 17473, loss: 1.4097\n",
            "step: 17474, loss: 1.3846\n",
            "step: 17475, loss: 1.3857\n",
            "step: 17476, loss: 1.4542\n",
            "step: 17477, loss: 1.4010\n",
            "step: 17478, loss: 1.3949\n",
            "step: 17479, loss: 1.3886\n",
            "step: 17480, loss: 1.3989\n",
            "step: 17481, loss: 1.3898\n",
            "step: 17482, loss: 1.3849\n",
            "step: 17483, loss: 1.3918\n",
            "step: 17484, loss: 1.4515\n",
            "step: 17485, loss: 1.4047\n",
            "step: 17486, loss: 1.3968\n",
            "step: 17487, loss: 1.3912\n",
            "step: 17488, loss: 1.3903\n",
            "step: 17489, loss: 1.3827\n",
            "step: 17490, loss: 1.3867\n",
            "step: 17491, loss: 1.3835\n",
            "step: 17492, loss: 1.4176\n",
            "step: 17493, loss: 1.3846\n",
            "step: 17494, loss: 1.4743\n",
            "step: 17495, loss: 1.3894\n",
            "step: 17496, loss: 1.4238\n",
            "step: 17497, loss: 1.3939\n",
            "step: 17498, loss: 1.3996\n",
            "step: 17499, loss: 1.3878\n",
            "step: 17500, loss: 1.3959\n",
            "step: 17501, loss: 1.3926\n",
            "step: 17502, loss: 1.5393\n",
            "step: 17503, loss: 1.3928\n",
            "step: 17504, loss: 1.3882\n",
            "step: 17505, loss: 1.3963\n",
            "step: 17506, loss: 1.3874\n",
            "step: 17507, loss: 1.4033\n",
            "step: 17508, loss: 1.3864\n",
            "step: 17509, loss: 1.3855\n",
            "step: 17510, loss: 1.3864\n",
            "step: 17511, loss: 1.3861\n",
            "step: 17512, loss: 1.3850\n",
            "step: 17513, loss: 1.3831\n",
            "step: 17514, loss: 1.3862\n",
            "step: 17515, loss: 1.4297\n",
            "step: 17516, loss: 1.3941\n",
            "step: 17517, loss: 1.4009\n",
            "step: 17518, loss: 1.4017\n",
            "step: 17519, loss: 1.3900\n",
            "step: 17520, loss: 1.3854\n",
            "step: 17521, loss: 1.4001\n",
            "step: 17522, loss: 1.3882\n",
            "step: 17523, loss: 1.4142\n",
            "step: 17524, loss: 1.3837\n",
            "step: 17525, loss: 1.3854\n",
            "step: 17526, loss: 1.4003\n",
            "step: 17527, loss: 1.3856\n",
            "step: 17528, loss: 1.3972\n",
            "step: 17529, loss: 1.3851\n",
            "step: 17530, loss: 1.4192\n",
            "step: 17531, loss: 1.4089\n",
            "step: 17532, loss: 1.3923\n",
            "step: 17533, loss: 1.3851\n",
            "step: 17534, loss: 1.3892\n",
            "step: 17535, loss: 1.3874\n",
            "step: 17536, loss: 1.4534\n",
            "step: 17537, loss: 1.3840\n",
            "step: 17538, loss: 1.4152\n",
            "step: 17539, loss: 1.4325\n",
            "step: 17540, loss: 1.4086\n",
            "step: 17541, loss: 1.3969\n",
            "step: 17542, loss: 1.3886\n",
            "step: 17543, loss: 1.4099\n",
            "step: 17544, loss: 1.3867\n",
            "step: 17545, loss: 1.3940\n",
            "step: 17546, loss: 1.3846\n",
            "step: 17547, loss: 1.3931\n",
            "step: 17548, loss: 1.3843\n",
            "step: 17549, loss: 1.4558\n",
            "step: 17550, loss: 1.3995\n",
            "step: 17551, loss: 1.3862\n",
            "step: 17552, loss: 1.4527\n",
            "step: 17553, loss: 1.3839\n",
            "step: 17554, loss: 1.3859\n",
            "step: 17555, loss: 1.4052\n",
            "step: 17556, loss: 1.4194\n",
            "step: 17557, loss: 1.3941\n",
            "step: 17558, loss: 1.4000\n",
            "step: 17559, loss: 1.4114\n",
            "step: 17560, loss: 1.3849\n",
            "step: 17561, loss: 1.3860\n",
            "step: 17562, loss: 1.3867\n",
            "step: 17563, loss: 1.3854\n",
            "step: 17564, loss: 1.4032\n",
            "step: 17565, loss: 1.4139\n",
            "step: 17566, loss: 1.3844\n",
            "step: 17567, loss: 1.3874\n",
            "step: 17568, loss: 1.3867\n",
            "step: 17569, loss: 1.3947\n",
            "step: 17570, loss: 1.4337\n",
            "step: 17571, loss: 1.4149\n",
            "step: 17572, loss: 1.3850\n",
            "step: 17573, loss: 1.3843\n",
            "step: 17574, loss: 1.3991\n",
            "step: 17575, loss: 1.4023\n",
            "step: 17576, loss: 1.4200\n",
            "step: 17577, loss: 1.3899\n",
            "step: 17578, loss: 1.3847\n",
            "step: 17579, loss: 1.4348\n",
            "step: 17580, loss: 1.4008\n",
            "step: 17581, loss: 1.3954\n",
            "step: 17582, loss: 1.3861\n",
            "step: 17583, loss: 1.4465\n",
            "step: 17584, loss: 1.4070\n",
            "step: 17585, loss: 1.3985\n",
            "step: 17586, loss: 1.3869\n",
            "step: 17587, loss: 1.3892\n",
            "step: 17588, loss: 1.3895\n",
            "step: 17589, loss: 1.4083\n",
            "step: 17590, loss: 1.3859\n",
            "step: 17591, loss: 1.3979\n",
            "step: 17592, loss: 1.4272\n",
            "step: 17593, loss: 1.3859\n",
            "step: 17594, loss: 1.4412\n",
            "step: 17595, loss: 1.3860\n",
            "step: 17596, loss: 1.4383\n",
            "step: 17597, loss: 1.3864\n",
            "step: 17598, loss: 1.4381\n",
            "step: 17599, loss: 1.4055\n",
            "step: 17600, loss: 1.4539\n",
            "step: 17601, loss: 1.3854\n",
            "step: 17602, loss: 1.4040\n",
            "step: 17603, loss: 1.3938\n",
            "step: 17604, loss: 1.3902\n",
            "step: 17605, loss: 1.3897\n",
            "step: 17606, loss: 1.4062\n",
            "step: 17607, loss: 1.3892\n",
            "step: 17608, loss: 1.4139\n",
            "step: 17609, loss: 1.4213\n",
            "step: 17610, loss: 1.4008\n",
            "step: 17611, loss: 1.3895\n",
            "step: 17612, loss: 1.3944\n",
            "step: 17613, loss: 1.3851\n",
            "step: 17614, loss: 1.3859\n",
            "step: 17615, loss: 1.4003\n",
            "step: 17616, loss: 1.3865\n",
            "step: 17617, loss: 1.3876\n",
            "step: 17618, loss: 1.4171\n",
            "step: 17619, loss: 1.3861\n",
            "step: 17620, loss: 1.3848\n",
            "step: 17621, loss: 1.4008\n",
            "step: 17622, loss: 1.3864\n",
            "step: 17623, loss: 1.4173\n",
            "step: 17624, loss: 1.4116\n",
            "step: 17625, loss: 1.4180\n",
            "step: 17626, loss: 1.3872\n",
            "step: 17627, loss: 1.3871\n",
            "step: 17628, loss: 1.3854\n",
            "step: 17629, loss: 1.3843\n",
            "step: 17630, loss: 1.4018\n",
            "step: 17631, loss: 1.4383\n",
            "step: 17632, loss: 1.3896\n",
            "step: 17633, loss: 1.3896\n",
            "step: 17634, loss: 1.3847\n",
            "step: 17635, loss: 1.3925\n",
            "step: 17636, loss: 1.3851\n",
            "step: 17637, loss: 1.3829\n",
            "step: 17638, loss: 1.3829\n",
            "step: 17639, loss: 1.3833\n",
            "step: 17640, loss: 1.4284\n",
            "step: 17641, loss: 1.4133\n",
            "step: 17642, loss: 1.4052\n",
            "step: 17643, loss: 1.3867\n",
            "step: 17644, loss: 1.3873\n",
            "step: 17645, loss: 1.3915\n",
            "step: 17646, loss: 1.4549\n",
            "step: 17647, loss: 1.3850\n",
            "step: 17648, loss: 1.3913\n",
            "step: 17649, loss: 1.3860\n",
            "step: 17650, loss: 1.4045\n",
            "step: 17651, loss: 1.3940\n",
            "step: 17652, loss: 1.3862\n",
            "step: 17653, loss: 1.3847\n",
            "step: 17654, loss: 1.3858\n",
            "step: 17655, loss: 1.3901\n",
            "step: 17656, loss: 1.3863\n",
            "step: 17657, loss: 1.4433\n",
            "step: 17658, loss: 1.4314\n",
            "step: 17659, loss: 1.3875\n",
            "step: 17660, loss: 1.3949\n",
            "step: 17661, loss: 1.3842\n",
            "step: 17662, loss: 1.4296\n",
            "step: 17663, loss: 1.4616\n",
            "step: 17664, loss: 1.3878\n",
            "step: 17665, loss: 1.4023\n",
            "step: 17666, loss: 1.3962\n",
            "step: 17667, loss: 1.4250\n",
            "step: 17668, loss: 1.3849\n",
            "step: 17669, loss: 1.4309\n",
            "step: 17670, loss: 1.3878\n",
            "step: 17671, loss: 1.3935\n",
            "step: 17672, loss: 1.3869\n",
            "step: 17673, loss: 1.3875\n",
            "step: 17674, loss: 1.4068\n",
            "step: 17675, loss: 1.4021\n",
            "step: 17676, loss: 1.3966\n",
            "step: 17677, loss: 1.3841\n",
            "step: 17678, loss: 1.4559\n",
            "step: 17679, loss: 1.3867\n",
            "step: 17680, loss: 1.4077\n",
            "step: 17681, loss: 1.3960\n",
            "step: 17682, loss: 1.3878\n",
            "step: 17683, loss: 1.3869\n",
            "step: 17684, loss: 1.4022\n",
            "step: 17685, loss: 1.3840\n",
            "step: 17686, loss: 1.3898\n",
            "step: 17687, loss: 1.4207\n",
            "step: 17688, loss: 1.3960\n",
            "step: 17689, loss: 1.3859\n",
            "step: 17690, loss: 1.4268\n",
            "step: 17691, loss: 1.3880\n",
            "step: 17692, loss: 1.4526\n",
            "step: 17693, loss: 1.3859\n",
            "step: 17694, loss: 1.3909\n",
            "step: 17695, loss: 1.4321\n",
            "step: 17696, loss: 1.3838\n",
            "step: 17697, loss: 1.3927\n",
            "step: 17698, loss: 1.3946\n",
            "step: 17699, loss: 1.4060\n",
            "step: 17700, loss: 1.3850\n",
            "step: 17701, loss: 1.4013\n",
            "step: 17702, loss: 1.4257\n",
            "step: 17703, loss: 1.4090\n",
            "step: 17704, loss: 1.4299\n",
            "step: 17705, loss: 1.3878\n",
            "step: 17706, loss: 1.3917\n",
            "step: 17707, loss: 1.4160\n",
            "step: 17708, loss: 1.3845\n",
            "step: 17709, loss: 1.3864\n",
            "step: 17710, loss: 1.4011\n",
            "step: 17711, loss: 1.4341\n",
            "step: 17712, loss: 1.3894\n",
            "step: 17713, loss: 1.3856\n",
            "step: 17714, loss: 1.3898\n",
            "step: 17715, loss: 1.3903\n",
            "step: 17716, loss: 1.3878\n",
            "step: 17717, loss: 1.3926\n",
            "step: 17718, loss: 1.4163\n",
            "step: 17719, loss: 1.3973\n",
            "step: 17720, loss: 1.3871\n",
            "step: 17721, loss: 1.3851\n",
            "step: 17722, loss: 1.4351\n",
            "step: 17723, loss: 1.3845\n",
            "step: 17724, loss: 1.4071\n",
            "step: 17725, loss: 1.3953\n",
            "step: 17726, loss: 1.3917\n",
            "step: 17727, loss: 1.4190\n",
            "step: 17728, loss: 1.3905\n",
            "step: 17729, loss: 1.3879\n",
            "step: 17730, loss: 1.3855\n",
            "step: 17731, loss: 1.3908\n",
            "step: 17732, loss: 1.3885\n",
            "step: 17733, loss: 1.3889\n",
            "step: 17734, loss: 1.4081\n",
            "step: 17735, loss: 1.3869\n",
            "step: 17736, loss: 1.3919\n",
            "step: 17737, loss: 1.4422\n",
            "step: 17738, loss: 1.4101\n",
            "step: 17739, loss: 1.3919\n",
            "step: 17740, loss: 1.3852\n",
            "step: 17741, loss: 1.3919\n",
            "step: 17742, loss: 1.3906\n",
            "step: 17743, loss: 1.4607\n",
            "step: 17744, loss: 1.3870\n",
            "step: 17745, loss: 1.3868\n",
            "step: 17746, loss: 1.3878\n",
            "step: 17747, loss: 1.4078\n",
            "step: 17748, loss: 1.3860\n",
            "step: 17749, loss: 1.3912\n",
            "step: 17750, loss: 1.3870\n",
            "step: 17751, loss: 1.3902\n",
            "step: 17752, loss: 1.3840\n",
            "step: 17753, loss: 1.3986\n",
            "step: 17754, loss: 1.4131\n",
            "step: 17755, loss: 1.3862\n",
            "step: 17756, loss: 1.3881\n",
            "step: 17757, loss: 1.4586\n",
            "step: 17758, loss: 1.4806\n",
            "step: 17759, loss: 1.4354\n",
            "step: 17760, loss: 1.3927\n",
            "step: 17761, loss: 1.3913\n",
            "step: 17762, loss: 1.3938\n",
            "step: 17763, loss: 1.3860\n",
            "step: 17764, loss: 1.4031\n",
            "step: 17765, loss: 1.3848\n",
            "step: 17766, loss: 1.4113\n",
            "step: 17767, loss: 1.3845\n",
            "step: 17768, loss: 1.4032\n",
            "step: 17769, loss: 1.3838\n",
            "step: 17770, loss: 1.4044\n",
            "step: 17771, loss: 1.4088\n",
            "step: 17772, loss: 1.4639\n",
            "step: 17773, loss: 1.4301\n",
            "step: 17774, loss: 1.3962\n",
            "step: 17775, loss: 1.3895\n",
            "step: 17776, loss: 1.4041\n",
            "step: 17777, loss: 1.3829\n",
            "step: 17778, loss: 1.3940\n",
            "step: 17779, loss: 1.3929\n",
            "step: 17780, loss: 1.3891\n",
            "step: 17781, loss: 1.3853\n",
            "step: 17782, loss: 1.3859\n",
            "step: 17783, loss: 1.3994\n",
            "step: 17784, loss: 1.4189\n",
            "step: 17785, loss: 1.4245\n",
            "step: 17786, loss: 1.3876\n",
            "step: 17787, loss: 1.3833\n",
            "step: 17788, loss: 1.4283\n",
            "step: 17789, loss: 1.3941\n",
            "step: 17790, loss: 1.3847\n",
            "step: 17791, loss: 1.4044\n",
            "step: 17792, loss: 1.4281\n",
            "step: 17793, loss: 1.3868\n",
            "step: 17794, loss: 1.3853\n",
            "step: 17795, loss: 1.3955\n",
            "step: 17796, loss: 1.4030\n",
            "step: 17797, loss: 1.3828\n",
            "step: 17798, loss: 1.4305\n",
            "step: 17799, loss: 1.4136\n",
            "step: 17800, loss: 1.3871\n",
            "step: 17801, loss: 1.4512\n",
            "step: 17802, loss: 1.3845\n",
            "step: 17803, loss: 1.3856\n",
            "step: 17804, loss: 1.3919\n",
            "step: 17805, loss: 1.4138\n",
            "step: 17806, loss: 1.3916\n",
            "step: 17807, loss: 1.4014\n",
            "step: 17808, loss: 1.3886\n",
            "step: 17809, loss: 1.3865\n",
            "step: 17810, loss: 1.4053\n",
            "step: 17811, loss: 1.3983\n",
            "step: 17812, loss: 1.4144\n",
            "step: 17813, loss: 1.3865\n",
            "step: 17814, loss: 1.4344\n",
            "step: 17815, loss: 1.4326\n",
            "step: 17816, loss: 1.3835\n",
            "step: 17817, loss: 1.3913\n",
            "step: 17818, loss: 1.3837\n",
            "step: 17819, loss: 1.3855\n",
            "step: 17820, loss: 1.3849\n",
            "step: 17821, loss: 1.4017\n",
            "step: 17822, loss: 1.4128\n",
            "step: 17823, loss: 1.3951\n",
            "step: 17824, loss: 1.3845\n",
            "step: 17825, loss: 1.3851\n",
            "step: 17826, loss: 1.3840\n",
            "step: 17827, loss: 1.3948\n",
            "step: 17828, loss: 1.3862\n",
            "step: 17829, loss: 1.3845\n",
            "step: 17830, loss: 1.3872\n",
            "step: 17831, loss: 1.4670\n",
            "step: 17832, loss: 1.3867\n",
            "step: 17833, loss: 1.3839\n",
            "step: 17834, loss: 1.3864\n",
            "step: 17835, loss: 1.3900\n",
            "step: 17836, loss: 1.3921\n",
            "step: 17837, loss: 1.3824\n",
            "step: 17838, loss: 1.3897\n",
            "step: 17839, loss: 1.4600\n",
            "step: 17840, loss: 1.4725\n",
            "step: 17841, loss: 1.4674\n",
            "step: 17842, loss: 1.3951\n",
            "step: 17843, loss: 1.4068\n",
            "step: 17844, loss: 1.3828\n",
            "step: 17845, loss: 1.4392\n",
            "step: 17846, loss: 1.3955\n",
            "step: 17847, loss: 1.3927\n",
            "step: 17848, loss: 1.3931\n",
            "step: 17849, loss: 1.3863\n",
            "step: 17850, loss: 1.4002\n",
            "step: 17851, loss: 1.3867\n",
            "step: 17852, loss: 1.3840\n",
            "step: 17853, loss: 1.3955\n",
            "step: 17854, loss: 1.4050\n",
            "step: 17855, loss: 1.4052\n",
            "step: 17856, loss: 1.3888\n",
            "step: 17857, loss: 1.4450\n",
            "step: 17858, loss: 1.3886\n",
            "step: 17859, loss: 1.3919\n",
            "step: 17860, loss: 1.4398\n",
            "step: 17861, loss: 1.4453\n",
            "step: 17862, loss: 1.4244\n",
            "step: 17863, loss: 1.3849\n",
            "step: 17864, loss: 1.4028\n",
            "step: 17865, loss: 1.3858\n",
            "step: 17866, loss: 1.4194\n",
            "step: 17867, loss: 1.3948\n",
            "step: 17868, loss: 1.3852\n",
            "step: 17869, loss: 1.3849\n",
            "step: 17870, loss: 1.3891\n",
            "step: 17871, loss: 1.4425\n",
            "step: 17872, loss: 1.3915\n",
            "step: 17873, loss: 1.3915\n",
            "step: 17874, loss: 1.3873\n",
            "step: 17875, loss: 1.3878\n",
            "step: 17876, loss: 1.3854\n",
            "step: 17877, loss: 1.4793\n",
            "step: 17878, loss: 1.4346\n",
            "step: 17879, loss: 1.3869\n",
            "step: 17880, loss: 1.3973\n",
            "step: 17881, loss: 1.3874\n",
            "step: 17882, loss: 1.3932\n",
            "step: 17883, loss: 1.4075\n",
            "step: 17884, loss: 1.3909\n",
            "step: 17885, loss: 1.4161\n",
            "step: 17886, loss: 1.4641\n",
            "step: 17887, loss: 1.3924\n",
            "step: 17888, loss: 1.3961\n",
            "step: 17889, loss: 1.4249\n",
            "step: 17890, loss: 1.3833\n",
            "step: 17891, loss: 1.4880\n",
            "step: 17892, loss: 1.3888\n",
            "step: 17893, loss: 1.4572\n",
            "step: 17894, loss: 1.3873\n",
            "step: 17895, loss: 1.4096\n",
            "step: 17896, loss: 1.4257\n",
            "step: 17897, loss: 1.3856\n",
            "step: 17898, loss: 1.3850\n",
            "step: 17899, loss: 1.4500\n",
            "step: 17900, loss: 1.3944\n",
            "step: 17901, loss: 1.3988\n",
            "step: 17902, loss: 1.5048\n",
            "step: 17903, loss: 1.3868\n",
            "step: 17904, loss: 1.3981\n",
            "step: 17905, loss: 1.3881\n",
            "step: 17906, loss: 1.3908\n",
            "step: 17907, loss: 1.4237\n",
            "step: 17908, loss: 1.3970\n",
            "step: 17909, loss: 1.4079\n",
            "step: 17910, loss: 1.3866\n",
            "step: 17911, loss: 1.4005\n",
            "step: 17912, loss: 1.4261\n",
            "step: 17913, loss: 1.3908\n",
            "step: 17914, loss: 1.4278\n",
            "step: 17915, loss: 1.3857\n",
            "step: 17916, loss: 1.3871\n",
            "step: 17917, loss: 1.3835\n",
            "step: 17918, loss: 1.4092\n",
            "step: 17919, loss: 1.3889\n",
            "step: 17920, loss: 1.3957\n",
            "step: 17921, loss: 1.3871\n",
            "step: 17922, loss: 1.4068\n",
            "step: 17923, loss: 1.4220\n",
            "step: 17924, loss: 1.3888\n",
            "step: 17925, loss: 1.3886\n",
            "step: 17926, loss: 1.3873\n",
            "step: 17927, loss: 1.3851\n",
            "step: 17928, loss: 1.3988\n",
            "step: 17929, loss: 1.3987\n",
            "step: 17930, loss: 1.3873\n",
            "step: 17931, loss: 1.3951\n",
            "step: 17932, loss: 1.3846\n",
            "step: 17933, loss: 1.3915\n",
            "step: 17934, loss: 1.3944\n",
            "step: 17935, loss: 1.3848\n",
            "step: 17936, loss: 1.3845\n",
            "step: 17937, loss: 1.3864\n",
            "step: 17938, loss: 1.3876\n",
            "step: 17939, loss: 1.3849\n",
            "step: 17940, loss: 1.3976\n",
            "step: 17941, loss: 1.3899\n",
            "step: 17942, loss: 1.3869\n",
            "step: 17943, loss: 1.3905\n",
            "step: 17944, loss: 1.3883\n",
            "step: 17945, loss: 1.3944\n",
            "step: 17946, loss: 1.3831\n",
            "step: 17947, loss: 1.3833\n",
            "step: 17948, loss: 1.4143\n",
            "step: 17949, loss: 1.3951\n",
            "step: 17950, loss: 1.3851\n",
            "step: 17951, loss: 1.4375\n",
            "step: 17952, loss: 1.3839\n",
            "step: 17953, loss: 1.3849\n",
            "step: 17954, loss: 1.3888\n",
            "step: 17955, loss: 1.3885\n",
            "step: 17956, loss: 1.3852\n",
            "step: 17957, loss: 1.4139\n",
            "step: 17958, loss: 1.3849\n",
            "step: 17959, loss: 1.4028\n",
            "step: 17960, loss: 1.3857\n",
            "step: 17961, loss: 1.4423\n",
            "step: 17962, loss: 1.3945\n",
            "step: 17963, loss: 1.3821\n",
            "step: 17964, loss: 1.3839\n",
            "step: 17965, loss: 1.4152\n",
            "step: 17966, loss: 1.4197\n",
            "step: 17967, loss: 1.3841\n",
            "step: 17968, loss: 1.3846\n",
            "step: 17969, loss: 1.3855\n",
            "step: 17970, loss: 1.3965\n",
            "step: 17971, loss: 1.4196\n",
            "step: 17972, loss: 1.3955\n",
            "step: 17973, loss: 1.4000\n",
            "step: 17974, loss: 1.4041\n",
            "step: 17975, loss: 1.3855\n",
            "step: 17976, loss: 1.3856\n",
            "step: 17977, loss: 1.4144\n",
            "step: 17978, loss: 1.3864\n",
            "step: 17979, loss: 1.3852\n",
            "step: 17980, loss: 1.3876\n",
            "step: 17981, loss: 1.3852\n",
            "step: 17982, loss: 1.3849\n",
            "step: 17983, loss: 1.3854\n",
            "step: 17984, loss: 1.3831\n",
            "step: 17985, loss: 1.4000\n",
            "step: 17986, loss: 1.3850\n",
            "step: 17987, loss: 1.3818\n",
            "step: 17988, loss: 1.3845\n",
            "step: 17989, loss: 1.3887\n",
            "step: 17990, loss: 1.4049\n",
            "step: 17991, loss: 1.4097\n",
            "step: 17992, loss: 1.4120\n",
            "step: 17993, loss: 1.3863\n",
            "step: 17994, loss: 1.3838\n",
            "step: 17995, loss: 1.4035\n",
            "step: 17996, loss: 1.3882\n",
            "step: 17997, loss: 1.3951\n",
            "step: 17998, loss: 1.4357\n",
            "step: 17999, loss: 1.3842\n",
            "step: 18000, loss: 1.3919\n",
            "saving model to ./models/model_18000.ckpt\n",
            "step: 18001, loss: 1.3918\n",
            "step: 18002, loss: 1.4088\n",
            "step: 18003, loss: 1.4001\n",
            "step: 18004, loss: 1.3831\n",
            "step: 18005, loss: 1.4088\n",
            "step: 18006, loss: 1.3877\n",
            "step: 18007, loss: 1.3935\n",
            "step: 18008, loss: 1.3964\n",
            "step: 18009, loss: 1.4054\n",
            "step: 18010, loss: 1.3901\n",
            "step: 18011, loss: 1.3860\n",
            "step: 18012, loss: 1.4072\n",
            "step: 18013, loss: 1.4216\n",
            "step: 18014, loss: 1.3831\n",
            "step: 18015, loss: 1.4174\n",
            "step: 18016, loss: 1.3939\n",
            "step: 18017, loss: 1.3919\n",
            "step: 18018, loss: 1.3843\n",
            "step: 18019, loss: 1.3828\n",
            "step: 18020, loss: 1.3840\n",
            "step: 18021, loss: 1.3866\n",
            "step: 18022, loss: 1.4257\n",
            "step: 18023, loss: 1.4042\n",
            "step: 18024, loss: 1.3837\n",
            "step: 18025, loss: 1.3851\n",
            "step: 18026, loss: 1.3851\n",
            "step: 18027, loss: 1.3907\n",
            "step: 18028, loss: 1.4575\n",
            "step: 18029, loss: 1.3841\n",
            "step: 18030, loss: 1.3844\n",
            "step: 18031, loss: 1.4478\n",
            "step: 18032, loss: 1.3865\n",
            "step: 18033, loss: 1.3886\n",
            "step: 18034, loss: 1.3867\n",
            "step: 18035, loss: 1.4031\n",
            "step: 18036, loss: 1.3865\n",
            "step: 18037, loss: 1.3888\n",
            "step: 18038, loss: 1.3836\n",
            "step: 18039, loss: 1.4117\n",
            "step: 18040, loss: 1.3849\n",
            "step: 18041, loss: 1.4483\n",
            "step: 18042, loss: 1.3833\n",
            "step: 18043, loss: 1.4023\n",
            "step: 18044, loss: 1.4257\n",
            "step: 18045, loss: 1.4245\n",
            "step: 18046, loss: 1.4188\n",
            "step: 18047, loss: 1.3926\n",
            "step: 18048, loss: 1.4317\n",
            "step: 18049, loss: 1.3918\n",
            "step: 18050, loss: 1.3923\n",
            "step: 18051, loss: 1.3971\n",
            "step: 18052, loss: 1.3935\n",
            "step: 18053, loss: 1.3859\n",
            "step: 18054, loss: 1.4176\n",
            "step: 18055, loss: 1.3937\n",
            "step: 18056, loss: 1.3841\n",
            "step: 18057, loss: 1.4306\n",
            "step: 18058, loss: 1.4287\n",
            "step: 18059, loss: 1.5303\n",
            "step: 18060, loss: 1.3856\n",
            "step: 18061, loss: 1.3872\n",
            "step: 18062, loss: 1.4036\n",
            "step: 18063, loss: 1.3972\n",
            "step: 18064, loss: 1.3884\n",
            "step: 18065, loss: 1.4408\n",
            "step: 18066, loss: 1.4833\n",
            "step: 18067, loss: 1.3860\n",
            "step: 18068, loss: 1.3875\n",
            "step: 18069, loss: 1.4430\n",
            "step: 18070, loss: 1.3900\n",
            "step: 18071, loss: 1.3912\n",
            "step: 18072, loss: 1.4079\n",
            "step: 18073, loss: 1.3854\n",
            "step: 18074, loss: 1.4133\n",
            "step: 18075, loss: 1.4012\n",
            "step: 18076, loss: 1.4187\n",
            "step: 18077, loss: 1.4525\n",
            "step: 18078, loss: 1.3833\n",
            "step: 18079, loss: 1.3850\n",
            "step: 18080, loss: 1.4003\n",
            "step: 18081, loss: 1.3858\n",
            "step: 18082, loss: 1.4524\n",
            "step: 18083, loss: 1.4043\n",
            "step: 18084, loss: 1.4032\n",
            "step: 18085, loss: 1.3916\n",
            "step: 18086, loss: 1.3884\n",
            "step: 18087, loss: 1.4003\n",
            "step: 18088, loss: 1.3865\n",
            "step: 18089, loss: 1.4077\n",
            "step: 18090, loss: 1.4085\n",
            "step: 18091, loss: 1.3831\n",
            "step: 18092, loss: 1.3833\n",
            "step: 18093, loss: 1.3854\n",
            "step: 18094, loss: 1.3932\n",
            "step: 18095, loss: 1.3842\n",
            "step: 18096, loss: 1.4167\n",
            "step: 18097, loss: 1.4663\n",
            "step: 18098, loss: 1.3941\n",
            "step: 18099, loss: 1.3908\n",
            "step: 18100, loss: 1.3848\n",
            "step: 18101, loss: 1.3825\n",
            "step: 18102, loss: 1.3841\n",
            "step: 18103, loss: 1.3881\n",
            "step: 18104, loss: 1.3867\n",
            "step: 18105, loss: 1.4114\n",
            "step: 18106, loss: 1.3929\n",
            "step: 18107, loss: 1.4077\n",
            "step: 18108, loss: 1.4553\n",
            "step: 18109, loss: 1.3936\n",
            "step: 18110, loss: 1.3913\n",
            "step: 18111, loss: 1.3861\n",
            "step: 18112, loss: 1.3928\n",
            "step: 18113, loss: 1.3846\n",
            "step: 18114, loss: 1.3924\n",
            "step: 18115, loss: 1.4999\n",
            "step: 18116, loss: 1.3825\n",
            "step: 18117, loss: 1.4583\n",
            "step: 18118, loss: 1.3977\n",
            "step: 18119, loss: 1.3854\n",
            "step: 18120, loss: 1.3945\n",
            "step: 18121, loss: 1.3843\n",
            "step: 18122, loss: 1.4022\n",
            "step: 18123, loss: 1.3822\n",
            "step: 18124, loss: 1.3847\n",
            "step: 18125, loss: 1.3841\n",
            "step: 18126, loss: 1.3895\n",
            "step: 18127, loss: 1.4315\n",
            "step: 18128, loss: 1.5247\n",
            "step: 18129, loss: 1.3878\n",
            "step: 18130, loss: 1.4086\n",
            "step: 18131, loss: 1.3857\n",
            "step: 18132, loss: 1.4028\n",
            "step: 18133, loss: 1.3878\n",
            "step: 18134, loss: 1.3835\n",
            "step: 18135, loss: 1.4048\n",
            "step: 18136, loss: 1.3849\n",
            "step: 18137, loss: 1.3856\n",
            "step: 18138, loss: 1.3850\n",
            "step: 18139, loss: 1.3888\n",
            "step: 18140, loss: 1.3823\n",
            "step: 18141, loss: 1.3844\n",
            "step: 18142, loss: 1.4136\n",
            "step: 18143, loss: 1.3817\n",
            "step: 18144, loss: 1.4315\n",
            "step: 18145, loss: 1.3936\n",
            "step: 18146, loss: 1.3935\n",
            "step: 18147, loss: 1.3962\n",
            "step: 18148, loss: 1.3849\n",
            "step: 18149, loss: 1.3838\n",
            "step: 18150, loss: 1.3994\n",
            "step: 18151, loss: 1.3947\n",
            "step: 18152, loss: 1.4071\n",
            "step: 18153, loss: 1.3854\n",
            "step: 18154, loss: 1.5114\n",
            "step: 18155, loss: 1.3927\n",
            "step: 18156, loss: 1.4049\n",
            "step: 18157, loss: 1.3831\n",
            "step: 18158, loss: 1.3907\n",
            "step: 18159, loss: 1.3868\n",
            "step: 18160, loss: 1.3856\n",
            "step: 18161, loss: 1.3901\n",
            "step: 18162, loss: 1.3893\n",
            "step: 18163, loss: 1.3923\n",
            "step: 18164, loss: 1.3992\n",
            "step: 18165, loss: 1.3902\n",
            "step: 18166, loss: 1.3835\n",
            "step: 18167, loss: 1.3861\n",
            "step: 18168, loss: 1.4095\n",
            "step: 18169, loss: 1.3829\n",
            "step: 18170, loss: 1.3821\n",
            "step: 18171, loss: 1.3838\n",
            "step: 18172, loss: 1.4274\n",
            "step: 18173, loss: 1.3852\n",
            "step: 18174, loss: 1.3873\n",
            "step: 18175, loss: 1.4260\n",
            "step: 18176, loss: 1.3907\n",
            "step: 18177, loss: 1.3822\n",
            "step: 18178, loss: 1.4076\n",
            "step: 18179, loss: 1.4094\n",
            "step: 18180, loss: 1.3825\n",
            "step: 18181, loss: 1.3864\n",
            "step: 18182, loss: 1.3822\n",
            "step: 18183, loss: 1.3825\n",
            "step: 18184, loss: 1.3823\n",
            "step: 18185, loss: 1.3843\n",
            "step: 18186, loss: 1.3855\n",
            "step: 18187, loss: 1.3834\n",
            "step: 18188, loss: 1.4125\n",
            "step: 18189, loss: 1.3830\n",
            "step: 18190, loss: 1.3840\n",
            "step: 18191, loss: 1.4422\n",
            "step: 18192, loss: 1.4127\n",
            "step: 18193, loss: 1.3823\n",
            "step: 18194, loss: 1.3830\n",
            "step: 18195, loss: 1.3833\n",
            "step: 18196, loss: 1.3849\n",
            "step: 18197, loss: 1.3831\n",
            "step: 18198, loss: 1.3858\n",
            "step: 18199, loss: 1.4343\n",
            "step: 18200, loss: 1.3820\n",
            "step: 18201, loss: 1.3840\n",
            "step: 18202, loss: 1.4024\n",
            "step: 18203, loss: 1.3854\n",
            "step: 18204, loss: 1.3895\n",
            "step: 18205, loss: 1.4042\n",
            "step: 18206, loss: 1.3864\n",
            "step: 18207, loss: 1.3863\n",
            "step: 18208, loss: 1.3846\n",
            "step: 18209, loss: 1.4145\n",
            "step: 18210, loss: 1.3936\n",
            "step: 18211, loss: 1.4263\n",
            "step: 18212, loss: 1.3903\n",
            "step: 18213, loss: 1.3909\n",
            "step: 18214, loss: 1.3919\n",
            "step: 18215, loss: 1.3945\n",
            "step: 18216, loss: 1.3867\n",
            "step: 18217, loss: 1.3976\n",
            "step: 18218, loss: 1.3873\n",
            "step: 18219, loss: 1.3827\n",
            "step: 18220, loss: 1.3819\n",
            "step: 18221, loss: 1.3934\n",
            "step: 18222, loss: 1.3986\n",
            "step: 18223, loss: 1.3847\n",
            "step: 18224, loss: 1.3853\n",
            "step: 18225, loss: 1.3883\n",
            "step: 18226, loss: 1.3833\n",
            "step: 18227, loss: 1.3899\n",
            "step: 18228, loss: 1.4052\n",
            "step: 18229, loss: 1.3848\n",
            "step: 18230, loss: 1.4024\n",
            "step: 18231, loss: 1.3856\n",
            "step: 18232, loss: 1.3885\n",
            "step: 18233, loss: 1.3847\n",
            "step: 18234, loss: 1.3902\n",
            "step: 18235, loss: 1.4381\n",
            "step: 18236, loss: 1.3916\n",
            "step: 18237, loss: 1.3916\n",
            "step: 18238, loss: 1.4731\n",
            "step: 18239, loss: 1.3832\n",
            "step: 18240, loss: 1.3946\n",
            "step: 18241, loss: 1.3838\n",
            "step: 18242, loss: 1.4025\n",
            "step: 18243, loss: 1.4108\n",
            "step: 18244, loss: 1.3835\n",
            "step: 18245, loss: 1.4374\n",
            "step: 18246, loss: 1.3859\n",
            "step: 18247, loss: 1.3875\n",
            "step: 18248, loss: 1.3870\n",
            "step: 18249, loss: 1.3887\n",
            "step: 18250, loss: 1.4068\n",
            "step: 18251, loss: 1.3916\n",
            "step: 18252, loss: 1.3898\n",
            "step: 18253, loss: 1.4247\n",
            "step: 18254, loss: 1.3835\n",
            "step: 18255, loss: 1.4095\n",
            "step: 18256, loss: 1.3965\n",
            "step: 18257, loss: 1.3857\n",
            "step: 18258, loss: 1.3837\n",
            "step: 18259, loss: 1.3880\n",
            "step: 18260, loss: 1.3965\n",
            "step: 18261, loss: 1.3908\n",
            "step: 18262, loss: 1.3964\n",
            "step: 18263, loss: 1.4284\n",
            "step: 18264, loss: 1.3967\n",
            "step: 18265, loss: 1.3894\n",
            "step: 18266, loss: 1.3842\n",
            "step: 18267, loss: 1.3852\n",
            "step: 18268, loss: 1.3867\n",
            "step: 18269, loss: 1.4119\n",
            "step: 18270, loss: 1.3912\n",
            "step: 18271, loss: 1.3843\n",
            "step: 18272, loss: 1.4624\n",
            "step: 18273, loss: 1.3879\n",
            "step: 18274, loss: 1.4021\n",
            "step: 18275, loss: 1.4254\n",
            "step: 18276, loss: 1.4080\n",
            "step: 18277, loss: 1.4035\n",
            "step: 18278, loss: 1.4162\n",
            "step: 18279, loss: 1.4024\n",
            "step: 18280, loss: 1.3948\n",
            "step: 18281, loss: 1.4166\n",
            "step: 18282, loss: 1.3831\n",
            "step: 18283, loss: 1.3871\n",
            "step: 18284, loss: 1.4140\n",
            "step: 18285, loss: 1.3844\n",
            "step: 18286, loss: 1.3858\n",
            "step: 18287, loss: 1.3859\n",
            "step: 18288, loss: 1.3900\n",
            "step: 18289, loss: 1.4075\n",
            "step: 18290, loss: 1.3963\n",
            "step: 18291, loss: 1.4378\n",
            "step: 18292, loss: 1.3896\n",
            "step: 18293, loss: 1.3882\n",
            "step: 18294, loss: 1.3916\n",
            "step: 18295, loss: 1.3912\n",
            "step: 18296, loss: 1.3944\n",
            "step: 18297, loss: 1.3875\n",
            "step: 18298, loss: 1.4053\n",
            "step: 18299, loss: 1.4348\n",
            "step: 18300, loss: 1.4114\n",
            "step: 18301, loss: 1.3861\n",
            "step: 18302, loss: 1.3848\n",
            "step: 18303, loss: 1.3894\n",
            "step: 18304, loss: 1.3844\n",
            "step: 18305, loss: 1.3845\n",
            "step: 18306, loss: 1.3951\n",
            "step: 18307, loss: 1.4096\n",
            "step: 18308, loss: 1.4390\n",
            "step: 18309, loss: 1.3857\n",
            "step: 18310, loss: 1.4009\n",
            "step: 18311, loss: 1.3908\n",
            "step: 18312, loss: 1.3895\n",
            "step: 18313, loss: 1.4041\n",
            "step: 18314, loss: 1.3833\n",
            "step: 18315, loss: 1.3885\n",
            "step: 18316, loss: 1.4018\n",
            "step: 18317, loss: 1.4160\n",
            "step: 18318, loss: 1.3849\n",
            "step: 18319, loss: 1.3906\n",
            "step: 18320, loss: 1.5192\n",
            "step: 18321, loss: 1.3843\n",
            "step: 18322, loss: 1.4011\n",
            "step: 18323, loss: 1.4021\n",
            "step: 18324, loss: 1.3858\n",
            "step: 18325, loss: 1.3841\n",
            "step: 18326, loss: 1.4145\n",
            "step: 18327, loss: 1.3972\n",
            "step: 18328, loss: 1.3836\n",
            "step: 18329, loss: 1.4153\n",
            "step: 18330, loss: 1.4138\n",
            "step: 18331, loss: 1.3885\n",
            "step: 18332, loss: 1.3933\n",
            "step: 18333, loss: 1.4305\n",
            "step: 18334, loss: 1.3934\n",
            "step: 18335, loss: 1.4342\n",
            "step: 18336, loss: 1.4084\n",
            "step: 18337, loss: 1.3831\n",
            "step: 18338, loss: 1.4305\n",
            "step: 18339, loss: 1.3834\n",
            "step: 18340, loss: 1.3839\n",
            "step: 18341, loss: 1.3929\n",
            "step: 18342, loss: 1.3834\n",
            "step: 18343, loss: 1.3848\n",
            "step: 18344, loss: 1.3847\n",
            "step: 18345, loss: 1.3863\n",
            "step: 18346, loss: 1.3975\n",
            "step: 18347, loss: 1.4469\n",
            "step: 18348, loss: 1.3897\n",
            "step: 18349, loss: 1.4486\n",
            "step: 18350, loss: 1.3924\n",
            "step: 18351, loss: 1.3828\n",
            "step: 18352, loss: 1.3822\n",
            "step: 18353, loss: 1.4213\n",
            "step: 18354, loss: 1.4623\n",
            "step: 18355, loss: 1.3867\n",
            "step: 18356, loss: 1.3870\n",
            "step: 18357, loss: 1.3878\n",
            "step: 18358, loss: 1.3833\n",
            "step: 18359, loss: 1.3838\n",
            "step: 18360, loss: 1.3862\n",
            "step: 18361, loss: 1.4150\n",
            "step: 18362, loss: 1.3858\n",
            "step: 18363, loss: 1.4147\n",
            "step: 18364, loss: 1.4196\n",
            "step: 18365, loss: 1.3918\n",
            "step: 18366, loss: 1.3907\n",
            "step: 18367, loss: 1.4213\n",
            "step: 18368, loss: 1.3981\n",
            "step: 18369, loss: 1.4111\n",
            "step: 18370, loss: 1.4214\n",
            "step: 18371, loss: 1.3921\n",
            "step: 18372, loss: 1.4857\n",
            "step: 18373, loss: 1.4091\n",
            "step: 18374, loss: 1.3871\n",
            "step: 18375, loss: 1.4494\n",
            "step: 18376, loss: 1.3860\n",
            "step: 18377, loss: 1.4323\n",
            "step: 18378, loss: 1.4100\n",
            "step: 18379, loss: 1.3859\n",
            "step: 18380, loss: 1.4639\n",
            "step: 18381, loss: 1.4134\n",
            "step: 18382, loss: 1.4013\n",
            "step: 18383, loss: 1.3887\n",
            "step: 18384, loss: 1.3859\n",
            "step: 18385, loss: 1.3827\n",
            "step: 18386, loss: 1.4001\n",
            "step: 18387, loss: 1.3848\n",
            "step: 18388, loss: 1.4266\n",
            "step: 18389, loss: 1.3885\n",
            "step: 18390, loss: 1.3927\n",
            "step: 18391, loss: 1.4076\n",
            "step: 18392, loss: 1.4165\n",
            "step: 18393, loss: 1.3996\n",
            "step: 18394, loss: 1.4173\n",
            "step: 18395, loss: 1.4012\n",
            "step: 18396, loss: 1.3852\n",
            "step: 18397, loss: 1.3919\n",
            "step: 18398, loss: 1.3968\n",
            "step: 18399, loss: 1.4123\n",
            "step: 18400, loss: 1.3825\n",
            "step: 18401, loss: 1.3848\n",
            "step: 18402, loss: 1.4018\n",
            "step: 18403, loss: 1.3841\n",
            "step: 18404, loss: 1.3981\n",
            "step: 18405, loss: 1.4042\n",
            "step: 18406, loss: 1.3839\n",
            "step: 18407, loss: 1.3852\n",
            "step: 18408, loss: 1.3857\n",
            "step: 18409, loss: 1.3833\n",
            "step: 18410, loss: 1.5120\n",
            "step: 18411, loss: 1.3895\n",
            "step: 18412, loss: 1.3940\n",
            "step: 18413, loss: 1.4350\n",
            "step: 18414, loss: 1.3964\n",
            "step: 18415, loss: 1.3843\n",
            "step: 18416, loss: 1.3881\n",
            "step: 18417, loss: 1.3889\n",
            "step: 18418, loss: 1.3833\n",
            "step: 18419, loss: 1.4153\n",
            "step: 18420, loss: 1.3832\n",
            "step: 18421, loss: 1.3991\n",
            "step: 18422, loss: 1.3887\n",
            "step: 18423, loss: 1.4458\n",
            "step: 18424, loss: 1.3826\n",
            "step: 18425, loss: 1.3952\n",
            "step: 18426, loss: 1.3828\n",
            "step: 18427, loss: 1.3911\n",
            "step: 18428, loss: 1.3874\n",
            "step: 18429, loss: 1.3868\n",
            "step: 18430, loss: 1.3852\n",
            "step: 18431, loss: 1.3826\n",
            "step: 18432, loss: 1.3900\n",
            "step: 18433, loss: 1.3849\n",
            "step: 18434, loss: 1.4626\n",
            "step: 18435, loss: 1.4278\n",
            "step: 18436, loss: 1.3856\n",
            "step: 18437, loss: 1.3865\n",
            "step: 18438, loss: 1.3959\n",
            "step: 18439, loss: 1.3965\n",
            "step: 18440, loss: 1.4257\n",
            "step: 18441, loss: 1.3889\n",
            "step: 18442, loss: 1.3878\n",
            "step: 18443, loss: 1.4062\n",
            "step: 18444, loss: 1.3838\n",
            "step: 18445, loss: 1.3839\n",
            "step: 18446, loss: 1.3839\n",
            "step: 18447, loss: 1.3876\n",
            "step: 18448, loss: 1.3973\n",
            "step: 18449, loss: 1.3868\n",
            "step: 18450, loss: 1.3848\n",
            "step: 18451, loss: 1.3841\n",
            "step: 18452, loss: 1.4070\n",
            "step: 18453, loss: 1.4080\n",
            "step: 18454, loss: 1.3833\n",
            "step: 18455, loss: 1.3829\n",
            "step: 18456, loss: 1.3833\n",
            "step: 18457, loss: 1.4048\n",
            "step: 18458, loss: 1.4384\n",
            "step: 18459, loss: 1.4942\n",
            "step: 18460, loss: 1.3914\n",
            "step: 18461, loss: 1.3897\n",
            "step: 18462, loss: 1.3838\n",
            "step: 18463, loss: 1.3836\n",
            "step: 18464, loss: 1.4001\n",
            "step: 18465, loss: 1.4128\n",
            "step: 18466, loss: 1.4264\n",
            "step: 18467, loss: 1.3971\n",
            "step: 18468, loss: 1.3880\n",
            "step: 18469, loss: 1.3980\n",
            "step: 18470, loss: 1.3990\n",
            "step: 18471, loss: 1.4251\n",
            "step: 18472, loss: 1.3928\n",
            "step: 18473, loss: 1.4289\n",
            "step: 18474, loss: 1.3859\n",
            "step: 18475, loss: 1.3926\n",
            "step: 18476, loss: 1.3945\n",
            "step: 18477, loss: 1.3854\n",
            "step: 18478, loss: 1.3916\n",
            "step: 18479, loss: 1.3845\n",
            "step: 18480, loss: 1.3891\n",
            "step: 18481, loss: 1.3868\n",
            "step: 18482, loss: 1.3840\n",
            "step: 18483, loss: 1.4229\n",
            "step: 18484, loss: 1.3826\n",
            "step: 18485, loss: 1.3850\n",
            "step: 18486, loss: 1.4125\n",
            "step: 18487, loss: 1.4477\n",
            "step: 18488, loss: 1.3876\n",
            "step: 18489, loss: 1.3838\n",
            "step: 18490, loss: 1.3975\n",
            "step: 18491, loss: 1.4001\n",
            "step: 18492, loss: 1.3838\n",
            "step: 18493, loss: 1.3993\n",
            "step: 18494, loss: 1.3890\n",
            "step: 18495, loss: 1.4201\n",
            "step: 18496, loss: 1.4273\n",
            "step: 18497, loss: 1.3845\n",
            "step: 18498, loss: 1.3897\n",
            "step: 18499, loss: 1.3845\n",
            "step: 18500, loss: 1.3946\n",
            "step: 18501, loss: 1.4136\n",
            "step: 18502, loss: 1.3833\n",
            "step: 18503, loss: 1.4330\n",
            "step: 18504, loss: 1.4120\n",
            "step: 18505, loss: 1.4580\n",
            "step: 18506, loss: 1.3882\n",
            "step: 18507, loss: 1.3849\n",
            "step: 18508, loss: 1.3868\n",
            "step: 18509, loss: 1.3827\n",
            "step: 18510, loss: 1.3857\n",
            "step: 18511, loss: 1.3864\n",
            "step: 18512, loss: 1.3864\n",
            "step: 18513, loss: 1.3854\n",
            "step: 18514, loss: 1.3863\n",
            "step: 18515, loss: 1.3861\n",
            "step: 18516, loss: 1.3907\n",
            "step: 18517, loss: 1.3861\n",
            "step: 18518, loss: 1.3846\n",
            "step: 18519, loss: 1.3823\n",
            "step: 18520, loss: 1.4066\n",
            "step: 18521, loss: 1.3937\n",
            "step: 18522, loss: 1.3820\n",
            "step: 18523, loss: 1.3953\n",
            "step: 18524, loss: 1.4128\n",
            "step: 18525, loss: 1.4117\n",
            "step: 18526, loss: 1.3834\n",
            "step: 18527, loss: 1.4739\n",
            "step: 18528, loss: 1.3977\n",
            "step: 18529, loss: 1.3856\n",
            "step: 18530, loss: 1.3824\n",
            "step: 18531, loss: 1.3986\n",
            "step: 18532, loss: 1.3897\n",
            "step: 18533, loss: 1.3938\n",
            "step: 18534, loss: 1.4241\n",
            "step: 18535, loss: 1.3839\n",
            "step: 18536, loss: 1.3940\n",
            "step: 18537, loss: 1.3833\n",
            "step: 18538, loss: 1.3850\n",
            "step: 18539, loss: 1.3929\n",
            "step: 18540, loss: 1.4601\n",
            "step: 18541, loss: 1.4592\n",
            "step: 18542, loss: 1.3910\n",
            "step: 18543, loss: 1.3920\n",
            "step: 18544, loss: 1.3888\n",
            "step: 18545, loss: 1.3927\n",
            "step: 18546, loss: 1.3887\n",
            "step: 18547, loss: 1.3844\n",
            "step: 18548, loss: 1.3908\n",
            "step: 18549, loss: 1.3844\n",
            "step: 18550, loss: 1.3827\n",
            "step: 18551, loss: 1.4343\n",
            "step: 18552, loss: 1.3938\n",
            "step: 18553, loss: 1.3888\n",
            "step: 18554, loss: 1.3995\n",
            "step: 18555, loss: 1.3882\n",
            "step: 18556, loss: 1.3839\n",
            "step: 18557, loss: 1.3840\n",
            "step: 18558, loss: 1.3947\n",
            "step: 18559, loss: 1.3900\n",
            "step: 18560, loss: 1.3843\n",
            "step: 18561, loss: 1.3900\n",
            "step: 18562, loss: 1.3829\n",
            "step: 18563, loss: 1.3838\n",
            "step: 18564, loss: 1.4109\n",
            "step: 18565, loss: 1.3903\n",
            "step: 18566, loss: 1.3861\n",
            "step: 18567, loss: 1.4981\n",
            "step: 18568, loss: 1.3888\n",
            "step: 18569, loss: 1.3871\n",
            "step: 18570, loss: 1.3863\n",
            "step: 18571, loss: 1.3835\n",
            "step: 18572, loss: 1.3822\n",
            "step: 18573, loss: 1.3881\n",
            "step: 18574, loss: 1.3829\n",
            "step: 18575, loss: 1.4373\n",
            "step: 18576, loss: 1.3832\n",
            "step: 18577, loss: 1.3833\n",
            "step: 18578, loss: 1.3827\n",
            "step: 18579, loss: 1.4089\n",
            "step: 18580, loss: 1.4039\n",
            "step: 18581, loss: 1.4051\n",
            "step: 18582, loss: 1.3858\n",
            "step: 18583, loss: 1.4091\n",
            "step: 18584, loss: 1.3837\n",
            "step: 18585, loss: 1.4422\n",
            "step: 18586, loss: 1.4012\n",
            "step: 18587, loss: 1.3843\n",
            "step: 18588, loss: 1.3934\n",
            "step: 18589, loss: 1.3841\n",
            "step: 18590, loss: 1.3830\n",
            "step: 18591, loss: 1.3833\n",
            "step: 18592, loss: 1.4085\n",
            "step: 18593, loss: 1.3848\n",
            "step: 18594, loss: 1.3873\n",
            "step: 18595, loss: 1.3841\n",
            "step: 18596, loss: 1.4112\n",
            "step: 18597, loss: 1.4129\n",
            "step: 18598, loss: 1.4161\n",
            "step: 18599, loss: 1.4471\n",
            "step: 18600, loss: 1.4166\n",
            "step: 18601, loss: 1.3836\n",
            "step: 18602, loss: 1.3824\n",
            "step: 18603, loss: 1.4062\n",
            "step: 18604, loss: 1.3833\n",
            "step: 18605, loss: 1.3828\n",
            "step: 18606, loss: 1.3817\n",
            "step: 18607, loss: 1.3837\n",
            "step: 18608, loss: 1.3885\n",
            "step: 18609, loss: 1.3965\n",
            "step: 18610, loss: 1.3827\n",
            "step: 18611, loss: 1.4150\n",
            "step: 18612, loss: 1.3821\n",
            "step: 18613, loss: 1.4169\n",
            "step: 18614, loss: 1.4020\n",
            "step: 18615, loss: 1.4499\n",
            "step: 18616, loss: 1.3838\n",
            "step: 18617, loss: 1.4456\n",
            "step: 18618, loss: 1.3833\n",
            "step: 18619, loss: 1.3832\n",
            "step: 18620, loss: 1.3910\n",
            "step: 18621, loss: 1.3999\n",
            "step: 18622, loss: 1.3990\n",
            "step: 18623, loss: 1.3880\n",
            "step: 18624, loss: 1.3870\n",
            "step: 18625, loss: 1.3817\n",
            "step: 18626, loss: 1.3904\n",
            "step: 18627, loss: 1.3825\n",
            "step: 18628, loss: 1.3838\n",
            "step: 18629, loss: 1.4210\n",
            "step: 18630, loss: 1.3841\n",
            "step: 18631, loss: 1.4078\n",
            "step: 18632, loss: 1.3851\n",
            "step: 18633, loss: 1.3866\n",
            "step: 18634, loss: 1.4190\n",
            "step: 18635, loss: 1.3837\n",
            "step: 18636, loss: 1.3835\n",
            "step: 18637, loss: 1.3838\n",
            "step: 18638, loss: 1.3907\n",
            "step: 18639, loss: 1.4200\n",
            "step: 18640, loss: 1.3859\n",
            "step: 18641, loss: 1.3954\n",
            "step: 18642, loss: 1.3929\n",
            "step: 18643, loss: 1.3845\n",
            "step: 18644, loss: 1.3918\n",
            "step: 18645, loss: 1.4015\n",
            "step: 18646, loss: 1.3823\n",
            "step: 18647, loss: 1.3851\n",
            "step: 18648, loss: 1.4283\n",
            "step: 18649, loss: 1.4109\n",
            "step: 18650, loss: 1.3863\n",
            "step: 18651, loss: 1.3859\n",
            "step: 18652, loss: 1.3869\n",
            "step: 18653, loss: 1.3897\n",
            "step: 18654, loss: 1.3818\n",
            "step: 18655, loss: 1.3889\n",
            "step: 18656, loss: 1.4041\n",
            "step: 18657, loss: 1.3805\n",
            "step: 18658, loss: 1.4179\n",
            "step: 18659, loss: 1.4096\n",
            "step: 18660, loss: 1.3942\n",
            "step: 18661, loss: 1.3867\n",
            "step: 18662, loss: 1.3820\n",
            "step: 18663, loss: 1.3851\n",
            "step: 18664, loss: 1.3819\n",
            "step: 18665, loss: 1.4024\n",
            "step: 18666, loss: 1.3832\n",
            "step: 18667, loss: 1.3821\n",
            "step: 18668, loss: 1.4111\n",
            "step: 18669, loss: 1.3911\n",
            "step: 18670, loss: 1.3884\n",
            "step: 18671, loss: 1.3812\n",
            "step: 18672, loss: 1.3904\n",
            "step: 18673, loss: 1.3821\n",
            "step: 18674, loss: 1.4919\n",
            "step: 18675, loss: 1.3821\n",
            "step: 18676, loss: 1.4611\n",
            "step: 18677, loss: 1.3893\n",
            "step: 18678, loss: 1.4187\n",
            "step: 18679, loss: 1.3966\n",
            "step: 18680, loss: 1.3826\n",
            "step: 18681, loss: 1.3957\n",
            "step: 18682, loss: 1.3809\n",
            "step: 18683, loss: 1.3862\n",
            "step: 18684, loss: 1.4384\n",
            "step: 18685, loss: 1.3827\n",
            "step: 18686, loss: 1.3977\n",
            "step: 18687, loss: 1.4158\n",
            "step: 18688, loss: 1.3830\n",
            "step: 18689, loss: 1.3830\n",
            "step: 18690, loss: 1.3812\n",
            "step: 18691, loss: 1.4199\n",
            "step: 18692, loss: 1.4265\n",
            "step: 18693, loss: 1.4033\n",
            "step: 18694, loss: 1.3830\n",
            "step: 18695, loss: 1.3812\n",
            "step: 18696, loss: 1.3944\n",
            "step: 18697, loss: 1.4018\n",
            "step: 18698, loss: 1.3855\n",
            "step: 18699, loss: 1.3907\n",
            "step: 18700, loss: 1.3844\n",
            "step: 18701, loss: 1.3916\n",
            "step: 18702, loss: 1.4681\n",
            "step: 18703, loss: 1.3950\n",
            "step: 18704, loss: 1.3857\n",
            "step: 18705, loss: 1.3857\n",
            "step: 18706, loss: 1.3929\n",
            "step: 18707, loss: 1.3859\n",
            "step: 18708, loss: 1.3928\n",
            "step: 18709, loss: 1.4018\n",
            "step: 18710, loss: 1.3848\n",
            "step: 18711, loss: 1.4310\n",
            "step: 18712, loss: 1.3845\n",
            "step: 18713, loss: 1.4701\n",
            "step: 18714, loss: 1.4451\n",
            "step: 18715, loss: 1.3855\n",
            "step: 18716, loss: 1.3866\n",
            "step: 18717, loss: 1.3844\n",
            "step: 18718, loss: 1.3885\n",
            "step: 18719, loss: 1.4397\n",
            "step: 18720, loss: 1.3833\n",
            "step: 18721, loss: 1.3824\n",
            "step: 18722, loss: 1.3882\n",
            "step: 18723, loss: 1.3885\n",
            "step: 18724, loss: 1.3871\n",
            "step: 18725, loss: 1.3875\n",
            "step: 18726, loss: 1.3858\n",
            "step: 18727, loss: 1.3852\n",
            "step: 18728, loss: 1.4205\n",
            "step: 18729, loss: 1.3880\n",
            "step: 18730, loss: 1.3816\n",
            "step: 18731, loss: 1.3839\n",
            "step: 18732, loss: 1.3836\n",
            "step: 18733, loss: 1.3874\n",
            "step: 18734, loss: 1.4103\n",
            "step: 18735, loss: 1.3868\n",
            "step: 18736, loss: 1.3863\n",
            "step: 18737, loss: 1.3971\n",
            "step: 18738, loss: 1.3829\n",
            "step: 18739, loss: 1.4142\n",
            "step: 18740, loss: 1.3902\n",
            "step: 18741, loss: 1.4176\n",
            "step: 18742, loss: 1.4013\n",
            "step: 18743, loss: 1.3825\n",
            "step: 18744, loss: 1.3865\n",
            "step: 18745, loss: 1.3835\n",
            "step: 18746, loss: 1.3846\n",
            "step: 18747, loss: 1.3986\n",
            "step: 18748, loss: 1.4386\n",
            "step: 18749, loss: 1.4375\n",
            "step: 18750, loss: 1.3930\n",
            "step: 18751, loss: 1.3864\n",
            "step: 18752, loss: 1.3818\n",
            "step: 18753, loss: 1.3844\n",
            "step: 18754, loss: 1.3860\n",
            "step: 18755, loss: 1.3825\n",
            "step: 18756, loss: 1.3984\n",
            "step: 18757, loss: 1.3819\n",
            "step: 18758, loss: 1.3909\n",
            "step: 18759, loss: 1.4317\n",
            "step: 18760, loss: 1.3842\n",
            "step: 18761, loss: 1.3831\n",
            "step: 18762, loss: 1.3802\n",
            "step: 18763, loss: 1.3810\n",
            "step: 18764, loss: 1.3830\n",
            "step: 18765, loss: 1.3821\n",
            "step: 18766, loss: 1.3832\n",
            "step: 18767, loss: 1.3830\n",
            "step: 18768, loss: 1.3834\n",
            "step: 18769, loss: 1.3845\n",
            "step: 18770, loss: 1.3861\n",
            "step: 18771, loss: 1.3843\n",
            "step: 18772, loss: 1.4465\n",
            "step: 18773, loss: 1.3985\n",
            "step: 18774, loss: 1.4042\n",
            "step: 18775, loss: 1.3868\n",
            "step: 18776, loss: 1.4202\n",
            "step: 18777, loss: 1.3853\n",
            "step: 18778, loss: 1.3872\n",
            "step: 18779, loss: 1.3955\n",
            "step: 18780, loss: 1.4074\n",
            "step: 18781, loss: 1.3848\n",
            "step: 18782, loss: 1.3900\n",
            "step: 18783, loss: 1.3933\n",
            "step: 18784, loss: 1.3972\n",
            "step: 18785, loss: 1.3844\n",
            "step: 18786, loss: 1.3885\n",
            "step: 18787, loss: 1.3884\n",
            "step: 18788, loss: 1.3853\n",
            "step: 18789, loss: 1.3848\n",
            "step: 18790, loss: 1.4781\n",
            "step: 18791, loss: 1.3877\n",
            "step: 18792, loss: 1.3850\n",
            "step: 18793, loss: 1.4449\n",
            "step: 18794, loss: 1.3856\n",
            "step: 18795, loss: 1.3835\n",
            "step: 18796, loss: 1.3820\n",
            "step: 18797, loss: 1.3856\n",
            "step: 18798, loss: 1.4181\n",
            "step: 18799, loss: 1.3834\n",
            "step: 18800, loss: 1.3895\n",
            "step: 18801, loss: 1.3960\n",
            "step: 18802, loss: 1.4037\n",
            "step: 18803, loss: 1.3823\n",
            "step: 18804, loss: 1.4099\n",
            "step: 18805, loss: 1.3952\n",
            "step: 18806, loss: 1.3914\n",
            "step: 18807, loss: 1.3998\n",
            "step: 18808, loss: 1.3966\n",
            "step: 18809, loss: 1.4320\n",
            "step: 18810, loss: 1.3854\n",
            "step: 18811, loss: 1.3863\n",
            "step: 18812, loss: 1.4428\n",
            "step: 18813, loss: 1.3945\n",
            "step: 18814, loss: 1.3867\n",
            "step: 18815, loss: 1.3916\n",
            "step: 18816, loss: 1.3963\n",
            "step: 18817, loss: 1.3804\n",
            "step: 18818, loss: 1.3931\n",
            "step: 18819, loss: 1.3827\n",
            "step: 18820, loss: 1.3833\n",
            "step: 18821, loss: 1.3880\n",
            "step: 18822, loss: 1.3841\n",
            "step: 18823, loss: 1.3874\n",
            "step: 18824, loss: 1.3981\n",
            "step: 18825, loss: 1.3922\n",
            "step: 18826, loss: 1.3866\n",
            "step: 18827, loss: 1.3835\n",
            "step: 18828, loss: 1.3902\n",
            "step: 18829, loss: 1.3826\n",
            "step: 18830, loss: 1.3996\n",
            "step: 18831, loss: 1.3867\n",
            "step: 18832, loss: 1.4092\n",
            "step: 18833, loss: 1.3955\n",
            "step: 18834, loss: 1.3884\n",
            "step: 18835, loss: 1.3826\n",
            "step: 18836, loss: 1.3869\n",
            "step: 18837, loss: 1.5096\n",
            "step: 18838, loss: 1.3875\n",
            "step: 18839, loss: 1.3894\n",
            "step: 18840, loss: 1.3845\n",
            "step: 18841, loss: 1.4188\n",
            "step: 18842, loss: 1.3943\n",
            "step: 18843, loss: 1.3843\n",
            "step: 18844, loss: 1.4082\n",
            "step: 18845, loss: 1.3922\n",
            "step: 18846, loss: 1.4207\n",
            "step: 18847, loss: 1.3936\n",
            "step: 18848, loss: 1.3927\n",
            "step: 18849, loss: 1.3815\n",
            "step: 18850, loss: 1.4418\n",
            "step: 18851, loss: 1.3905\n",
            "step: 18852, loss: 1.4350\n",
            "step: 18853, loss: 1.3906\n",
            "step: 18854, loss: 1.4159\n",
            "step: 18855, loss: 1.3862\n",
            "step: 18856, loss: 1.3830\n",
            "step: 18857, loss: 1.3844\n",
            "step: 18858, loss: 1.3818\n",
            "step: 18859, loss: 1.4247\n",
            "step: 18860, loss: 1.4100\n",
            "step: 18861, loss: 1.3859\n",
            "step: 18862, loss: 1.3874\n",
            "step: 18863, loss: 1.4198\n",
            "step: 18864, loss: 1.3819\n",
            "step: 18865, loss: 1.4216\n",
            "step: 18866, loss: 1.3816\n",
            "step: 18867, loss: 1.3823\n",
            "step: 18868, loss: 1.4393\n",
            "step: 18869, loss: 1.3895\n",
            "step: 18870, loss: 1.3864\n",
            "step: 18871, loss: 1.3880\n",
            "step: 18872, loss: 1.4128\n",
            "step: 18873, loss: 1.3829\n",
            "step: 18874, loss: 1.3928\n",
            "step: 18875, loss: 1.3853\n",
            "step: 18876, loss: 1.4006\n",
            "step: 18877, loss: 1.3866\n",
            "step: 18878, loss: 1.3910\n",
            "step: 18879, loss: 1.4576\n",
            "step: 18880, loss: 1.3810\n",
            "step: 18881, loss: 1.3943\n",
            "step: 18882, loss: 1.3856\n",
            "step: 18883, loss: 1.4501\n",
            "step: 18884, loss: 1.3843\n",
            "step: 18885, loss: 1.3880\n",
            "step: 18886, loss: 1.3833\n",
            "step: 18887, loss: 1.3897\n",
            "step: 18888, loss: 1.4018\n",
            "step: 18889, loss: 1.4118\n",
            "step: 18890, loss: 1.3833\n",
            "step: 18891, loss: 1.3852\n",
            "step: 18892, loss: 1.3834\n",
            "step: 18893, loss: 1.3835\n",
            "step: 18894, loss: 1.3907\n",
            "step: 18895, loss: 1.4045\n",
            "step: 18896, loss: 1.3864\n",
            "step: 18897, loss: 1.3854\n",
            "step: 18898, loss: 1.3873\n",
            "step: 18899, loss: 1.3852\n",
            "step: 18900, loss: 1.3820\n",
            "step: 18901, loss: 1.4065\n",
            "step: 18902, loss: 1.3837\n",
            "step: 18903, loss: 1.3920\n",
            "step: 18904, loss: 1.3980\n",
            "step: 18905, loss: 1.3960\n",
            "step: 18906, loss: 1.3822\n",
            "step: 18907, loss: 1.3817\n",
            "step: 18908, loss: 1.4397\n",
            "step: 18909, loss: 1.3812\n",
            "step: 18910, loss: 1.3828\n",
            "step: 18911, loss: 1.3824\n",
            "step: 18912, loss: 1.4741\n",
            "step: 18913, loss: 1.3835\n",
            "step: 18914, loss: 1.3822\n",
            "step: 18915, loss: 1.3826\n",
            "step: 18916, loss: 1.3870\n",
            "step: 18917, loss: 1.4035\n",
            "step: 18918, loss: 1.3946\n",
            "step: 18919, loss: 1.3841\n",
            "step: 18920, loss: 1.3938\n",
            "step: 18921, loss: 1.3981\n",
            "step: 18922, loss: 1.3844\n",
            "step: 18923, loss: 1.4251\n",
            "step: 18924, loss: 1.3879\n",
            "step: 18925, loss: 1.4277\n",
            "step: 18926, loss: 1.4106\n",
            "step: 18927, loss: 1.3857\n",
            "step: 18928, loss: 1.4734\n",
            "step: 18929, loss: 1.4602\n",
            "step: 18930, loss: 1.3928\n",
            "step: 18931, loss: 1.3844\n",
            "step: 18932, loss: 1.3829\n",
            "step: 18933, loss: 1.3836\n",
            "step: 18934, loss: 1.4001\n",
            "step: 18935, loss: 1.3838\n",
            "step: 18936, loss: 1.3928\n",
            "step: 18937, loss: 1.3896\n",
            "step: 18938, loss: 1.3862\n",
            "step: 18939, loss: 1.4129\n",
            "step: 18940, loss: 1.4870\n",
            "step: 18941, loss: 1.3823\n",
            "step: 18942, loss: 1.4081\n",
            "step: 18943, loss: 1.3869\n",
            "step: 18944, loss: 1.4104\n",
            "step: 18945, loss: 1.3844\n",
            "step: 18946, loss: 1.3817\n",
            "step: 18947, loss: 1.3928\n",
            "step: 18948, loss: 1.3848\n",
            "step: 18949, loss: 1.4079\n",
            "step: 18950, loss: 1.3831\n",
            "step: 18951, loss: 1.3829\n",
            "step: 18952, loss: 1.3911\n",
            "step: 18953, loss: 1.3886\n",
            "step: 18954, loss: 1.3849\n",
            "step: 18955, loss: 1.4013\n",
            "step: 18956, loss: 1.3879\n",
            "step: 18957, loss: 1.4308\n",
            "step: 18958, loss: 1.3925\n",
            "step: 18959, loss: 1.3823\n",
            "step: 18960, loss: 1.4129\n",
            "step: 18961, loss: 1.4011\n",
            "step: 18962, loss: 1.4297\n",
            "step: 18963, loss: 1.3831\n",
            "step: 18964, loss: 1.3818\n",
            "step: 18965, loss: 1.4481\n",
            "step: 18966, loss: 1.3870\n",
            "step: 18967, loss: 1.4014\n",
            "step: 18968, loss: 1.3900\n",
            "step: 18969, loss: 1.3887\n",
            "step: 18970, loss: 1.3842\n",
            "step: 18971, loss: 1.3974\n",
            "step: 18972, loss: 1.3893\n",
            "step: 18973, loss: 1.3836\n",
            "step: 18974, loss: 1.3894\n",
            "step: 18975, loss: 1.4157\n",
            "step: 18976, loss: 1.3877\n",
            "step: 18977, loss: 1.4099\n",
            "step: 18978, loss: 1.3997\n",
            "step: 18979, loss: 1.4033\n",
            "step: 18980, loss: 1.3839\n",
            "step: 18981, loss: 1.3843\n",
            "step: 18982, loss: 1.3903\n",
            "step: 18983, loss: 1.3806\n",
            "step: 18984, loss: 1.3804\n",
            "step: 18985, loss: 1.3828\n",
            "step: 18986, loss: 1.3926\n",
            "step: 18987, loss: 1.4524\n",
            "step: 18988, loss: 1.3847\n",
            "step: 18989, loss: 1.4675\n",
            "step: 18990, loss: 1.3821\n",
            "step: 18991, loss: 1.3872\n",
            "step: 18992, loss: 1.3809\n",
            "step: 18993, loss: 1.4003\n",
            "step: 18994, loss: 1.3847\n",
            "step: 18995, loss: 1.4337\n",
            "step: 18996, loss: 1.4083\n",
            "step: 18997, loss: 1.4288\n",
            "step: 18998, loss: 1.3828\n",
            "step: 18999, loss: 1.3805\n",
            "step: 19000, loss: 1.3937\n",
            "saving model to ./models/model_19000.ckpt\n",
            "step: 19001, loss: 1.4086\n",
            "step: 19002, loss: 1.3823\n",
            "step: 19003, loss: 1.3848\n",
            "step: 19004, loss: 1.3828\n",
            "step: 19005, loss: 1.3875\n",
            "step: 19006, loss: 1.3816\n",
            "step: 19007, loss: 1.3836\n",
            "step: 19008, loss: 1.3849\n",
            "step: 19009, loss: 1.3984\n",
            "step: 19010, loss: 1.3839\n",
            "step: 19011, loss: 1.3826\n",
            "step: 19012, loss: 1.3844\n",
            "step: 19013, loss: 1.3823\n",
            "step: 19014, loss: 1.3815\n",
            "step: 19015, loss: 1.3850\n",
            "step: 19016, loss: 1.3817\n",
            "step: 19017, loss: 1.4037\n",
            "step: 19018, loss: 1.3857\n",
            "step: 19019, loss: 1.4446\n",
            "step: 19020, loss: 1.4069\n",
            "step: 19021, loss: 1.3813\n",
            "step: 19022, loss: 1.3827\n",
            "step: 19023, loss: 1.4016\n",
            "step: 19024, loss: 1.4622\n",
            "step: 19025, loss: 1.3810\n",
            "step: 19026, loss: 1.3977\n",
            "step: 19027, loss: 1.3825\n",
            "step: 19028, loss: 1.3956\n",
            "step: 19029, loss: 1.4179\n",
            "step: 19030, loss: 1.3832\n",
            "step: 19031, loss: 1.3829\n",
            "step: 19032, loss: 1.3858\n",
            "step: 19033, loss: 1.3927\n",
            "step: 19034, loss: 1.3988\n",
            "step: 19035, loss: 1.3820\n",
            "step: 19036, loss: 1.4065\n",
            "step: 19037, loss: 1.3816\n",
            "step: 19038, loss: 1.3807\n",
            "step: 19039, loss: 1.4275\n",
            "step: 19040, loss: 1.4391\n",
            "step: 19041, loss: 1.3875\n",
            "step: 19042, loss: 1.3853\n",
            "step: 19043, loss: 1.3875\n",
            "step: 19044, loss: 1.3850\n",
            "step: 19045, loss: 1.3970\n",
            "step: 19046, loss: 1.3869\n",
            "step: 19047, loss: 1.3892\n",
            "step: 19048, loss: 1.3879\n",
            "step: 19049, loss: 1.3910\n",
            "step: 19050, loss: 1.3977\n",
            "step: 19051, loss: 1.3970\n",
            "step: 19052, loss: 1.4054\n",
            "step: 19053, loss: 1.3878\n",
            "step: 19054, loss: 1.3880\n",
            "step: 19055, loss: 1.3804\n",
            "step: 19056, loss: 1.3847\n",
            "step: 19057, loss: 1.4056\n",
            "step: 19058, loss: 1.3824\n",
            "step: 19059, loss: 1.3841\n",
            "step: 19060, loss: 1.3832\n",
            "step: 19061, loss: 1.3834\n",
            "step: 19062, loss: 1.3826\n",
            "step: 19063, loss: 1.3866\n",
            "step: 19064, loss: 1.3855\n",
            "step: 19065, loss: 1.3862\n",
            "step: 19066, loss: 1.3830\n",
            "step: 19067, loss: 1.3837\n",
            "step: 19068, loss: 1.3859\n",
            "step: 19069, loss: 1.3815\n",
            "step: 19070, loss: 1.3855\n",
            "step: 19071, loss: 1.3829\n",
            "step: 19072, loss: 1.4061\n",
            "step: 19073, loss: 1.4125\n",
            "step: 19074, loss: 1.4221\n",
            "step: 19075, loss: 1.3821\n",
            "step: 19076, loss: 1.3871\n",
            "step: 19077, loss: 1.4488\n",
            "step: 19078, loss: 1.3827\n",
            "step: 19079, loss: 1.3824\n",
            "step: 19080, loss: 1.3804\n",
            "step: 19081, loss: 1.3827\n",
            "step: 19082, loss: 1.3856\n",
            "step: 19083, loss: 1.4237\n",
            "step: 19084, loss: 1.4161\n",
            "step: 19085, loss: 1.4007\n",
            "step: 19086, loss: 1.4187\n",
            "step: 19087, loss: 1.3803\n",
            "step: 19088, loss: 1.3815\n",
            "step: 19089, loss: 1.3838\n",
            "step: 19090, loss: 1.3813\n",
            "step: 19091, loss: 1.4018\n",
            "step: 19092, loss: 1.3840\n",
            "step: 19093, loss: 1.3807\n",
            "step: 19094, loss: 1.4263\n",
            "step: 19095, loss: 1.3856\n",
            "step: 19096, loss: 1.3883\n",
            "step: 19097, loss: 1.3916\n",
            "step: 19098, loss: 1.3823\n",
            "step: 19099, loss: 1.3841\n",
            "step: 19100, loss: 1.3832\n",
            "step: 19101, loss: 1.3913\n",
            "step: 19102, loss: 1.3864\n",
            "step: 19103, loss: 1.3813\n",
            "step: 19104, loss: 1.3872\n",
            "step: 19105, loss: 1.3886\n",
            "step: 19106, loss: 1.3858\n",
            "step: 19107, loss: 1.3862\n",
            "step: 19108, loss: 1.3836\n",
            "step: 19109, loss: 1.3845\n",
            "step: 19110, loss: 1.4014\n",
            "step: 19111, loss: 1.4035\n",
            "step: 19112, loss: 1.3838\n",
            "step: 19113, loss: 1.3828\n",
            "step: 19114, loss: 1.4005\n",
            "step: 19115, loss: 1.3932\n",
            "step: 19116, loss: 1.3831\n",
            "step: 19117, loss: 1.3851\n",
            "step: 19118, loss: 1.3814\n",
            "step: 19119, loss: 1.3956\n",
            "step: 19120, loss: 1.3817\n",
            "step: 19121, loss: 1.4273\n",
            "step: 19122, loss: 1.3833\n",
            "step: 19123, loss: 1.4025\n",
            "step: 19124, loss: 1.4090\n",
            "step: 19125, loss: 1.3888\n",
            "step: 19126, loss: 1.3805\n",
            "step: 19127, loss: 1.4002\n",
            "step: 19128, loss: 1.3939\n",
            "step: 19129, loss: 1.3847\n",
            "step: 19130, loss: 1.4040\n",
            "step: 19131, loss: 1.3912\n",
            "step: 19132, loss: 1.4125\n",
            "step: 19133, loss: 1.3809\n",
            "step: 19134, loss: 1.3871\n",
            "step: 19135, loss: 1.3825\n",
            "step: 19136, loss: 1.3834\n",
            "step: 19137, loss: 1.3854\n",
            "step: 19138, loss: 1.3956\n",
            "step: 19139, loss: 1.3920\n",
            "step: 19140, loss: 1.4118\n",
            "step: 19141, loss: 1.3923\n",
            "step: 19142, loss: 1.4011\n",
            "step: 19143, loss: 1.4818\n",
            "step: 19144, loss: 1.5316\n",
            "step: 19145, loss: 1.3813\n",
            "step: 19146, loss: 1.3828\n",
            "step: 19147, loss: 1.4331\n",
            "step: 19148, loss: 1.4187\n",
            "step: 19149, loss: 1.3936\n",
            "step: 19150, loss: 1.3807\n",
            "step: 19151, loss: 1.3819\n",
            "step: 19152, loss: 1.4002\n",
            "step: 19153, loss: 1.3837\n",
            "step: 19154, loss: 1.3887\n",
            "step: 19155, loss: 1.4027\n",
            "step: 19156, loss: 1.3819\n",
            "step: 19157, loss: 1.4637\n",
            "step: 19158, loss: 1.3830\n",
            "step: 19159, loss: 1.4274\n",
            "step: 19160, loss: 1.3840\n",
            "step: 19161, loss: 1.3936\n",
            "step: 19162, loss: 1.3934\n",
            "step: 19163, loss: 1.3853\n",
            "step: 19164, loss: 1.3813\n",
            "step: 19165, loss: 1.3843\n",
            "step: 19166, loss: 1.3820\n",
            "step: 19167, loss: 1.3828\n",
            "step: 19168, loss: 1.3828\n",
            "step: 19169, loss: 1.3992\n",
            "step: 19170, loss: 1.3847\n",
            "step: 19171, loss: 1.3843\n",
            "step: 19172, loss: 1.4310\n",
            "step: 19173, loss: 1.3827\n",
            "step: 19174, loss: 1.3896\n",
            "step: 19175, loss: 1.4270\n",
            "step: 19176, loss: 1.3846\n",
            "step: 19177, loss: 1.4077\n",
            "step: 19178, loss: 1.4041\n",
            "step: 19179, loss: 1.3970\n",
            "step: 19180, loss: 1.3821\n",
            "step: 19181, loss: 1.3828\n",
            "step: 19182, loss: 1.4018\n",
            "step: 19183, loss: 1.3827\n",
            "step: 19184, loss: 1.4068\n",
            "step: 19185, loss: 1.4010\n",
            "step: 19186, loss: 1.3884\n",
            "step: 19187, loss: 1.3843\n",
            "step: 19188, loss: 1.3904\n",
            "step: 19189, loss: 1.4286\n",
            "step: 19190, loss: 1.3861\n",
            "step: 19191, loss: 1.3913\n",
            "step: 19192, loss: 1.3849\n",
            "step: 19193, loss: 1.3846\n",
            "step: 19194, loss: 1.3895\n",
            "step: 19195, loss: 1.4319\n",
            "step: 19196, loss: 1.4349\n",
            "step: 19197, loss: 1.3848\n",
            "step: 19198, loss: 1.3931\n",
            "step: 19199, loss: 1.3848\n",
            "step: 19200, loss: 1.3974\n",
            "step: 19201, loss: 1.3903\n",
            "step: 19202, loss: 1.4958\n",
            "step: 19203, loss: 1.3855\n",
            "step: 19204, loss: 1.4186\n",
            "step: 19205, loss: 1.4173\n",
            "step: 19206, loss: 1.3846\n",
            "step: 19207, loss: 1.4291\n",
            "step: 19208, loss: 1.3834\n",
            "step: 19209, loss: 1.4086\n",
            "step: 19210, loss: 1.3838\n",
            "step: 19211, loss: 1.4042\n",
            "step: 19212, loss: 1.3881\n",
            "step: 19213, loss: 1.4376\n",
            "step: 19214, loss: 1.3842\n",
            "step: 19215, loss: 1.3841\n",
            "step: 19216, loss: 1.3848\n",
            "step: 19217, loss: 1.3820\n",
            "step: 19218, loss: 1.4171\n",
            "step: 19219, loss: 1.3807\n",
            "step: 19220, loss: 1.3822\n",
            "step: 19221, loss: 1.4400\n",
            "step: 19222, loss: 1.4271\n",
            "step: 19223, loss: 1.3827\n",
            "step: 19224, loss: 1.3878\n",
            "step: 19225, loss: 1.3855\n",
            "step: 19226, loss: 1.3844\n",
            "step: 19227, loss: 1.3818\n",
            "step: 19228, loss: 1.4228\n",
            "step: 19229, loss: 1.3871\n",
            "step: 19230, loss: 1.3884\n",
            "step: 19231, loss: 1.3833\n",
            "step: 19232, loss: 1.3897\n",
            "step: 19233, loss: 1.3982\n",
            "step: 19234, loss: 1.3977\n",
            "step: 19235, loss: 1.3836\n",
            "step: 19236, loss: 1.3821\n",
            "step: 19237, loss: 1.3819\n",
            "step: 19238, loss: 1.3842\n",
            "step: 19239, loss: 1.3829\n",
            "step: 19240, loss: 1.3901\n",
            "step: 19241, loss: 1.3826\n",
            "step: 19242, loss: 1.4034\n",
            "step: 19243, loss: 1.3944\n",
            "step: 19244, loss: 1.3934\n",
            "step: 19245, loss: 1.3871\n",
            "step: 19246, loss: 1.4153\n",
            "step: 19247, loss: 1.3869\n",
            "step: 19248, loss: 1.3822\n",
            "step: 19249, loss: 1.4614\n",
            "step: 19250, loss: 1.4196\n",
            "step: 19251, loss: 1.3898\n",
            "step: 19252, loss: 1.3815\n",
            "step: 19253, loss: 1.4208\n",
            "step: 19254, loss: 1.4055\n",
            "step: 19255, loss: 1.3916\n",
            "step: 19256, loss: 1.3841\n",
            "step: 19257, loss: 1.3915\n",
            "step: 19258, loss: 1.3994\n",
            "step: 19259, loss: 1.3858\n",
            "step: 19260, loss: 1.4343\n",
            "step: 19261, loss: 1.3868\n",
            "step: 19262, loss: 1.3817\n",
            "step: 19263, loss: 1.3863\n",
            "step: 19264, loss: 1.3884\n",
            "step: 19265, loss: 1.4165\n",
            "step: 19266, loss: 1.3897\n",
            "step: 19267, loss: 1.3840\n",
            "step: 19268, loss: 1.3883\n",
            "step: 19269, loss: 1.3863\n",
            "step: 19270, loss: 1.4351\n",
            "step: 19271, loss: 1.3917\n",
            "step: 19272, loss: 1.3996\n",
            "step: 19273, loss: 1.4383\n",
            "step: 19274, loss: 1.3863\n",
            "step: 19275, loss: 1.4055\n",
            "step: 19276, loss: 1.3881\n",
            "step: 19277, loss: 1.3855\n",
            "step: 19278, loss: 1.4027\n",
            "step: 19279, loss: 1.4753\n",
            "step: 19280, loss: 1.4093\n",
            "step: 19281, loss: 1.3856\n",
            "step: 19282, loss: 1.3840\n",
            "step: 19283, loss: 1.3916\n",
            "step: 19284, loss: 1.3880\n",
            "step: 19285, loss: 1.4537\n",
            "step: 19286, loss: 1.4254\n",
            "step: 19287, loss: 1.3867\n",
            "step: 19288, loss: 1.3858\n",
            "step: 19289, loss: 1.3859\n",
            "step: 19290, loss: 1.4246\n",
            "step: 19291, loss: 1.3868\n",
            "step: 19292, loss: 1.5287\n",
            "step: 19293, loss: 1.4404\n",
            "step: 19294, loss: 1.3859\n",
            "step: 19295, loss: 1.4340\n",
            "step: 19296, loss: 1.4772\n",
            "step: 19297, loss: 1.4090\n",
            "step: 19298, loss: 1.4556\n",
            "step: 19299, loss: 1.4506\n",
            "step: 19300, loss: 1.4243\n",
            "step: 19301, loss: 1.4201\n",
            "step: 19302, loss: 1.3970\n",
            "step: 19303, loss: 1.4177\n",
            "step: 19304, loss: 1.4205\n",
            "step: 19305, loss: 1.4258\n",
            "step: 19306, loss: 1.3884\n",
            "step: 19307, loss: 1.4140\n",
            "step: 19308, loss: 1.3952\n",
            "step: 19309, loss: 1.4852\n",
            "step: 19310, loss: 1.4227\n",
            "step: 19311, loss: 1.3905\n",
            "step: 19312, loss: 1.3916\n",
            "step: 19313, loss: 1.3937\n",
            "step: 19314, loss: 1.4026\n",
            "step: 19315, loss: 1.4547\n",
            "step: 19316, loss: 1.4306\n",
            "step: 19317, loss: 1.4471\n",
            "step: 19318, loss: 1.4031\n",
            "step: 19319, loss: 1.4065\n",
            "step: 19320, loss: 1.4125\n",
            "step: 19321, loss: 1.4255\n",
            "step: 19322, loss: 1.3883\n",
            "step: 19323, loss: 1.4448\n",
            "step: 19324, loss: 1.3899\n",
            "step: 19325, loss: 1.4027\n",
            "step: 19326, loss: 1.3982\n",
            "step: 19327, loss: 1.3962\n",
            "step: 19328, loss: 1.3905\n",
            "step: 19329, loss: 1.4092\n",
            "step: 19330, loss: 1.4091\n",
            "step: 19331, loss: 1.4110\n",
            "step: 19332, loss: 1.3997\n",
            "step: 19333, loss: 1.3923\n",
            "step: 19334, loss: 1.3968\n",
            "step: 19335, loss: 1.4647\n",
            "step: 19336, loss: 1.4252\n",
            "step: 19337, loss: 1.4110\n",
            "step: 19338, loss: 1.3887\n",
            "step: 19339, loss: 1.3914\n",
            "step: 19340, loss: 1.3871\n",
            "step: 19341, loss: 1.4089\n",
            "step: 19342, loss: 1.4073\n",
            "step: 19343, loss: 1.3872\n",
            "step: 19344, loss: 1.3852\n",
            "step: 19345, loss: 1.3881\n",
            "step: 19346, loss: 1.3922\n",
            "step: 19347, loss: 1.3862\n",
            "step: 19348, loss: 1.3889\n",
            "step: 19349, loss: 1.3972\n",
            "step: 19350, loss: 1.4611\n",
            "step: 19351, loss: 1.3886\n",
            "step: 19352, loss: 1.3883\n",
            "step: 19353, loss: 1.3896\n",
            "step: 19354, loss: 1.4158\n",
            "step: 19355, loss: 1.4411\n",
            "step: 19356, loss: 1.3877\n",
            "step: 19357, loss: 1.3864\n",
            "step: 19358, loss: 1.4121\n",
            "step: 19359, loss: 1.3911\n",
            "step: 19360, loss: 1.4078\n",
            "step: 19361, loss: 1.3851\n",
            "step: 19362, loss: 1.4212\n",
            "step: 19363, loss: 1.3851\n",
            "step: 19364, loss: 1.3915\n",
            "step: 19365, loss: 1.3967\n",
            "step: 19366, loss: 1.3900\n",
            "step: 19367, loss: 1.3919\n",
            "step: 19368, loss: 1.4255\n",
            "step: 19369, loss: 1.3895\n",
            "step: 19370, loss: 1.3872\n",
            "step: 19371, loss: 1.4342\n",
            "step: 19372, loss: 1.4121\n",
            "step: 19373, loss: 1.5116\n",
            "step: 19374, loss: 1.3867\n",
            "step: 19375, loss: 1.3920\n",
            "step: 19376, loss: 1.3921\n",
            "step: 19377, loss: 1.3876\n",
            "step: 19378, loss: 1.4014\n",
            "step: 19379, loss: 1.4140\n",
            "step: 19380, loss: 1.4266\n",
            "step: 19381, loss: 1.3869\n",
            "step: 19382, loss: 1.3961\n",
            "step: 19383, loss: 1.4688\n",
            "step: 19384, loss: 1.4190\n",
            "step: 19385, loss: 1.3879\n",
            "step: 19386, loss: 1.3998\n",
            "step: 19387, loss: 1.4358\n",
            "step: 19388, loss: 1.4005\n",
            "step: 19389, loss: 1.4436\n",
            "step: 19390, loss: 1.4410\n",
            "step: 19391, loss: 1.5006\n",
            "step: 19392, loss: 1.3958\n",
            "step: 19393, loss: 1.3860\n",
            "step: 19394, loss: 1.4576\n",
            "step: 19395, loss: 1.4023\n",
            "step: 19396, loss: 1.4554\n",
            "step: 19397, loss: 1.4013\n",
            "step: 19398, loss: 1.3879\n",
            "step: 19399, loss: 1.4153\n",
            "step: 19400, loss: 1.4398\n",
            "step: 19401, loss: 1.3900\n",
            "step: 19402, loss: 1.3892\n",
            "step: 19403, loss: 1.4196\n",
            "step: 19404, loss: 1.3902\n",
            "step: 19405, loss: 1.3867\n",
            "step: 19406, loss: 1.4135\n",
            "step: 19407, loss: 1.3930\n",
            "step: 19408, loss: 1.3864\n",
            "step: 19409, loss: 1.4432\n",
            "step: 19410, loss: 1.3975\n",
            "step: 19411, loss: 1.3945\n",
            "step: 19412, loss: 1.3944\n",
            "step: 19413, loss: 1.4092\n",
            "step: 19414, loss: 1.3998\n",
            "step: 19415, loss: 1.4306\n",
            "step: 19416, loss: 1.4173\n",
            "step: 19417, loss: 1.4557\n",
            "step: 19418, loss: 1.5008\n",
            "step: 19419, loss: 1.4048\n",
            "step: 19420, loss: 1.4042\n",
            "step: 19421, loss: 1.3939\n",
            "step: 19422, loss: 1.4510\n",
            "step: 19423, loss: 1.4085\n",
            "step: 19424, loss: 1.4048\n",
            "step: 19425, loss: 1.4107\n",
            "step: 19426, loss: 1.3867\n",
            "step: 19427, loss: 1.3856\n",
            "step: 19428, loss: 1.4539\n",
            "step: 19429, loss: 1.3860\n",
            "step: 19430, loss: 1.3936\n",
            "step: 19431, loss: 1.3866\n",
            "step: 19432, loss: 1.4036\n",
            "step: 19433, loss: 1.3900\n",
            "step: 19434, loss: 1.3918\n",
            "step: 19435, loss: 1.3947\n",
            "step: 19436, loss: 1.4565\n",
            "step: 19437, loss: 1.3918\n",
            "step: 19438, loss: 1.4219\n",
            "step: 19439, loss: 1.3899\n",
            "step: 19440, loss: 1.3850\n",
            "step: 19441, loss: 1.3971\n",
            "step: 19442, loss: 1.3878\n",
            "step: 19443, loss: 1.3945\n",
            "step: 19444, loss: 1.3917\n",
            "step: 19445, loss: 1.4163\n",
            "step: 19446, loss: 1.3867\n",
            "step: 19447, loss: 1.3909\n",
            "step: 19448, loss: 1.4398\n",
            "step: 19449, loss: 1.4812\n",
            "step: 19450, loss: 1.3876\n",
            "step: 19451, loss: 1.3895\n",
            "step: 19452, loss: 1.3920\n",
            "step: 19453, loss: 1.4113\n",
            "step: 19454, loss: 1.4000\n",
            "step: 19455, loss: 1.4587\n",
            "step: 19456, loss: 1.4865\n",
            "step: 19457, loss: 1.4519\n",
            "step: 19458, loss: 1.4364\n",
            "step: 19459, loss: 1.4604\n",
            "step: 19460, loss: 1.5244\n",
            "step: 19461, loss: 1.4535\n",
            "step: 19462, loss: 1.4277\n",
            "step: 19463, loss: 1.4919\n",
            "step: 19464, loss: 1.5006\n",
            "step: 19465, loss: 1.4446\n",
            "step: 19466, loss: 1.4460\n",
            "step: 19467, loss: 1.4470\n",
            "step: 19468, loss: 1.4636\n",
            "step: 19469, loss: 1.4557\n",
            "step: 19470, loss: 1.4252\n",
            "step: 19471, loss: 1.4143\n",
            "step: 19472, loss: 1.4243\n",
            "step: 19473, loss: 1.4184\n",
            "step: 19474, loss: 1.4257\n",
            "step: 19475, loss: 1.5599\n",
            "step: 19476, loss: 1.4089\n",
            "step: 19477, loss: 1.4087\n",
            "step: 19478, loss: 1.4659\n",
            "step: 19479, loss: 1.3943\n",
            "step: 19480, loss: 1.3968\n",
            "step: 19481, loss: 1.3964\n",
            "step: 19482, loss: 1.4056\n",
            "step: 19483, loss: 1.3953\n",
            "step: 19484, loss: 1.4311\n",
            "step: 19485, loss: 1.4174\n",
            "step: 19486, loss: 1.4012\n",
            "step: 19487, loss: 1.3966\n",
            "step: 19488, loss: 1.4680\n",
            "step: 19489, loss: 1.4179\n",
            "step: 19490, loss: 1.4484\n",
            "step: 19491, loss: 1.3935\n",
            "step: 19492, loss: 1.4143\n",
            "step: 19493, loss: 1.3994\n",
            "step: 19494, loss: 1.3915\n",
            "step: 19495, loss: 1.4000\n",
            "step: 19496, loss: 1.4404\n",
            "step: 19497, loss: 1.4595\n",
            "step: 19498, loss: 1.3944\n",
            "step: 19499, loss: 1.4177\n",
            "step: 19500, loss: 1.4144\n",
            "step: 19501, loss: 1.3910\n",
            "step: 19502, loss: 1.3918\n",
            "step: 19503, loss: 1.3907\n",
            "step: 19504, loss: 1.4245\n",
            "step: 19505, loss: 1.4053\n",
            "step: 19506, loss: 1.4153\n",
            "step: 19507, loss: 1.4356\n",
            "step: 19508, loss: 1.4015\n",
            "step: 19509, loss: 1.3944\n",
            "step: 19510, loss: 1.3940\n",
            "step: 19511, loss: 1.3921\n",
            "step: 19512, loss: 1.4116\n",
            "step: 19513, loss: 1.4236\n",
            "step: 19514, loss: 1.4384\n",
            "step: 19515, loss: 1.3865\n",
            "step: 19516, loss: 1.4010\n",
            "step: 19517, loss: 1.4049\n",
            "step: 19518, loss: 1.4150\n",
            "step: 19519, loss: 1.3886\n",
            "step: 19520, loss: 1.4091\n",
            "step: 19521, loss: 1.4208\n",
            "step: 19522, loss: 1.3927\n",
            "step: 19523, loss: 1.3934\n",
            "step: 19524, loss: 1.3959\n",
            "step: 19525, loss: 1.3899\n",
            "step: 19526, loss: 1.4242\n",
            "step: 19527, loss: 1.4428\n",
            "step: 19528, loss: 1.3907\n",
            "step: 19529, loss: 1.4292\n",
            "step: 19530, loss: 1.4379\n",
            "step: 19531, loss: 1.4015\n",
            "step: 19532, loss: 1.3900\n",
            "step: 19533, loss: 1.4293\n",
            "step: 19534, loss: 1.4139\n",
            "step: 19535, loss: 1.3915\n",
            "step: 19536, loss: 1.3984\n",
            "step: 19537, loss: 1.3968\n",
            "step: 19538, loss: 1.3915\n",
            "step: 19539, loss: 1.3882\n",
            "step: 19540, loss: 1.3891\n",
            "step: 19541, loss: 1.4499\n",
            "step: 19542, loss: 1.4084\n",
            "step: 19543, loss: 1.3931\n",
            "step: 19544, loss: 1.4581\n",
            "step: 19545, loss: 1.5097\n",
            "step: 19546, loss: 1.3915\n",
            "step: 19547, loss: 1.4427\n",
            "step: 19548, loss: 1.4260\n",
            "step: 19549, loss: 1.3884\n",
            "step: 19550, loss: 1.3884\n",
            "step: 19551, loss: 1.3971\n",
            "step: 19552, loss: 1.3946\n",
            "step: 19553, loss: 1.4657\n",
            "step: 19554, loss: 1.3866\n",
            "step: 19555, loss: 1.3947\n",
            "step: 19556, loss: 1.4430\n",
            "step: 19557, loss: 1.4002\n",
            "step: 19558, loss: 1.4280\n",
            "step: 19559, loss: 1.3978\n",
            "step: 19560, loss: 1.4486\n",
            "step: 19561, loss: 1.3888\n",
            "step: 19562, loss: 1.4104\n",
            "step: 19563, loss: 1.4341\n",
            "step: 19564, loss: 1.3909\n",
            "step: 19565, loss: 1.3898\n",
            "step: 19566, loss: 1.4533\n",
            "step: 19567, loss: 1.4143\n",
            "step: 19568, loss: 1.3897\n",
            "step: 19569, loss: 1.4025\n",
            "step: 19570, loss: 1.3947\n",
            "step: 19571, loss: 1.4632\n",
            "step: 19572, loss: 1.4576\n",
            "step: 19573, loss: 1.4332\n",
            "step: 19574, loss: 1.4074\n",
            "step: 19575, loss: 1.4318\n",
            "step: 19576, loss: 1.3982\n",
            "step: 19577, loss: 1.4111\n",
            "step: 19578, loss: 1.3892\n",
            "step: 19579, loss: 1.3946\n",
            "step: 19580, loss: 1.3903\n",
            "step: 19581, loss: 1.3956\n",
            "step: 19582, loss: 1.4356\n",
            "step: 19583, loss: 1.3891\n",
            "step: 19584, loss: 1.3923\n",
            "step: 19585, loss: 1.4073\n",
            "step: 19586, loss: 1.5141\n",
            "step: 19587, loss: 1.3933\n",
            "step: 19588, loss: 1.4115\n",
            "step: 19589, loss: 1.3878\n",
            "step: 19590, loss: 1.3893\n",
            "step: 19591, loss: 1.3932\n",
            "step: 19592, loss: 1.4999\n",
            "step: 19593, loss: 1.3875\n",
            "step: 19594, loss: 1.4162\n",
            "step: 19595, loss: 1.4041\n",
            "step: 19596, loss: 1.4149\n",
            "step: 19597, loss: 1.4102\n",
            "step: 19598, loss: 1.4259\n",
            "step: 19599, loss: 1.4772\n",
            "step: 19600, loss: 1.3976\n",
            "step: 19601, loss: 1.3863\n",
            "step: 19602, loss: 1.4294\n",
            "step: 19603, loss: 1.4139\n",
            "step: 19604, loss: 1.4154\n",
            "step: 19605, loss: 1.4112\n",
            "step: 19606, loss: 1.4040\n",
            "step: 19607, loss: 1.4530\n",
            "step: 19608, loss: 1.3879\n",
            "step: 19609, loss: 1.4035\n",
            "step: 19610, loss: 1.4057\n",
            "step: 19611, loss: 1.3915\n",
            "step: 19612, loss: 1.4150\n",
            "step: 19613, loss: 1.3883\n",
            "step: 19614, loss: 1.4029\n",
            "step: 19615, loss: 1.4055\n",
            "step: 19616, loss: 1.4037\n",
            "step: 19617, loss: 1.4148\n",
            "step: 19618, loss: 1.3898\n",
            "step: 19619, loss: 1.4204\n",
            "step: 19620, loss: 1.4183\n",
            "step: 19621, loss: 1.4099\n",
            "step: 19622, loss: 1.3879\n",
            "step: 19623, loss: 1.3977\n",
            "step: 19624, loss: 1.4098\n",
            "step: 19625, loss: 1.3920\n",
            "step: 19626, loss: 1.4025\n",
            "step: 19627, loss: 1.3944\n",
            "step: 19628, loss: 1.4430\n",
            "step: 19629, loss: 1.3959\n",
            "step: 19630, loss: 1.4149\n",
            "step: 19631, loss: 1.3878\n",
            "step: 19632, loss: 1.3879\n",
            "step: 19633, loss: 1.3939\n",
            "step: 19634, loss: 1.4334\n",
            "step: 19635, loss: 1.3865\n",
            "step: 19636, loss: 1.3886\n",
            "step: 19637, loss: 1.3926\n",
            "step: 19638, loss: 1.3984\n",
            "step: 19639, loss: 1.4135\n",
            "step: 19640, loss: 1.4279\n",
            "step: 19641, loss: 1.4902\n",
            "step: 19642, loss: 1.3944\n",
            "step: 19643, loss: 1.3890\n",
            "step: 19644, loss: 1.3953\n",
            "step: 19645, loss: 1.3884\n",
            "step: 19646, loss: 1.4426\n",
            "step: 19647, loss: 1.3888\n",
            "step: 19648, loss: 1.4140\n",
            "step: 19649, loss: 1.4031\n",
            "step: 19650, loss: 1.3948\n",
            "step: 19651, loss: 1.3919\n",
            "step: 19652, loss: 1.3907\n",
            "step: 19653, loss: 1.3887\n",
            "step: 19654, loss: 1.3929\n",
            "step: 19655, loss: 1.4854\n",
            "step: 19656, loss: 1.3972\n",
            "step: 19657, loss: 1.3885\n",
            "step: 19658, loss: 1.3853\n",
            "step: 19659, loss: 1.3905\n",
            "step: 19660, loss: 1.3862\n",
            "step: 19661, loss: 1.4582\n",
            "step: 19662, loss: 1.3878\n",
            "step: 19663, loss: 1.3880\n",
            "step: 19664, loss: 1.3908\n",
            "step: 19665, loss: 1.4221\n",
            "step: 19666, loss: 1.3997\n",
            "step: 19667, loss: 1.4067\n",
            "step: 19668, loss: 1.3871\n",
            "step: 19669, loss: 1.4608\n",
            "step: 19670, loss: 1.3962\n",
            "step: 19671, loss: 1.4246\n",
            "step: 19672, loss: 1.3907\n",
            "step: 19673, loss: 1.3854\n",
            "step: 19674, loss: 1.3846\n",
            "step: 19675, loss: 1.3867\n",
            "step: 19676, loss: 1.4104\n",
            "step: 19677, loss: 1.3936\n",
            "step: 19678, loss: 1.4421\n",
            "step: 19679, loss: 1.3908\n",
            "step: 19680, loss: 1.3914\n",
            "step: 19681, loss: 1.3958\n",
            "step: 19682, loss: 1.3858\n",
            "step: 19683, loss: 1.4181\n",
            "step: 19684, loss: 1.3904\n",
            "step: 19685, loss: 1.3974\n",
            "step: 19686, loss: 1.3853\n",
            "step: 19687, loss: 1.3866\n",
            "step: 19688, loss: 1.3835\n",
            "step: 19689, loss: 1.3886\n",
            "step: 19690, loss: 1.3926\n",
            "step: 19691, loss: 1.4414\n",
            "step: 19692, loss: 1.3963\n",
            "step: 19693, loss: 1.3882\n",
            "step: 19694, loss: 1.3866\n",
            "step: 19695, loss: 1.3928\n",
            "step: 19696, loss: 1.3878\n",
            "step: 19697, loss: 1.4452\n",
            "step: 19698, loss: 1.4933\n",
            "step: 19699, loss: 1.4091\n",
            "step: 19700, loss: 1.3830\n",
            "step: 19701, loss: 1.3892\n",
            "step: 19702, loss: 1.3838\n",
            "step: 19703, loss: 1.3849\n",
            "step: 19704, loss: 1.3846\n",
            "step: 19705, loss: 1.4143\n",
            "step: 19706, loss: 1.3945\n",
            "step: 19707, loss: 1.3834\n",
            "step: 19708, loss: 1.3867\n",
            "step: 19709, loss: 1.4240\n",
            "step: 19710, loss: 1.3841\n",
            "step: 19711, loss: 1.4208\n",
            "step: 19712, loss: 1.4260\n",
            "step: 19713, loss: 1.3870\n",
            "step: 19714, loss: 1.4288\n",
            "step: 19715, loss: 1.3869\n",
            "step: 19716, loss: 1.3906\n",
            "step: 19717, loss: 1.3882\n",
            "step: 19718, loss: 1.3982\n",
            "step: 19719, loss: 1.4303\n",
            "step: 19720, loss: 1.4085\n",
            "step: 19721, loss: 1.3900\n",
            "step: 19722, loss: 1.4943\n",
            "step: 19723, loss: 1.4091\n",
            "step: 19724, loss: 1.3882\n",
            "step: 19725, loss: 1.3868\n",
            "step: 19726, loss: 1.4243\n",
            "step: 19727, loss: 1.3914\n",
            "step: 19728, loss: 1.4081\n",
            "step: 19729, loss: 1.3905\n",
            "step: 19730, loss: 1.3852\n",
            "step: 19731, loss: 1.3850\n",
            "step: 19732, loss: 1.4253\n",
            "step: 19733, loss: 1.3869\n",
            "step: 19734, loss: 1.3875\n",
            "step: 19735, loss: 1.3861\n",
            "step: 19736, loss: 1.3838\n",
            "step: 19737, loss: 1.3966\n",
            "step: 19738, loss: 1.3837\n",
            "step: 19739, loss: 1.4033\n",
            "step: 19740, loss: 1.3865\n",
            "step: 19741, loss: 1.4533\n",
            "step: 19742, loss: 1.3885\n",
            "step: 19743, loss: 1.4014\n",
            "step: 19744, loss: 1.3934\n",
            "step: 19745, loss: 1.4030\n",
            "step: 19746, loss: 1.4024\n",
            "step: 19747, loss: 1.3977\n",
            "step: 19748, loss: 1.3959\n",
            "step: 19749, loss: 1.3877\n",
            "step: 19750, loss: 1.3881\n",
            "step: 19751, loss: 1.4112\n",
            "step: 19752, loss: 1.3919\n",
            "step: 19753, loss: 1.4006\n",
            "step: 19754, loss: 1.3840\n",
            "step: 19755, loss: 1.3841\n",
            "step: 19756, loss: 1.3859\n",
            "step: 19757, loss: 1.3937\n",
            "step: 19758, loss: 1.3870\n",
            "step: 19759, loss: 1.3883\n",
            "step: 19760, loss: 1.4034\n",
            "step: 19761, loss: 1.3980\n",
            "step: 19762, loss: 1.3876\n",
            "step: 19763, loss: 1.3881\n",
            "step: 19764, loss: 1.3883\n",
            "step: 19765, loss: 1.4075\n",
            "step: 19766, loss: 1.3905\n",
            "step: 19767, loss: 1.3891\n",
            "step: 19768, loss: 1.4085\n",
            "step: 19769, loss: 1.3965\n",
            "step: 19770, loss: 1.4014\n",
            "step: 19771, loss: 1.4112\n",
            "step: 19772, loss: 1.3900\n",
            "step: 19773, loss: 1.4181\n",
            "step: 19774, loss: 1.5225\n",
            "step: 19775, loss: 1.3823\n",
            "step: 19776, loss: 1.4628\n",
            "step: 19777, loss: 1.3849\n",
            "step: 19778, loss: 1.3905\n",
            "step: 19779, loss: 1.4047\n",
            "step: 19780, loss: 1.3978\n",
            "step: 19781, loss: 1.3949\n",
            "step: 19782, loss: 1.4202\n",
            "step: 19783, loss: 1.3874\n",
            "step: 19784, loss: 1.3832\n",
            "step: 19785, loss: 1.4190\n",
            "step: 19786, loss: 1.4300\n",
            "step: 19787, loss: 1.4098\n",
            "step: 19788, loss: 1.3867\n",
            "step: 19789, loss: 1.3889\n",
            "step: 19790, loss: 1.3970\n",
            "step: 19791, loss: 1.3920\n",
            "step: 19792, loss: 1.3859\n",
            "step: 19793, loss: 1.3985\n",
            "step: 19794, loss: 1.4027\n",
            "step: 19795, loss: 1.3851\n",
            "step: 19796, loss: 1.3947\n",
            "step: 19797, loss: 1.3838\n",
            "step: 19798, loss: 1.3843\n",
            "step: 19799, loss: 1.3979\n",
            "step: 19800, loss: 1.3985\n",
            "step: 19801, loss: 1.4174\n",
            "step: 19802, loss: 1.4164\n",
            "step: 19803, loss: 1.4669\n",
            "step: 19804, loss: 1.4106\n",
            "step: 19805, loss: 1.3912\n",
            "step: 19806, loss: 1.4232\n",
            "step: 19807, loss: 1.4407\n",
            "step: 19808, loss: 1.4202\n",
            "step: 19809, loss: 1.3875\n",
            "step: 19810, loss: 1.3923\n",
            "step: 19811, loss: 1.3912\n",
            "step: 19812, loss: 1.3933\n",
            "step: 19813, loss: 1.4745\n",
            "step: 19814, loss: 1.3853\n",
            "step: 19815, loss: 1.3965\n",
            "step: 19816, loss: 1.3885\n",
            "step: 19817, loss: 1.3935\n",
            "step: 19818, loss: 1.4021\n",
            "step: 19819, loss: 1.4603\n",
            "step: 19820, loss: 1.3996\n",
            "step: 19821, loss: 1.4597\n",
            "step: 19822, loss: 1.3884\n",
            "step: 19823, loss: 1.3852\n",
            "step: 19824, loss: 1.4486\n",
            "step: 19825, loss: 1.4154\n",
            "step: 19826, loss: 1.4565\n",
            "step: 19827, loss: 1.3953\n",
            "step: 19828, loss: 1.3843\n",
            "step: 19829, loss: 1.4277\n",
            "step: 19830, loss: 1.4417\n",
            "step: 19831, loss: 1.3971\n",
            "step: 19832, loss: 1.3857\n",
            "step: 19833, loss: 1.3821\n",
            "step: 19834, loss: 1.3852\n",
            "step: 19835, loss: 1.3861\n",
            "step: 19836, loss: 1.3960\n",
            "step: 19837, loss: 1.4024\n",
            "step: 19838, loss: 1.3894\n",
            "step: 19839, loss: 1.3871\n",
            "step: 19840, loss: 1.3902\n",
            "step: 19841, loss: 1.3843\n",
            "step: 19842, loss: 1.3829\n",
            "step: 19843, loss: 1.4316\n",
            "step: 19844, loss: 1.3888\n",
            "step: 19845, loss: 1.3868\n",
            "step: 19846, loss: 1.3851\n",
            "step: 19847, loss: 1.3879\n",
            "step: 19848, loss: 1.3879\n",
            "step: 19849, loss: 1.4005\n",
            "step: 19850, loss: 1.4084\n",
            "step: 19851, loss: 1.4036\n",
            "step: 19852, loss: 1.4332\n",
            "step: 19853, loss: 1.4200\n",
            "step: 19854, loss: 1.3854\n",
            "step: 19855, loss: 1.4554\n",
            "step: 19856, loss: 1.4186\n",
            "step: 19857, loss: 1.4052\n",
            "step: 19858, loss: 1.4368\n",
            "step: 19859, loss: 1.3939\n",
            "step: 19860, loss: 1.3992\n",
            "step: 19861, loss: 1.3848\n",
            "step: 19862, loss: 1.4646\n",
            "step: 19863, loss: 1.3856\n",
            "step: 19864, loss: 1.3974\n",
            "step: 19865, loss: 1.3950\n",
            "step: 19866, loss: 1.4076\n",
            "step: 19867, loss: 1.3833\n",
            "step: 19868, loss: 1.3910\n",
            "step: 19869, loss: 1.4272\n",
            "step: 19870, loss: 1.3978\n",
            "step: 19871, loss: 1.3853\n",
            "step: 19872, loss: 1.4015\n",
            "step: 19873, loss: 1.3891\n",
            "step: 19874, loss: 1.4649\n",
            "step: 19875, loss: 1.4222\n",
            "step: 19876, loss: 1.4527\n",
            "step: 19877, loss: 1.3992\n",
            "step: 19878, loss: 1.4092\n",
            "step: 19879, loss: 1.3860\n",
            "step: 19880, loss: 1.3900\n",
            "step: 19881, loss: 1.4307\n",
            "step: 19882, loss: 1.4273\n",
            "step: 19883, loss: 1.3911\n",
            "step: 19884, loss: 1.4452\n",
            "step: 19885, loss: 1.3974\n",
            "step: 19886, loss: 1.3873\n",
            "step: 19887, loss: 1.3902\n",
            "step: 19888, loss: 1.3850\n",
            "step: 19889, loss: 1.4051\n",
            "step: 19890, loss: 1.3905\n",
            "step: 19891, loss: 1.3974\n",
            "step: 19892, loss: 1.3934\n",
            "step: 19893, loss: 1.3847\n",
            "step: 19894, loss: 1.3828\n",
            "step: 19895, loss: 1.4131\n",
            "step: 19896, loss: 1.3851\n",
            "step: 19897, loss: 1.3867\n",
            "step: 19898, loss: 1.3883\n",
            "step: 19899, loss: 1.3838\n",
            "step: 19900, loss: 1.3994\n",
            "step: 19901, loss: 1.4385\n",
            "step: 19902, loss: 1.3845\n",
            "step: 19903, loss: 1.3895\n",
            "step: 19904, loss: 1.3840\n",
            "step: 19905, loss: 1.3865\n",
            "step: 19906, loss: 1.4020\n",
            "step: 19907, loss: 1.3833\n",
            "step: 19908, loss: 1.4110\n",
            "step: 19909, loss: 1.3845\n",
            "step: 19910, loss: 1.3843\n",
            "step: 19911, loss: 1.3880\n",
            "step: 19912, loss: 1.4360\n",
            "step: 19913, loss: 1.4269\n",
            "step: 19914, loss: 1.3924\n",
            "step: 19915, loss: 1.3875\n",
            "step: 19916, loss: 1.4531\n",
            "step: 19917, loss: 1.4214\n",
            "step: 19918, loss: 1.3863\n",
            "step: 19919, loss: 1.4121\n",
            "step: 19920, loss: 1.3852\n",
            "step: 19921, loss: 1.3976\n",
            "step: 19922, loss: 1.3911\n",
            "step: 19923, loss: 1.3986\n",
            "step: 19924, loss: 1.3837\n",
            "step: 19925, loss: 1.4059\n",
            "step: 19926, loss: 1.3896\n",
            "step: 19927, loss: 1.4073\n",
            "step: 19928, loss: 1.3904\n",
            "step: 19929, loss: 1.3848\n",
            "step: 19930, loss: 1.4149\n",
            "step: 19931, loss: 1.4146\n",
            "step: 19932, loss: 1.3868\n",
            "step: 19933, loss: 1.3835\n",
            "step: 19934, loss: 1.3861\n",
            "step: 19935, loss: 1.3823\n",
            "step: 19936, loss: 1.4263\n",
            "step: 19937, loss: 1.4186\n",
            "step: 19938, loss: 1.3977\n",
            "step: 19939, loss: 1.3996\n",
            "step: 19940, loss: 1.3854\n",
            "step: 19941, loss: 1.3833\n",
            "step: 19942, loss: 1.3897\n",
            "step: 19943, loss: 1.4379\n",
            "step: 19944, loss: 1.3869\n",
            "step: 19945, loss: 1.3969\n",
            "step: 19946, loss: 1.3828\n",
            "step: 19947, loss: 1.3834\n",
            "step: 19948, loss: 1.3871\n",
            "step: 19949, loss: 1.3951\n",
            "step: 19950, loss: 1.4691\n",
            "step: 19951, loss: 1.4030\n",
            "step: 19952, loss: 1.3904\n",
            "step: 19953, loss: 1.3860\n",
            "step: 19954, loss: 1.3923\n",
            "step: 19955, loss: 1.4413\n",
            "step: 19956, loss: 1.3932\n",
            "step: 19957, loss: 1.4090\n",
            "step: 19958, loss: 1.3927\n",
            "step: 19959, loss: 1.3898\n",
            "step: 19960, loss: 1.4586\n",
            "step: 19961, loss: 1.4488\n",
            "step: 19962, loss: 1.3872\n",
            "step: 19963, loss: 1.4042\n",
            "step: 19964, loss: 1.4897\n",
            "step: 19965, loss: 1.3863\n",
            "step: 19966, loss: 1.3973\n",
            "step: 19967, loss: 1.3893\n",
            "step: 19968, loss: 1.3916\n",
            "step: 19969, loss: 1.3863\n",
            "step: 19970, loss: 1.3902\n",
            "step: 19971, loss: 1.3860\n",
            "step: 19972, loss: 1.3944\n",
            "step: 19973, loss: 1.4013\n",
            "step: 19974, loss: 1.3947\n",
            "step: 19975, loss: 1.4057\n",
            "step: 19976, loss: 1.4297\n",
            "step: 19977, loss: 1.3905\n",
            "step: 19978, loss: 1.4268\n",
            "step: 19979, loss: 1.4033\n",
            "step: 19980, loss: 1.3863\n",
            "step: 19981, loss: 1.3955\n",
            "step: 19982, loss: 1.4315\n",
            "step: 19983, loss: 1.3851\n",
            "step: 19984, loss: 1.4934\n",
            "step: 19985, loss: 1.4264\n",
            "step: 19986, loss: 1.4572\n",
            "step: 19987, loss: 1.3874\n",
            "step: 19988, loss: 1.3853\n",
            "step: 19989, loss: 1.4217\n",
            "step: 19990, loss: 1.4158\n",
            "step: 19991, loss: 1.3904\n",
            "step: 19992, loss: 1.3949\n",
            "step: 19993, loss: 1.3914\n",
            "step: 19994, loss: 1.3926\n",
            "step: 19995, loss: 1.4214\n",
            "step: 19996, loss: 1.4073\n",
            "step: 19997, loss: 1.4114\n",
            "step: 19998, loss: 1.3862\n",
            "step: 19999, loss: 1.4034\n",
            "step: 20000, loss: 1.3863\n",
            "saving model to ./models/model_20000.ckpt\n",
            "Batch 0\n",
            "epoch: 0, eval_bleu 0.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nszu01Zn0SeS"
      },
      "source": [
        " #!rm /content/Abstractive-Summarization-With-Transfer-Learning/models/*"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdlGilhsJ8eY",
        "outputId": "f1a8be1c-1e6d-468d-d00d-9aa437672354"
      },
      "source": [
        "#!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmTFDfr5Dsre"
      },
      "source": [
        "#!python Inference.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7vzz7QjJa39"
      },
      "source": [
        "# from flask import Flask,request,render_template\r\n",
        "import requests \r\n",
        "import json\r\n",
        "# from collections import OrderedDict\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N09qsdsJLfZP"
      },
      "source": [
        "import sys\r\n",
        "\r\n",
        "if not 'texar_repo' in sys.path:\r\n",
        "  sys.path += ['texar_repo']\r\n",
        "\r\n",
        "from config import *\r\n",
        "from model import *\r\n",
        "from preprocess2 import *"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnA9cSZsLhxS"
      },
      "source": [
        "start_tokens = tf.fill([tx.utils.get_batch_size(src_input_ids)],\r\n",
        "                       bos_token_id)\r\n",
        "predictions = decoder(\r\n",
        "    memory=encoder_output,\r\n",
        "    memory_sequence_length=src_input_length,\r\n",
        "    decoding_strategy='infer_greedy',\r\n",
        "    beam_width=beam_width,\r\n",
        "    alpha=alpha,\r\n",
        "    start_tokens=start_tokens,\r\n",
        "    end_token=eos_token_id,\r\n",
        "    max_decoding_length=400,\r\n",
        "    mode=tf.estimator.ModeKeys.PREDICT\r\n",
        ")\r\n",
        "if beam_width <= 1:\r\n",
        "    inferred_ids = predictions[0].sample_id\r\n",
        "else:\r\n",
        "    # Uses the best sample by beam search\r\n",
        "    inferred_ids = predictions['sample_id'][:, :, 0]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LNokau0LpA2"
      },
      "source": [
        "tokenizer = tokenization.FullTokenizer(\r\n",
        "      vocab_file=os.path.join(bert_pretrain_dir, 'vocab.txt'),\r\n",
        "      do_lower_case=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_RErlUCLsFq"
      },
      "source": [
        "sess = tf.Session()\r\n",
        "def infer_single_example(story,actual_summary,tokenizer, sess):\r\n",
        "      example = {'src_txt':story,\r\n",
        "      'tgt_txt':actual_summary\r\n",
        "      }\r\n",
        "      # print(example.keys())\r\n",
        "      features = convert_single_example(1,example,max_seq_length_src,max_seq_length_tgt,\r\n",
        "         tokenizer)\r\n",
        "      feed_dict = {\r\n",
        "      src_input_ids:np.array(features.src_input_ids).reshape(1,-1),\r\n",
        "      src_segment_ids : np.array(features.src_segment_ids).reshape(1,-1)\r\n",
        "\r\n",
        "      }\r\n",
        "\r\n",
        "      references, hypotheses = [], []\r\n",
        "      fetches = {\r\n",
        "      'inferred_ids': inferred_ids,\r\n",
        "      }\r\n",
        "      fetches_ = sess.run(fetches, feed_dict=feed_dict)\r\n",
        "      labels = np.array(features.tgt_labels).reshape(1,-1)\r\n",
        "      hypotheses.extend(h.tolist() for h in fetches_['inferred_ids'])\r\n",
        "      references.extend(r.tolist() for r in labels)\r\n",
        "      hypotheses = utils.list_strip_eos(hypotheses, eos_token_id)\r\n",
        "      references = utils.list_strip_eos(references, eos_token_id)\r\n",
        "      hwords = tokenizer.convert_ids_to_tokens(hypotheses[0])\r\n",
        "      rwords = tokenizer.convert_ids_to_tokens(references[0])\r\n",
        "\r\n",
        "      hwords = tx.utils.str_join(hwords).replace(\" ##\",\"\")\r\n",
        "      rwords = tx.utils.str_join(rwords).replace(\" ##\",\"\")\r\n",
        "      print(\"Original\",rwords)\r\n",
        "      print(\"Generated\",hwords)\r\n",
        "      return hwords"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGQeJSigL6L7",
        "outputId": "98061c7e-bd2e-4b76-a25c-f00d3c4dc5dd"
      },
      "source": [
        "story = 'The new question is that we know how many class data includes, but what if number of class is unknow in data. This is kind of like hyperparameter in KNN or regressions.'\r\n",
        "summ = 'The new question is that we know how many class data includes, but what if number of class is unknow in data. This is kind of like hyperparameter in KNN or regressions.'\r\n",
        "\r\n",
        "sess.run(tf.global_variables_initializer())\r\n",
        "sess.run(tf.local_variables_initializer())\r\n",
        "sess.run(tf.tables_initializer())\r\n",
        "saver.restore(sess, tf.train.latest_checkpoint(model_dir))\r\n",
        "\r\n",
        "\r\n",
        "bloop = infer_single_example(story, summ, tokenizer, sess)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./models/model_20000.ckpt\n",
            "Original the new question is that we know how many class data includes , but what if number of class is unknow in data . this is kind of like hyperparameter in knn or regressions .\n",
            "Generated the new question is that we know how many class data includes , but what if number of un unknow in data . this is kind of like hyperparameter in knn or regressions .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWk_yKr4-hFN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "02c846e4-ae57-4963-c2d1-1b2f5c572509"
      },
      "source": [
        "bloop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'is we we we we new is new new is that is is is is we question is is is question is question is is is is is know class is is regressionmetermetermetermetermetermetermetermetermetermetermetermetermetermetermetermetermetermetermetermetermetermetermetermetermeterparametermetermetermetermetermetermetermetermetermetermetermeterparametermetermetermeterparametermetermetermeter in is questionmetermetermeterparametermetermetermetermetermetermetermetermetermetermetermetermetermetermetermetermetermetermetermetermeter in know is is that we data data know is is is is is new new is is is is is is is is is is is is new is is is is is is that that is new is is is that new is is is is is is is is that we is is is is is is that is is is is is is is is is that that is is is is is is we datametermeter is is is un is that new is that is is that that is is is that is is is that is is is new is is is is is is is that is is is is is is is new is is is that that is is is is is is is is that is is is that is is is is is that is is is is is is is is un is is is is that is question class is is is is is is is is is new is is is is is is is is is is is that is is is is new is that we is is is is is new is is is is new is is is is is new is is is is is is is new we is is is is is is that that is that is we datametermetermeterparameter in is is is that is is is new is is new new is is un is we datametermetermeterparametermetermetermetermetermetermetermeternowmetermetermetermetermetermeterparametermeter in is is is is is that is that we is is'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "eUqdqXg4xNIc",
        "outputId": "0aca1602-ac28-4fa8-a48b-63404b662b8d"
      },
      "source": [
        "bloop"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the new question is that we know how many class data includes , but what if number of un unknow in data . this is kind of like hyperparameter in knn or regressions .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3JE3letPPFr",
        "outputId": "7f1f053d-bbcc-4360-c27d-ab4cfc703ec6"
      },
      "source": [
        "#!zip -r /content/Try.zip /content/Abstractive-Summarization-With-Transfer-Learning"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.DS_Store (deflated 96%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.ipynb_checkpoints/Untitled-checkpoint.ipynb (deflated 21%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/main.py (deflated 71%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/Inference.py (deflated 59%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/Readme.md (deflated 46%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/data/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/data/train_story.txt (deflated 93%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/data/eval_summ.txt (deflated 93%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/data/eval.tf_record (deflated 98%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/data/train.tf_record (deflated 98%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/data/train_summ.txt (deflated 93%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/data/eval_story.txt (deflated 93%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/uncased_L-12_H-768_A-12/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/uncased_L-12_H-768_A-12/.DS_Store (deflated 97%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/uncased_L-12_H-768_A-12/bert_model.ckpt.index (deflated 67%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/uncased_L-12_H-768_A-12/bert_config.json (deflated 47%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/uncased_L-12_H-768_A-12/bert_model.ckpt.meta (deflated 92%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/uncased_L-12_H-768_A-12/vocab.txt (deflated 53%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/model.py (deflated 67%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/.DS_Store (deflated 97%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/model_25.ckpt.meta (deflated 94%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/logging.txt (deflated 79%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/best-model.ckpt.meta (deflated 94%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/tmp.eval.src (deflated 100%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/model_75.ckpt.meta (deflated 94%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/model_100.ckpt.index (deflated 71%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/events.out.tfevents.1609423492.93501ff5e58a (deflated 94%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/model_25.ckpt.index (deflated 71%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/model_50.ckpt.meta (deflated 94%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/model_75.ckpt.index (deflated 71%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/model_50.ckpt.index (deflated 71%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/best-model.ckpt.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/tmp.eval.tgt (deflated 95%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/model_25.ckpt.data-00000-of-00001 (deflated 8%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/model_100.ckpt.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/best-model.ckpt.index (deflated 71%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/model_50.ckpt.data-00000-of-00001 (deflated 8%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/model_100.ckpt.meta (deflated 94%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/checkpoint (deflated 73%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/models/model_75.ckpt.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/requirements.txt (deflated 48%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/README.md (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/CHANGELOG.md (deflated 39%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/LICENSE (deflated 65%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/bin/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/bin/train.py (deflated 70%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/bin/average_checkpoints.py (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/bin/utils/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/bin/utils/learn_bpe (deflated 67%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/bin/utils/spm_decode (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/bin/utils/apply_bpe (deflated 68%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/bin/utils/README.md (deflated 63%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/bin/utils/spm_encode (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/bin/utils/make_vocab.py (deflated 58%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/bin/utils/spm_train (deflated 63%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/bin/utils/multi-bleu.perl (deflated 65%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/sentence_classifier/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/sentence_classifier/sst_data_preprocessor.py (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/sentence_classifier/README.md (deflated 48%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/sentence_classifier/clas_main.py (deflated 59%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/sentence_classifier/config_kim.py (deflated 54%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/memory_network_lm/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/memory_network_lm/ptb_reader.py (deflated 63%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/memory_network_lm/lm_ptb_memnet.py (deflated 65%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/memory_network_lm/README.md (deflated 53%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/memory_network_lm/config.py (deflated 47%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/language_model_ptb/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/language_model_ptb/ptb_reader.py (deflated 61%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/language_model_ptb/config_small.py (deflated 49%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/language_model_ptb/lm_ptb.py (deflated 61%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/language_model_ptb/README.md (deflated 48%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/language_model_ptb/config_large.py (deflated 48%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/language_model_ptb/config_medium.py (deflated 48%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/vae_text/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/vae_text/config_trans_yahoo.py (deflated 67%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/vae_text/prepare_data.py (deflated 54%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/vae_text/README.md (deflated 55%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/vae_text/config_lstm_ptb.py (deflated 65%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/vae_text/config_lstm_yahoo.py (deflated 64%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/vae_text/config_trans_ptb.py (deflated 68%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/vae_text/vae_train.py (deflated 70%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/text_style_transfer/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/text_style_transfer/main.py (deflated 67%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/text_style_transfer/ctrl_gen_model.py (deflated 71%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/text_style_transfer/prepare_data.py (deflated 41%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/text_style_transfer/README.md (deflated 59%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/text_style_transfer/config.py (deflated 67%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/config_wmt14.py (deflated 31%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/requirements.txt (deflated 3%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/scripts/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/scripts/iwslt15_en_vi.sh (deflated 58%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/scripts/wmt14_en_de.sh (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/bleu_tool.py (deflated 63%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/config_iwslt15.py (deflated 33%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/README.md (deflated 65%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/__pycache__/bleu_tool.cpython-36.pyc (deflated 48%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/config_model.py (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/preprocess_data.sh (deflated 70%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/transformer_main.py (deflated 68%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/utils/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/utils/__init__.py (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/utils/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/utils/__pycache__/__init__.cpython-36.pyc (deflated 17%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/utils/__pycache__/data_utils.cpython-36.pyc (deflated 47%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/utils/__pycache__/utils.cpython-36.pyc (deflated 40%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/utils/utils.py (deflated 55%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/utils/preprocess.py (deflated 71%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/transformer/utils/data_utils.py (deflated 62%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/rl_gym/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/rl_gym/README.md (deflated 49%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/rl_gym/ac_cartpole.py (deflated 53%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/rl_gym/dqn_cartpole.py (deflated 53%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/rl_gym/pg_cartpole.py (deflated 53%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/rl_gym/config.py (deflated 39%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_attn/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_attn/config_model_full.py (deflated 75%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_attn/prepare_data.py (deflated 49%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_attn/README.md (deflated 49%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_attn/config_iwslt14.py (deflated 75%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_attn/config_model.py (deflated 60%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_attn/config_toy_copy.py (deflated 79%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_attn/seq2seq_attn.py (deflated 65%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_configs/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_configs/README.md (deflated 49%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_configs/config_model_small.yml (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_configs/config_model_medium.yml (deflated 58%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_configs/config_data_toy_copy.yml (deflated 67%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/README.md (deflated 73%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/hierarchical_dialog/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/hierarchical_dialog/config_model_biminor.py (deflated 69%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/hierarchical_dialog/hred.py (deflated 71%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/hierarchical_dialog/sw_loader.py (deflated 68%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/hierarchical_dialog/config_model_uniminor.py (deflated 72%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/hierarchical_dialog/config_data.py (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/hierarchical_dialog/README.md (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/bert_classifier_main.py (deflated 69%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/config_data_mrpc.py (deflated 25%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/README.md (deflated 60%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/__pycache__/config_classifier.cpython-36.pyc (deflated 20%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/bert_config_lib/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/bert_config_lib/__init__.py (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/bert_config_lib/config_model_uncased_L-12_H-768_A-12.py (deflated 66%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/bert_config_lib/README.md (deflated 30%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/config_classifier.py (deflated 50%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/config_data_sst.py (deflated 25%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/utils/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/utils/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/utils/__pycache__/tokenization.cpython-36.pyc (deflated 54%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/utils/__pycache__/model_utils.cpython-36.pyc (deflated 49%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/utils/__pycache__/data_utils.cpython-36.pyc (deflated 65%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/utils/tokenization.py (deflated 70%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/utils/model_utils.py (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/bert/utils/data_utils.py (deflated 78%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/distributed_gpu/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/distributed_gpu/ptb_reader.py (deflated 62%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/distributed_gpu/config_small.py (deflated 49%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/distributed_gpu/lm_ptb_distributed.py (deflated 63%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/distributed_gpu/README.md (deflated 59%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/distributed_gpu/config_large.py (deflated 48%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/distributed_gpu/config_medium.py (deflated 48%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/interpolation_main.py (deflated 72%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/configs/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/configs/__init__.py (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/configs/config_giga.py (deflated 76%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/configs/config_iwslt14.py (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/configs/config_model.py (deflated 67%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/requirements.txt (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/README.md (deflated 62%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/raml_main.py (deflated 72%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/interpolation_helper.py (deflated 71%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/scheduled_sampling_main.py (deflated 70%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/interpolation_decoder.py (deflated 63%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/baseline_seq2seq_attn_main.py (deflated 70%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/utils/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/utils/prepare_data.py (deflated 49%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/utils/raml_samples_generation/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/utils/raml_samples_generation/vocab.py (deflated 70%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/utils/raml_samples_generation/process_samples.py (deflated 75%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/utils/raml_samples_generation/README.md (deflated 36%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/utils/raml_samples_generation/gen_samples_giga.sh (deflated 52%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/utils/raml_samples_generation/util.py (deflated 62%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_exposure_bias/utils/raml_samples_generation/gen_samples_iwslt14.sh (deflated 53%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_rl/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_rl/prepare_data.py (deflated 49%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_rl/README.md (deflated 54%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_rl/config_iwslt14.py (deflated 75%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_rl/config_model.py (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_rl/config_toy_copy.py (deflated 78%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seq2seq_rl/seq2seq_attn_pg.py (deflated 67%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/torchtext/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/torchtext/config_small.py (deflated 49%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/torchtext/requirements.txt (deflated 3%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/torchtext/batchfirst_bptt.py (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/torchtext/README.md (deflated 31%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/torchtext/lm_torchtext.py (deflated 62%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/torchtext/.gitignore (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/sequence_tagging/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/sequence_tagging/ner.py (deflated 68%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/sequence_tagging/scores.py (deflated 44%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/sequence_tagging/conll_writer.py (deflated 59%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/sequence_tagging/README.md (deflated 45%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/sequence_tagging/conll_reader.py (deflated 68%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/sequence_tagging/conlleval (deflated 76%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/sequence_tagging/config.py (deflated 51%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seqgan/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seqgan/config_coco.py (deflated 71%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seqgan/README.md (deflated 60%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seqgan/config_ptb_large.py (deflated 70%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seqgan/config_ptb_small.py (deflated 70%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seqgan/config_ptb_medium.py (deflated 70%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seqgan/seqgan_train.py (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/examples/seqgan/data_utils.py (deflated 53%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/core/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/core/__init__.py (deflated 46%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/core/optimization_test.py (deflated 77%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/core/layers_test.py (deflated 82%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/core/layers.py (deflated 79%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/core/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/core/__pycache__/explorations.cpython-36.pyc (deflated 59%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/core/__pycache__/layers.cpython-36.pyc (deflated 67%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/core/__pycache__/__init__.cpython-36.pyc (deflated 32%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/core/__pycache__/optimization.cpython-36.pyc (deflated 65%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/core/__pycache__/replay_memories.cpython-36.pyc (deflated 62%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/core/optimization.py (deflated 77%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/core/explorations.py (deflated 67%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/core/replay_memories.py (deflated 67%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/__init__.py (deflated 51%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/hyperparams.py (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/evals/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/evals/__init__.py (deflated 44%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/evals/metrics.py (deflated 66%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/evals/bleu.py (deflated 70%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/evals/bleu_moses.py (deflated 68%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/evals/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/evals/__pycache__/bleu.cpython-36.pyc (deflated 58%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/evals/__pycache__/bleu_moses.cpython-36.pyc (deflated 55%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/evals/__pycache__/__init__.cpython-36.pyc (deflated 29%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/evals/__pycache__/metrics.cpython-36.pyc (deflated 51%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/evals/bleu_test.py (deflated 81%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data_decoders.py (deflated 80%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/__init__.py (deflated 47%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/data_iterators_test.py (deflated 87%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/paired_text_data_test.py (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/__init__.py (deflated 52%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/mono_text_data_test.py (deflated 82%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/scalar_data_test.py (deflated 75%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/mono_text_data.py (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/data_base.py (deflated 69%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/data_iterators.py (deflated 84%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/dataset_utils_test.py (deflated 61%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/__pycache__/scalar_data.cpython-36.pyc (deflated 56%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/__pycache__/data_base.cpython-36.pyc (deflated 59%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/__pycache__/mono_text_data.cpython-36.pyc (deflated 63%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/__pycache__/data_iterators.cpython-36.pyc (deflated 77%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/__pycache__/multi_aligned_data.cpython-36.pyc (deflated 62%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/__pycache__/dataset_utils.cpython-36.pyc (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/__pycache__/text_data_base.cpython-36.pyc (deflated 43%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/__pycache__/paired_text_data.cpython-36.pyc (deflated 66%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/__pycache__/__init__.cpython-36.pyc (deflated 45%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/text_data_base.py (deflated 64%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/multi_aligned_data_test.py (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/paired_text_data.py (deflated 78%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/scalar_data.py (deflated 68%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/dataset_utils.py (deflated 70%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data/multi_aligned_data.py (deflated 77%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/vocabulary.py (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/__pycache__/vocabulary.cpython-36.pyc (deflated 65%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/__pycache__/data_decoders.cpython-36.pyc (deflated 69%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/__pycache__/embedding.cpython-36.pyc (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/__pycache__/__init__.cpython-36.pyc (deflated 35%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/__pycache__/data_utils.cpython-36.pyc (deflated 49%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data_utils_test.py (deflated 58%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/vocabulary_test.py (deflated 62%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/embedding_test.py (deflated 62%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/data_utils.py (deflated 66%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/data/embedding.py (deflated 69%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/models/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/models/__init__.py (deflated 43%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/models/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/models/__pycache__/model_base.cpython-36.pyc (deflated 55%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/models/__pycache__/__init__.cpython-36.pyc (deflated 28%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/models/model_base.py (deflated 61%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/models/seq2seq/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/models/seq2seq/__init__.py (deflated 44%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/models/seq2seq/basic_seq2seq.py (deflated 70%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/models/seq2seq/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/models/seq2seq/__pycache__/seq2seq_base.cpython-36.pyc (deflated 62%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/models/seq2seq/__pycache__/__init__.cpython-36.pyc (deflated 32%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/models/seq2seq/__pycache__/basic_seq2seq.cpython-36.pyc (deflated 54%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/models/seq2seq/seq2seq_base.py (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/seq_pg_agent.py (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/agent_gym_utils.py (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/__init__.py (deflated 49%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/agent_utils.py (deflated 66%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/episodic_agent_base.py (deflated 63%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/pg_agent.py (deflated 70%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/__pycache__/dqn_agent.cpython-36.pyc (deflated 63%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/__pycache__/seq_agent_base.cpython-36.pyc (deflated 42%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/__pycache__/agent_gym_utils.cpython-36.pyc (deflated 46%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/__pycache__/ac_agent.cpython-36.pyc (deflated 61%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/__pycache__/agent_base.cpython-36.pyc (deflated 49%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/__pycache__/__init__.cpython-36.pyc (deflated 37%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/__pycache__/seq_pg_agent.cpython-36.pyc (deflated 61%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/__pycache__/pg_agent.cpython-36.pyc (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/__pycache__/agent_utils.cpython-36.pyc (deflated 55%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/__pycache__/episodic_agent_base.cpython-36.pyc (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/agent_base.py (deflated 56%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/dqn_agent.py (deflated 76%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/ac_agent.py (deflated 71%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/agent_utils_test.py (deflated 61%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/seq_pg_agent_test.py (deflated 58%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/agents/seq_agent_base.py (deflated 48%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/networks/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/networks/__init__.py (deflated 46%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/networks/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/networks/__pycache__/conv_networks.cpython-36.pyc (deflated 63%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/networks/__pycache__/network_base.cpython-36.pyc (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/networks/__pycache__/networks.cpython-36.pyc (deflated 53%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/networks/__pycache__/__init__.cpython-36.pyc (deflated 36%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/networks/network_base.py (deflated 69%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/networks/conv_networks_test.py (deflated 76%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/networks/networks.py (deflated 59%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/networks/networks_test.py (deflated 60%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/networks/conv_networks.py (deflated 75%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/__init__.py (deflated 52%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/connectors/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/connectors/__init__.py (deflated 45%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/connectors/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/connectors/__pycache__/connectors.cpython-36.pyc (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/connectors/__pycache__/connector_base.cpython-36.pyc (deflated 51%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/connectors/__pycache__/__init__.cpython-36.pyc (deflated 34%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/connectors/connector_base.py (deflated 55%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/connectors/connectors.py (deflated 81%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/connectors/connectors_test.py (deflated 76%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/embedders/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/embedders/__init__.py (deflated 47%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/embedders/embedders_test.py (deflated 82%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/embedders/embedders.py (deflated 69%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/embedders/position_embedders.py (deflated 70%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/embedders/embedder_utils.py (deflated 71%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/embedders/embedder_base.py (deflated 62%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/embedders/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/embedders/__pycache__/position_embedders.cpython-36.pyc (deflated 59%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/embedders/__pycache__/embedder_base.cpython-36.pyc (deflated 48%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/embedders/__pycache__/__init__.cpython-36.pyc (deflated 38%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/embedders/__pycache__/embedders.cpython-36.pyc (deflated 63%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/embedders/__pycache__/embedder_utils.cpython-36.pyc (deflated 66%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/embedders/embedder_utils_test.py (deflated 65%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/decoders/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/decoders/__init__.py (deflated 51%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/decoders/transformer_decoders.py (deflated 77%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/decoders/beam_search_decode_test.py (deflated 80%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/decoders/transformer_decoders_test.py (deflated 80%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/decoders/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/decoders/__pycache__/transformer_decoders.cpython-36.pyc (deflated 61%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/decoders/__pycache__/rnn_decoders.cpython-36.pyc (deflated 71%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/decoders/__pycache__/rnn_decoder_helpers.cpython-36.pyc (deflated 66%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/decoders/__pycache__/__init__.cpython-36.pyc (deflated 45%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/decoders/__pycache__/beam_search_decode.cpython-36.pyc (deflated 61%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/decoders/__pycache__/rnn_decoder_base.cpython-36.pyc (deflated 67%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/decoders/beam_search_decode.py (deflated 71%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/decoders/rnn_decoders.py (deflated 78%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/decoders/rnn_decoder_helpers.py (deflated 75%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/decoders/rnn_decoder_base.py (deflated 75%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/decoders/rnn_decoders_test.py (deflated 86%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/__pycache__/__init__.cpython-36.pyc (deflated 45%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/qnets/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/qnets/__init__.py (deflated 42%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/qnets/qnets.py (deflated 73%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/qnets/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/qnets/__pycache__/qnets.cpython-36.pyc (deflated 65%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/qnets/__pycache__/__init__.cpython-36.pyc (deflated 26%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/policies/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/policies/__init__.py (deflated 38%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/policies/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/policies/__pycache__/__init__.cpython-36.pyc (deflated 26%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/policies/__pycache__/policy_nets.cpython-36.pyc (deflated 66%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/policies/policy_nets_test.py (deflated 64%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/policies/policy_nets.py (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/__init__.py (deflated 52%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/transformer_encoders.py (deflated 76%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/hierarchical_encoders_test.py (deflated 80%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/hierarchical_encoders.py (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/__pycache__/transformer_encoders.cpython-36.pyc (deflated 61%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/__pycache__/multihead_attention.cpython-36.pyc (deflated 53%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/__pycache__/__init__.cpython-36.pyc (deflated 46%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/__pycache__/hierarchical_encoders.cpython-36.pyc (deflated 63%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/__pycache__/rnn_encoders.cpython-36.pyc (deflated 72%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/__pycache__/encoder_base.cpython-36.pyc (deflated 45%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/__pycache__/conv_encoders.cpython-36.pyc (deflated 49%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/rnn_encoders_test.py (deflated 84%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/conv_encoders_test.py (deflated 76%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/encoder_base.py (deflated 50%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/conv_encoders.py (deflated 54%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/rnn_encoders.py (deflated 81%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/encoders/multihead_attention.py (deflated 69%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/classifiers/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/classifiers/__init__.py (deflated 45%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/classifiers/conv_classifiers_test.py (deflated 67%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/classifiers/rnn_classifiers.py (deflated 72%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/classifiers/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/classifiers/__pycache__/classifier_base.cpython-36.pyc (deflated 46%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/classifiers/__pycache__/__init__.cpython-36.pyc (deflated 35%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/classifiers/__pycache__/conv_classifiers.cpython-36.pyc (deflated 60%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/classifiers/__pycache__/rnn_classifiers.cpython-36.pyc (deflated 60%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/classifiers/rnn_classifiers_test.py (deflated 84%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/classifiers/classifier_base.py (deflated 51%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/classifiers/conv_classifiers.py (deflated 69%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/memory/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/memory/memory_network_test.py (deflated 71%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/memory/__init__.py (deflated 43%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/memory/embed_fns.py (deflated 60%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/memory/memory_network.py (deflated 78%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/memory/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/memory/__pycache__/memory_network.cpython-36.pyc (deflated 69%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/memory/__pycache__/embed_fns.cpython-36.pyc (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/modules/memory/__pycache__/__init__.cpython-36.pyc (deflated 30%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/__pycache__/module_base.cpython-36.pyc (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/__pycache__/hyperparams.cpython-36.pyc (deflated 61%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/__pycache__/__init__.cpython-36.pyc (deflated 38%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/__pycache__/context.cpython-36.pyc (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/context.py (deflated 66%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/hyperparams_test.py (deflated 77%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/run/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/run/__init__.py (deflated 41%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/run/executor.py (deflated 72%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/run/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/run/__pycache__/__init__.cpython-36.pyc (deflated 22%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/run/__pycache__/executor.cpython-36.pyc (deflated 64%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/run/executor_test.py (deflated 67%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/mle_losses.py (deflated 85%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/__init__.py (deflated 49%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/rewards_test.py (deflated 86%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/__pycache__/adv_losses.cpython-36.pyc (deflated 50%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/__pycache__/rewards.cpython-36.pyc (deflated 54%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/__pycache__/entropy.cpython-36.pyc (deflated 72%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/__pycache__/losses_utils.cpython-36.pyc (deflated 61%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/__pycache__/pg_losses.cpython-36.pyc (deflated 77%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/__pycache__/__init__.cpython-36.pyc (deflated 39%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/__pycache__/mle_losses.cpython-36.pyc (deflated 80%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/rewards.py (deflated 71%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/adv_losses_test.py (deflated 64%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/losses_utils.py (deflated 73%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/mle_losses_test.py (deflated 79%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/adv_losses.py (deflated 61%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/pg_losses.py (deflated 81%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/rl_losses.py (deflated 72%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/losses/entropy.py (deflated 78%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/module_base.py (deflated 64%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/__init__.py (deflated 51%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/variables.py (deflated 58%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/utils_io.py (deflated 67%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/shapes_test.py (deflated 62%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/exceptions.py (deflated 42%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/transformer_attentions.py (deflated 63%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/mode_test.py (deflated 69%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/__pycache__/utils_io.cpython-36.pyc (deflated 54%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/__pycache__/transformer_attentions.cpython-36.pyc (deflated 51%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/__pycache__/average_recorder.cpython-36.pyc (deflated 63%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/__pycache__/exceptions.cpython-36.pyc (deflated 33%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/__pycache__/mode.cpython-36.pyc (deflated 68%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/__pycache__/transformer_utils.cpython-36.pyc (deflated 53%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/__pycache__/__init__.cpython-36.pyc (deflated 41%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/__pycache__/beam_search.cpython-36.pyc (deflated 64%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/__pycache__/variables.cpython-36.pyc (deflated 48%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/__pycache__/utils.cpython-36.pyc (deflated 70%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/__pycache__/dtypes.cpython-36.pyc (deflated 48%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/__pycache__/shapes.cpython-36.pyc (deflated 64%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/utils_test.py (deflated 79%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/average_recorder_test.py (deflated 77%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/shapes.py (deflated 73%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/utils.py (deflated 78%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/mode.py (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/transformer_utils.py (deflated 66%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/beam_search.py (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/dtypes.py (deflated 63%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/utils/average_recorder.py (deflated 73%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/texar/context_test.py (deflated 74%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/.travis.yml (deflated 41%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/.pylintrc (deflated 63%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/index.rst (deflated 51%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/_static/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/_static/img/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/_static/img/logo_h.png (deflated 3%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/_static/css/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/_static/css/custom_theme.css (deflated 52%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/requirements.txt (deflated 15%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/conf.py (deflated 62%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/code/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/code/models.rst (deflated 63%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/code/agents.rst (deflated 78%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/code/context.rst (deflated 73%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/code/utils.rst (deflated 86%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/code/txtgen.rst (deflated 11%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/code/hyperparams.rst (deflated 25%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/code/losses.rst (deflated 85%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/code/evals.rst (deflated 77%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/code/modules.rst (deflated 87%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/code/run.rst (deflated 27%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/code/core.rst (deflated 85%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/code/data.rst (deflated 84%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/tutorials/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/tutorials/tutorial.rst (deflated 17%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/Makefile (deflated 73%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/make.bat (deflated 76%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/examples.md (deflated 79%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/docs/get_started.md (deflated 58%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/.gitignore (deflated 57%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/setup.py (deflated 56%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/texar_repo/config.py (deflated 53%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/__pycache__/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/__pycache__/model.cpython-36.pyc (deflated 42%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/__pycache__/preprocess.cpython-36.pyc (deflated 56%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/__pycache__/config.cpython-36.pyc (deflated 37%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/preprocess.py (deflated 76%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/BERT_SUMM.ipynb (deflated 80%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/description (deflated 14%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/hooks/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/hooks/pre-commit.sample (deflated 43%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/hooks/fsmonitor-watchman.sample (deflated 53%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/hooks/pre-push.sample (deflated 50%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/info/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/info/exclude (deflated 28%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/logs/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/logs/refs/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/logs/refs/remotes/origin/HEAD (deflated 27%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/logs/refs/heads/master (deflated 27%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/logs/HEAD (deflated 27%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/objects/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/objects/info/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/objects/pack/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/objects/pack/pack-15a10913c9b82a2494a96bd90e245a65cb8f773a.idx (deflated 5%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/objects/pack/pack-15a10913c9b82a2494a96bd90e245a65cb8f773a.pack (deflated 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/index (deflated 62%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/branches/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/refs/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/refs/remotes/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/refs/heads/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/refs/heads/master (deflated 2%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/refs/tags/ (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/config (deflated 31%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/packed-refs (deflated 11%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/.git/HEAD (stored 0%)\n",
            "  adding: content/Abstractive-Summarization-With-Transfer-Learning/config.py (deflated 55%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBefVvD5PXom"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}